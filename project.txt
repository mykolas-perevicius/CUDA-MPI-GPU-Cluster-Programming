
================================================================================

=== FILE: final_project/v4_mpi_cuda/Makefile ===

# Makefile for V4: MPI + CUDA (Host Staging)

# Use nvcc as the primary compiler, but tell it to use mpicxx for host code compilation/linking
NVCC       := nvcc
CXX_MPI    := mpicxx
# Ensure mpicxx is found if not in default PATH, e.g., export PATH=/path/to/openmpi/bin:$PATH
# Or provide full path: CXX_MPI := /path/to/openmpi/bin/mpicxx

NVCC_FLAGS := -std=c++11 -O3 -Xcompiler="-Wall -Wextra -Wno-unused-parameter -Wno-cast-function-type" \
              -gencode arch=compute_75,code=compute_75 \
              -gencode arch=compute_75,code=sm_75 \
              -ccbin=$(CXX_MPI)

TARGET    := template
INCLUDES  := -I./include

# --- List Source Files ---
# .cpp files are compiled by nvcc using the C++ compiler specified by -ccbin
SRCS_CPP  := src/main_mpi_cuda.cpp
# .cu files are compiled by nvcc's CUDA compiler
SRCS_CU   := src/alexnet_mpi_cuda.cu src/layers_mpi_cuda.cu

OBJS      := $(SRCS_CPP:.cpp=.o) $(SRCS_CU:.cu=.o)

# --- Libraries ---
# Get MPI linker flags dynamically from mpicxx. Handle potential errors.
MPI_LDFLAGS := $(shell $(CXX_MPI) --showme:link 2>/dev/null)
ifeq ($(MPI_LDFLAGS),)
    $(error "Failed to get MPI linker flags from $(CXX_MPI). Is Open MPI configured correctly?")
endif
# Add CUDA runtime library
CUDA_LDFLAGS := -lcudart -lm

LDFLAGS   := $(MPI_LDFLAGS) $(CUDA_LDFLAGS)

.PHONY: all clean

all: $(TARGET)

# --- Compile Rules ---
# Rule for compiling .cu files (CUDA code)
# Use -dc for relocatable device code needed for separate compilation
%.o: %.cu $(wildcard include/*.hpp) Makefile
	$(NVCC) $(NVCC_FLAGS) $(INCLUDES) -dc $< -o $@

# Rule for compiling .cpp files (Host code, using nvcc which invokes mpicxx via -ccbin)
# No -dc needed here.
%.o: %.cpp $(wildcard include/*.hpp) Makefile
	$(NVCC) $(NVCC_FLAGS) $(INCLUDES) -c $< -o $@

# --- Link Rule ---
# Link all object files using nvcc (which uses mpicxx for final linking via -ccbin)
# Note: Linking .o files generated with -dc requires nvcc for device linking step.
$(TARGET): $(OBJS)
	$(NVCC) $(NVCC_FLAGS) $(OBJS) -o $@ $(LDFLAGS)

clean:
	rm -f $(TARGET) $(OBJS)

================================================================================

=== FILE: final_project/v4_mpi_cuda/include/alexnet.hpp ===

// final_project/v4_mpi_cuda/include/alexnet.hpp
#ifndef ALEXNET_MPI_CUDA_HPP
#define ALEXNET_MPI_CUDA_HPP

#include <vector>
#include <string> // For error messages
#include <cstddef> // For size_t

// Holds layer parameters (same as V1/V3)
// Host vectors store the master copy, will be copied to device.
struct LayerParams {
    std::vector<float> weights;
    std::vector<float> biases;
    int K, F, S, P;      // Conv params
    int F_pool, S_pool;  // Pooling
    int N_lrn;           // LRN window
    float alpha, beta, k_lrn;
};

// Function to run the forward pass using MPI and CUDA
// Takes the local portion of input data on host, performs computation on GPU,
// returns the local portion of the final output on host.
// **Declaration Only** - Definition is in alexnet_mpi_cuda.cu
// *** ADDED original total height H argument ***
void alexnetForwardPassMPI_CUDA(
    const std::vector<float>& h_localInput, // Local input data slice on host
    int localH,          // Height of the local input slice
    int H, int W, int C, // *** ADDED H *** Original Dimensions (H, W, C)
    const LayerParams& p1, // Params for Block 1 (Conv1, Pool1)
    const LayerParams& p2, // Params for Block 2 (Conv2, Pool2, LRN2)
    std::vector<float>& h_localOutput, // Local output slice on host (resized by function)
    int rank, int size   // MPI rank and size for halo logic
);

// Helper inline functions for calculating output dimensions
inline int convOutDim(int D, int F, int P, int S) {
    if (S <= 0) return 0; // Avoid division by zero
    // Ensure filter fits, considering padding
    if (F > D + 2 * P) return 0;
    return (D + 2*P - F) / S + 1;
}

inline int poolOutDim(int D, int F, int S) {
     if (S <= 0) return 0; // Avoid division by zero
    // Standard pooling, assumes P=0
    // Handle edge case where input dim is less than filter dim
    if (D < F) return 0;
    return (D - F) / S + 1;
}

#endif // ALEXNET_MPI_CUDA_HPP

================================================================================

=== FILE: final_project/v4_mpi_cuda/include/layers.hpp ===

#ifndef LAYERS_MPI_CUDA_HPP
#define LAYERS_MPI_CUDA_HPP

#include <cstddef> // Include for size_t

// This header primarily declares the CUDA kernel *launcher* functions.
// The actual kernels (__global__) are defined in layers_mpi_cuda.cu.
// These signatures are identical to V3.
// **Declarations Only** - Definitions are in layers_mpi_cuda.cu

// Launches conv kernel: output size (Ho×Wo×K)
// Operates entirely on device memory pointers.
void cudaConvLayer(
    float* d_output,
    const float* d_input,
    const float* d_weights,
    const float* d_biases,
    int H, int W, int C, // Input dimensions (of d_input)
    const int K, const int F, const int S, const int P); // Layer params

// Elementwise ReLU in‐place on device memory
void cudaReluLayer(float* d_data, size_t N); // N = total number of elements

// Max‐pool kernel launcher on device memory
void cudaMaxPoolLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C, // Input dimensions (of d_input)
    int F_pool, int S_pool); // Pooling params

// Local response normalization kernel launcher on device memory
void cudaLRNLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C, // Input dimensions (of d_input)
    int N, float alpha, float beta, float k); // LRN params

#endif // LAYERS_MPI_CUDA_HPP

================================================================================

=== FILE: final_project/v4_mpi_cuda/src/main_mpi_cuda.cpp ===

// final_project/v4_mpi_cuda/src/main_mpi_cuda.cpp
#include <mpi.h>
#include <iostream>
#include <vector>
#include <numeric> // For std::accumulate
#include <chrono>
#include <algorithm> // For std::min
#include <cmath>     // For std::ceil, std::max
#include <cuda_runtime.h>
#include <limits.h> // For INT_MAX

#include "../include/alexnet.hpp" // Includes LayerParams, forward pass prototype, dim helpers

// Simple helper to check CUDA calls from host code if needed (mainly used in .cu files)
// In this .cpp file, major CUDA errors are more likely caught by MPI aborts triggered in alexnet_mpi_cuda.cu
inline void hostCheckCUDA(cudaError_t err, const char* file, int line) {
    if (err != cudaSuccess) {
        // Ensure only one rank prints the error before aborting
        int rank;
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        if (rank == 0) {
             fprintf(stderr, "CUDA Error in host code at %s:%d - %s\n", file, line, cudaGetErrorString(err));
             fflush(stderr);
        }
        MPI_Abort(MPI_COMM_WORLD, 1); // Use MPI_Abort for coordinated exit
    }
}
#define HOST_CUDA_CHECK(call) { hostCheckCUDA((call), __FILE__, __LINE__); }

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // --- 1. Root node initialization ---
    int H = 227, W = 227, C = 3; // Standard AlexNet input - H is now crucial and broadcast
    LayerParams p1, p2;
    std::vector<float> h_inputData; // Full input data on host (rank 0 only)
    std::vector<float> h_finalOutput; // Gathered final output on host (rank 0 only)
    int finalH = -1, finalW = -1, finalC = -1; // Expected final dimensions

    if (rank == 0) {
        std::cout << "--- AlexNet MPI+CUDA (V4 - Host Staging) ---" << std::endl;
        std::cout << "Initializing data and parameters on Rank 0..." << std::endl;

        h_inputData.assign((size_t)H * W * C, 1.0f);
        p1.K = 96; p1.F = 11; p1.S = 4; p1.P = 0;
        p1.F_pool = 3; p1.S_pool = 2;
        if (p1.F % 2 == 0 || p1.F_pool % 2 == 0) { std::cerr << "Warning: Filter sizes should be odd for simpler F/2 halo calculation." << std::endl; }
        p1.weights.assign((size_t)p1.K * C * p1.F * p1.F, 0.01f);
        p1.biases.assign(p1.K, 0.0f);
        p2.K = 256; p2.F = 5; p2.S = 1; p2.P = 2;
        p2.F_pool = 3; p2.S_pool = 2;
        if (p2.F % 2 == 0 || p2.F_pool % 2 == 0) { std::cerr << "Warning: Filter sizes should be odd for simpler F/2 halo calculation." << std::endl; }
        p2.N_lrn = 5; p2.alpha = 1e-4f; p2.beta = 0.75f; p2.k_lrn = 2.0f;
        int C_conv2_input = p1.K;
        p2.weights.assign((size_t)p2.K * C_conv2_input * p2.F * p2.F, 0.01f);
        p2.biases.assign(p2.K, 0.0f);

        int Hc1 = convOutDim(H, p1.F, p1.P, p1.S); int Wc1 = convOutDim(W, p1.F, p1.P, p1.S);
        int Hp1 = poolOutDim(Hc1, p1.F_pool, p1.S_pool); int Wp1 = poolOutDim(Wc1, p1.F_pool, p1.S_pool);
        int Hc2 = convOutDim(Hp1, p2.F, p2.P, p2.S); int Wc2 = convOutDim(Wp1, p2.F, p2.P, p2.S);
        int Hp2 = poolOutDim(Hc2, p2.F_pool, p2.S_pool); int Wp2 = poolOutDim(Wc2, p2.F_pool, p2.S_pool);
        finalH = Hp2; finalW = Wp2; finalC = p2.K;

        if (finalH <= 0 || finalW <= 0 || finalC <=0) { std::cerr << "Error: Calculated final dimensions invalid (" << finalH << "x" << finalW << "x" << finalC << ")." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
        std::cout << "Expected final output dimensions: H=" << finalH << ", W=" << finalW << ", C=" << finalC << std::endl;
        std::cout << "Initialization complete." << std::endl;
    }

    // --- 2. Broadcast essential parameters ---
    MPI_Bcast(&H, 1, MPI_INT, 0, MPI_COMM_WORLD); // *** H is now broadcast ***
    MPI_Bcast(&W, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&C, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p1.K, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p1.F, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p1.S, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p1.P, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p1.F_pool, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p1.S_pool, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p2.K, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p2.F, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p2.S, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p2.P, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p2.F_pool, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&p2.S_pool, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&p2.N_lrn, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&finalH, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&finalW, 1, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(&finalC, 1, MPI_INT, 0, MPI_COMM_WORLD);

    float lrn_params[3];
    if (rank == 0) { lrn_params[0] = p2.alpha; lrn_params[1] = p2.beta; lrn_params[2] = p2.k_lrn; }
    MPI_Bcast(lrn_params, 3, MPI_FLOAT, 0, MPI_COMM_WORLD);
    if (rank != 0) { p2.alpha = lrn_params[0]; p2.beta = lrn_params[1]; p2.k_lrn = lrn_params[2]; }

    size_t w1_size = (size_t)p1.K * C * p1.F * p1.F; size_t b1_size = (size_t)p1.K;
    size_t w2_size = (size_t)p2.K * p1.K * p2.F * p2.F; size_t b2_size = (size_t)p2.K;
    if (rank != 0) { p1.weights.resize(w1_size); p1.biases.resize(b1_size); p2.weights.resize(w2_size); p2.biases.resize(b2_size); }
    if (w1_size > INT_MAX || b1_size > INT_MAX || w2_size > INT_MAX || b2_size > INT_MAX) { if (rank == 0) std::cerr << "Error: Parameter size exceeds INT_MAX." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
    MPI_Bcast(p1.weights.data(), static_cast<int>(w1_size), MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_Bcast(p1.biases.data(), static_cast<int>(b1_size), MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_Bcast(p2.weights.data(), static_cast<int>(w2_size), MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_Bcast(p2.biases.data(), static_cast<int>(b2_size), MPI_FLOAT, 0, MPI_COMM_WORLD);

    // --- 3. Scatter Input Data ---
    std::vector<int> sendCounts(size); std::vector<int> displacements(size);
    std::vector<float> h_localInput; int localH = 0; int rowSize = W * C;
    if (rowSize <= 0 && H > 0 && W > 0 && C > 0) { if (rank == 0) std::cerr << "Error: Calculated rowSize invalid." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
    if (rank == 0) {
        int baseRows = H / size; int extraRows = H % size; int currentDisplacement = 0;
        for (int i = 0; i < size; ++i) {
            int rowsForRank = baseRows + (i < extraRows ? 1 : 0); sendCounts[i] = rowsForRank * rowSize;
            if (rowsForRank > 0 && rowSize > 0 && sendCounts[i] / rowsForRank != rowSize ) { std::cerr << "Error: Scatter count overflow rank " << i << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
            displacements[i] = currentDisplacement;
            if (sendCounts[i] < 0 || currentDisplacement < 0 || currentDisplacement > INT_MAX - sendCounts[i]) { std::cerr << "Error: Scatter displacement overflow rank " << i << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
            currentDisplacement += sendCounts[i];
        }
        if (currentDisplacement != H * W * C && H > 0) { std::cerr << "Error: Total scatter size mismatch." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
    }
    int rowsForMyRank; std::vector<int> rowsPerRank(size);
    if (rank == 0) { int baseRows = H / size; int extraRows = H % size; for(int i=0; i<size; ++i) rowsPerRank[i] = baseRows + (i < extraRows ? 1 : 0); }
    MPI_Scatter(rowsPerRank.data(), 1, MPI_INT, &rowsForMyRank, 1, MPI_INT, 0, MPI_COMM_WORLD);
    localH = rowsForMyRank;
    int localInputSize = localH * rowSize; if (localInputSize < 0) localInputSize = 0;
    if (localInputSize > 0) h_localInput.resize(localInputSize); else h_localInput.clear();
    MPI_Scatterv( (rank == 0 && !h_inputData.empty()) ? h_inputData.data() : nullptr, sendCounts.data(), displacements.data(), MPI_FLOAT, h_localInput.empty() ? nullptr : h_localInput.data(), localInputSize, MPI_FLOAT, 0, MPI_COMM_WORLD);

    // --- 4. Select GPU and Run Forward Pass ---
    int deviceId = 0; int numDevices = 0; cudaError_t cuda_err = cudaGetDeviceCount(&numDevices);
    if (cuda_err != cudaSuccess) { if (rank == 0) fprintf(stderr, "CUDA error cudaGetDeviceCount: %s\n", cudaGetErrorString(cuda_err)); MPI_Abort(MPI_COMM_WORLD, 1); }
    if (numDevices > 0) { deviceId = rank % numDevices; HOST_CUDA_CHECK(cudaSetDevice(deviceId)); /* Print info if needed */ }
    else { if (rank == 0) std::cerr << "Error: No CUDA devices found." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
    std::vector<float> h_localOutput;
    MPI_Barrier(MPI_COMM_WORLD); auto t_start = std::chrono::high_resolution_clock::now();

    // *** UPDATE THE CALL: Pass H ***
    alexnetForwardPassMPI_CUDA(h_localInput, localH, H, W, C, p1, p2, h_localOutput, rank, size);

    MPI_Barrier(MPI_COMM_WORLD); auto t_end = std::chrono::high_resolution_clock::now();

    // --- 5. Gather Final Results ---
    int localOutputSize = static_cast<int>(h_localOutput.size()); if (localOutputSize < 0) localOutputSize = 0;
    std::vector<int> recvCounts(size, 0); std::vector<int> recvDisplacements(size, 0);
    MPI_Gather(&localOutputSize, 1, MPI_INT, recvCounts.empty() ? nullptr : recvCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);
    long long totalFinalSize_ll = 0;
    if (rank == 0) {
        if (recvCounts.empty()) { std::cerr << "Error: recvCounts empty rank 0." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
        recvDisplacements.resize(size); recvDisplacements[0] = 0;
        if (recvCounts[0] < 0) { std::cerr << "Error: Rank 0 neg recv count." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
        totalFinalSize_ll = recvCounts[0];
        for (int i = 1; i < size; ++i) {
            if (recvCounts[i-1] < 0) { std::cerr << "Error: Rank " << i-1 << " neg recv count." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
            long long prevDisp_ll = recvDisplacements[i-1];
            if (prevDisp_ll > INT_MAX - recvCounts[i-1]) { std::cerr << "Error: Gather displ overflow rank " << i << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
            recvDisplacements[i] = prevDisp_ll + recvCounts[i-1];
            if (recvCounts[i] < 0) { std::cerr << "Error: Rank " << i << " neg recv count." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
            totalFinalSize_ll += recvCounts[i];
        }
        if (totalFinalSize_ll > h_finalOutput.max_size()) { std::cerr << "Error: Total gather size exceeds vector max_size." << std::endl; MPI_Abort(MPI_COMM_WORLD, 1); }
        if (totalFinalSize_ll < 0) totalFinalSize_ll = 0;
        h_finalOutput.resize(static_cast<size_t>(totalFinalSize_ll));
    }
    MPI_Gatherv( h_localOutput.empty() ? nullptr : h_localOutput.data(), localOutputSize, MPI_FLOAT, (rank == 0 && !h_finalOutput.empty()) ? h_finalOutput.data() : nullptr, recvCounts.empty() ? nullptr : recvCounts.data(), recvDisplacements.empty() ? nullptr : recvDisplacements.data(), MPI_FLOAT, 0, MPI_COMM_WORLD);

    // --- 6. Final Output and Timing (Rank 0) ---
    if (rank == 0) {
        double duration_ms = std::chrono::duration<double, std::milli>(t_end - t_start).count();
        std::cout << "AlexNet MPI+CUDA Forward Pass completed in " << duration_ms << " ms" << std::endl;
        int expectedTotalSize = finalH * finalW * finalC; size_t gatheredSize = h_finalOutput.size();
        if (finalH > 0) { std::cout << "Final Output Shape: " << finalH << "x" << finalW << "x" << finalC << std::endl; }
        else { std::cout << "Final Output Shape: Calculation resulted in non-positive expected dimensions." << std::endl; }
        if (finalH > 0 && gatheredSize != static_cast<size_t>(expectedTotalSize)) { std::cerr << "WARNING: Gathered size (" << gatheredSize << ") != expected (" << expectedTotalSize << "). Check logic." << std::endl; }
        else if (finalH <= 0) { std::cout << "Gathered total size: " << gatheredSize << " elements." << std::endl; }
        std::cout << "Final Output (first 10 values):";
        size_t num_to_print = std::min((size_t)10, h_finalOutput.size());
        for (size_t i = 0; i < num_to_print; ++i) { std::cout << " " << h_finalOutput[i]; }
        if (h_finalOutput.size() > 10) std::cout << " ...";
        std::cout << std::endl;
    }

    MPI_Finalize();
    return 0;
}

================================================================================

=== FILE: final_project/v4_mpi_cuda/src/alexnet_mpi_cuda.cu ===

// final_project/v4_mpi_cuda/src/alexnet_mpi_cuda.cu
#include <cuda_runtime.h>
#include <mpi.h>
#include <vector>
#include <cmath>
#include <algorithm>
#include <iostream> // For debug prints
#include <cstdio>   // For fprintf, stderr, PRIu64 with cinttypes
#include <cstdlib>  // For exit
#include <climits>  // Include for SIZE_MAX and INT_MAX
#include <cinttypes> // Include for PRIu64 macro
#ifndef SIZE_MAX // Define SIZE_MAX if not defined (e.g., by older standards/compilers)
#define SIZE_MAX ((size_t)-1)
#endif


#include "../include/alexnet.hpp" // Includes LayerParams, dim helpers
#include "../include/layers.hpp"  // Includes CUDA kernel launchers

// Helper macro for checking CUDA errors from .cu files
#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      int rank_for_error; MPI_Comm_rank(MPI_COMM_WORLD, &rank_for_error); \
      fprintf(stderr,                                       \
        "[Rank %d] CUDA error in %s:%d: %s (%d)\n",         \
         rank_for_error, __FILE__, __LINE__, cudaGetErrorString(err), err);  \
      fflush(stderr); /* Ensure message prints before abort */ \
      MPI_Abort(MPI_COMM_WORLD, err); /* Coordinated abort */ \
    }                                                       \
  } while(0)

// Integer ceiling division helper
inline int ceil_div(int numerator, int denominator) {
    if (denominator == 0) return 0; // Or handle error appropriately
    // Ensure calculation handles negative numerator correctly if needed, though unlikely here.
    return (numerator + denominator - 1) / denominator;
}

// Helper to calculate the start index of the output range based on input range start
inline int mapRangeStart(int in_start, int F, int P, int S) {
    if (S <= 0) return 0;
    // out_start = ceil((in_start - F + 1 + P) / S)
    // Need to handle potential negative intermediate value correctly for ceil
    int numerator = in_start - F + 1 + P;
    // Manual ceil for potentially negative numerator: floor((num + den - 1) / den) works for positive den
    // Simpler for positive S: (numerator > 0) ? (numerator + S - 1) / S : numerator / S;
    // Even simpler: use the helper
     return ceil_div(numerator, S);
}

// Helper to calculate the end index of the output range based on input range end
inline int mapRangeEnd(int in_end, int /*F*/, int P, int S) {
     if (S <= 0) return -1; // Indicate invalid range
    // out_end = floor((in_end + P) / S)
    int numerator = in_end + P;
    if (numerator < 0) {
        // Handle negative numerator for floor division if necessary, though unlikely here.
        // Standard integer division truncates towards zero. For floor with negative: (num - den + 1) / den ?
        // Let's assume in_end+P is usually non-negative.
        return (numerator / S) - ( (numerator % S != 0 && numerator < 0) ? 1 : 0); // More robust floor
    } else {
        return numerator / S; // Standard integer division works as floor for non-negative
    }
}


// --- alexnetForwardPassMPI_CUDA Implementation ---
void alexnetForwardPassMPI_CUDA(
    const std::vector<float>& h_localInput, // Local input data slice on host
    int localH,          // Height of the local input slice
    int H, int W, int C, // *** USE PASSED H *** Original Dimensions (H, W, C)
    const LayerParams& p1, // Params for Block 1 (Conv1, Pool1)
    const LayerParams& p2, // Params for Block 2 (Conv2, Pool2, LRN2)
    std::vector<float>& h_localOutput, // Local output slice on host (resized by function)
    int rank, int size   // MPI rank and size for halo logic
) {

    // --- 0. Handle Empty Input Case ---
    if (localH <= 0 || H <= 0 || W <= 0 || C <= 0) {
        h_localOutput.clear();
         // if(rank == 0) fprintf(stderr, "[Rank %d] Info: Skipping computation due to zero input/dimension (localH=%d, H=%d, W=%d, C=%d).\n", rank, localH, H, W, C);
        return;
    }

    // --- 1. Calculate Dimensions & Halo Sizes ---
    int haloRows1 = (p1.F > 1) ? p1.F / 2 : 0;
    int haloRows2 = (p2.F > 1) ? p2.F / 2 : 0;

    bool hasTopHalo = (rank > 0 && haloRows1 > 0);
    bool hasBotHalo = (rank < size - 1 && haloRows1 > 0);
    int paddedH1 = localH + (hasTopHalo ? haloRows1 : 0) + (hasBotHalo ? haloRows1 : 0);

    // Calculate overall dimensions of intermediate layers based on *padded* height
    // These represent the buffer sizes needed.
    int Hc1 = convOutDim(paddedH1, p1.F, p1.P, p1.S);
    int Wc1 = convOutDim(W, p1.F, p1.P, p1.S);
    int Hp1 = poolOutDim(Hc1, p1.F_pool, p1.S_pool);
    int Wp1 = poolOutDim(Wc1, p1.F_pool, p1.S_pool);
    int C1 = p1.K;
    // *** DEBUG PRINT ***
    // fprintf(stderr, "[Rank %d] Calc1: localH=%d, paddedH1=%d, Hc1=%d, Wc1=%d, Hp1=%d, Wp1=%d, C1=%d\n", rank, localH, paddedH1, Hc1, Wc1, Hp1, Wp1, C1);


    // --- Calculate Trimming for Pool1 ---
    // Find the global row indices for this rank's original input data
    int myGlobalStartRow = 0;
    if (size > 0 && H > 0) {
         int baseRows = H / size; int extraRows = H % size;
         for(int r=0; r<rank; ++r) { myGlobalStartRow += baseRows + (r < extraRows ? 1 : 0); }
    } else if (H <= 0) { fprintf(stderr, "[Rank %d] Error: Total height H is invalid (%d).\n", rank, H); MPI_Abort(MPI_COMM_WORLD, 1); }
    int myGlobalEndRow = myGlobalStartRow + localH - 1;
    if (myGlobalEndRow >= H) { fprintf(stderr, "[Rank %d] Error: Calculated global end row (%d) >= total height (%d). localH=%d, startRow=%d\n", rank, myGlobalEndRow, H, localH, myGlobalStartRow); MPI_Abort(MPI_COMM_WORLD, 1); }
    // *** DEBUG PRINT ***
    // fprintf(stderr, "[Rank %d] Global Rows: Start=%d, End=%d (localH=%d)\n", rank, myGlobalStartRow, myGlobalEndRow, localH);


    // Map these global start/end rows through the layers using CORRECT range mapping
    // Map Conv1
    int conv1_start_ho = mapRangeStart(myGlobalStartRow, p1.F, p1.P, p1.S);
    int conv1_end_ho   = mapRangeEnd(myGlobalEndRow, p1.F, p1.P, p1.S);
    // Map Pool1 (using Conv1's output range as input)
    int pool1_start_ho = mapRangeStart(conv1_start_ho, p1.F_pool, 0, p1.S_pool); // Pool P=0
    int pool1_end_ho   = mapRangeEnd(conv1_end_ho, p1.F_pool, 0, p1.S_pool); // Pool P=0

    // Calculate valid height for Pool1 output on this rank
    int validHp1 = (pool1_end_ho >= pool1_start_ho) ? (pool1_end_ho - pool1_start_ho + 1) : 0;

    // Calculate the *local* start index (trimTop1) within the Hp1-sized buffer
    // Need the global start index if rank 0 processed input row 0
    int global_conv1_start_for_row0 = mapRangeStart(0, p1.F, p1.P, p1.S);
    int global_pool1_start_for_row0 = mapRangeStart(global_conv1_start_for_row0, p1.F_pool, 0, p1.S_pool);

    int trimTop1 = pool1_start_ho - global_pool1_start_for_row0;

    // Clamp trimTop1 and validHp1 based on the actual buffer size Hp1
    if (trimTop1 < 0) trimTop1 = 0;
    if (trimTop1 >= Hp1) { // If start offset is beyond buffer size
        trimTop1 = (Hp1 > 0 ? Hp1 : 0);
        validHp1 = 0;
    } else { // If start offset is within buffer, check end
        if (trimTop1 + validHp1 > Hp1) { // If calculated valid region extends beyond buffer
            validHp1 = std::max(0, Hp1 - trimTop1); // Clamp valid height
        }
    }
    if (validHp1 < 0) validHp1 = 0; // Final safety
    // *** DEBUG PRINT ***
    fprintf(stderr, "[Rank %d] Pool1 Trim: Hp1=%d, global_pool1_range=[%d, %d], validHp1=%d, trimTop1=%d\n", rank, Hp1, pool1_start_ho, pool1_end_ho, validHp1, trimTop1);


    // --- Dimensions for Block 2 ---
    bool hasTopHalo2 = (rank > 0 && haloRows2 > 0 && validHp1 > 0);
    bool hasBotHalo2 = (rank < size - 1 && haloRows2 > 0 && validHp1 > 0);
    int paddedH2 = validHp1 + (hasTopHalo2 ? haloRows2 : 0) + (hasBotHalo2 ? haloRows2 : 0);

    // Overall dimensions based on padded height
    int Hc2 = convOutDim(paddedH2, p2.F, p2.P, p2.S);
    int Wc2 = convOutDim(Wp1, p2.F, p2.P, p2.S);
    int Hp2 = poolOutDim(Hc2, p2.F_pool, p2.S_pool);
    int Wp2 = poolOutDim(Wc2, p2.F_pool, p2.S_pool);
    int C2 = p2.K;
    // *** DEBUG PRINT ***
    // fprintf(stderr, "[Rank %d] Calc2: validHp1=%d, paddedH2=%d, Hc2=%d, Wc2=%d, Hp2=%d, Wp2=%d, C2=%d\n", rank, validHp1, paddedH2, Hc2, Wc2, Hp2, Wp2, C2);


    // --- Calculate Trimming for Pool2 ---
    // Map the global Pool1 output range through Conv2 and Pool2
    int global_pool1_start = pool1_start_ho; // Global start index of this rank's valid Pool1 output
    int global_pool1_end = pool1_end_ho;     // Global end index

    // Map Conv2
    int conv2_start_ho = mapRangeStart(global_pool1_start, p2.F, p2.P, p2.S);
    int conv2_end_ho   = mapRangeEnd(global_pool1_end, p2.F, p2.P, p2.S);
    // Map Pool2
    int pool2_start_ho = mapRangeStart(conv2_start_ho, p2.F_pool, 0, p2.S_pool);
    int pool2_end_ho   = mapRangeEnd(conv2_end_ho, p2.F_pool, 0, p2.S_pool);

    // Calculate final valid height for this rank
    int finalLocalH = (pool2_end_ho >= pool2_start_ho) ? (pool2_end_ho - pool2_start_ho + 1) : 0;

    // Calculate trimTop2 relative to the Hp2-sized buffer
    // Need the global start index if rank 0 processed input pool1 row 0 (derived from input row 0)
    int global_conv2_start_for_row0 = mapRangeStart(global_pool1_start_for_row0, p2.F, p2.P, p2.S);
    int global_pool2_start_for_row0 = mapRangeStart(global_conv2_start_for_row0, p2.F_pool, 0, p2.S_pool);

    int trimTop2 = pool2_start_ho - global_pool2_start_for_row0;

    // Clamp trimTop2 and finalLocalH based on actual buffer size Hp2
     if (trimTop2 < 0) trimTop2 = 0;
     if (trimTop2 >= Hp2) {
         trimTop2 = (Hp2 > 0 ? Hp2 : 0);
         finalLocalH = 0;
     } else {
         if (trimTop2 + finalLocalH > Hp2) {
             finalLocalH = std::max(0, Hp2 - trimTop2);
         }
     }
     if (finalLocalH < 0) finalLocalH = 0; // Final safety
     // *** DEBUG PRINT ***
     fprintf(stderr, "[Rank %d] Pool2 Trim: Hp2=%d, global_pool2_range=[%d, %d], finalLocalH=%d, trimTop2=%d\n", rank, Hp2, pool2_start_ho, pool2_end_ho, finalLocalH, trimTop2);


    // --- 2. Allocate Device Memory ---
    // (Allocation code remains the same)
    float *d_input_padded1 = nullptr, *d_conv1_out = nullptr, *d_pool1_out = nullptr,
          *d_input_padded2 = nullptr, *d_conv2_out = nullptr, *d_pool2_out = nullptr,
          *d_lrn2_out = nullptr;
    float *d_weights1 = nullptr, *d_biases1 = nullptr,
          *d_weights2 = nullptr, *d_biases2 = nullptr;

    size_t inputPadded1Size = (size_t)paddedH1 * W * C;
    size_t conv1OutSize = (size_t)Hc1 * Wc1 * C1;
    size_t pool1OutSize = (size_t)Hp1 * Wp1 * C1;
    size_t inputPadded2Size = (size_t)paddedH2 * Wp1 * C1;
    size_t conv2OutSize = (size_t)Hc2 * Wc2 * C2;
    size_t pool2OutSize = (size_t)Hp2 * Wp2 * C2;
    size_t lrn2OutSize = pool2OutSize;

    size_t w1Size = p1.weights.size(); size_t b1Size = p1.biases.size();
    size_t w2Size = p2.weights.size(); size_t b2Size = p2.biases.size();

    if (inputPadded1Size > 0) CUDA_CHECK(cudaMalloc(&d_input_padded1, inputPadded1Size * sizeof(float))); else d_input_padded1=nullptr;
    if (conv1OutSize > 0)     CUDA_CHECK(cudaMalloc(&d_conv1_out, conv1OutSize * sizeof(float)));     else d_conv1_out = nullptr;
    if (pool1OutSize > 0)     CUDA_CHECK(cudaMalloc(&d_pool1_out, pool1OutSize * sizeof(float)));     else d_pool1_out = nullptr;
    if (inputPadded2Size > 0) CUDA_CHECK(cudaMalloc(&d_input_padded2, inputPadded2Size * sizeof(float))); else d_input_padded2 = nullptr;
    if (conv2OutSize > 0)     CUDA_CHECK(cudaMalloc(&d_conv2_out, conv2OutSize * sizeof(float)));     else d_conv2_out = nullptr;
    if (pool2OutSize > 0)     CUDA_CHECK(cudaMalloc(&d_pool2_out, pool2OutSize * sizeof(float)));     else d_pool2_out = nullptr;
    if (lrn2OutSize > 0)      CUDA_CHECK(cudaMalloc(&d_lrn2_out, lrn2OutSize * sizeof(float)));      else d_lrn2_out = nullptr;

    if (w1Size > 0) CUDA_CHECK(cudaMalloc(&d_weights1, w1Size * sizeof(float))); else d_weights1 = nullptr;
    if (b1Size > 0) CUDA_CHECK(cudaMalloc(&d_biases1, b1Size * sizeof(float))); else d_biases1 = nullptr;
    if (w2Size > 0) CUDA_CHECK(cudaMalloc(&d_weights2, w2Size * sizeof(float))); else d_weights2 = nullptr;
    if (b2Size > 0) CUDA_CHECK(cudaMalloc(&d_biases2, b2Size * sizeof(float))); else d_biases2 = nullptr;


    // --- 3. Copy Initial Data Host -> Device ---
    // (Copy code remains the same, including bounds checks)
    if (w1Size > 0 && !p1.weights.empty() && d_weights1) CUDA_CHECK(cudaMemcpy(d_weights1, p1.weights.data(), w1Size * sizeof(float), cudaMemcpyHostToDevice));
    if (b1Size > 0 && !p1.biases.empty() && d_biases1)   CUDA_CHECK(cudaMemcpy(d_biases1, p1.biases.data(), b1Size * sizeof(float), cudaMemcpyHostToDevice));
    if (w2Size > 0 && !p2.weights.empty() && d_weights2) CUDA_CHECK(cudaMemcpy(d_weights2, p2.weights.data(), w2Size * sizeof(float), cudaMemcpyHostToDevice));
    if (b2Size > 0 && !p2.biases.empty() && d_biases2)   CUDA_CHECK(cudaMemcpy(d_biases2, p2.biases.data(), b2Size * sizeof(float), cudaMemcpyHostToDevice));

    size_t localInputSizeBytes = h_localInput.size() * sizeof(float);
    size_t inputRowSizeBytes = (W > 0 && C > 0) ? (size_t)W * C * sizeof(float) : 0;
    size_t inputHalo1SizeBytes = (size_t)haloRows1 * inputRowSizeBytes;
    size_t inputOffsetElements = (hasTopHalo ? inputHalo1SizeBytes : 0) / sizeof(float);

    if (localInputSizeBytes > 0 && d_input_padded1 != nullptr) {
        if (inputOffsetElements * sizeof(float) + localInputSizeBytes > inputPadded1Size * sizeof(float)) {
             fprintf(stderr, "[Rank %d] Error: Initial H->D copy exceeds destination buffer bounds (Offset=%" PRIu64 ", Size=%" PRIu64 ", BufferSize=%" PRIu64 ").\n", rank, (uint64_t)(inputOffsetElements*sizeof(float)), (uint64_t)localInputSizeBytes, (uint64_t)(inputPadded1Size*sizeof(float)) ); MPI_Abort(MPI_COMM_WORLD, 1);
        }
        CUDA_CHECK(cudaMemcpy(d_input_padded1 + inputOffsetElements, h_localInput.data(), localInputSizeBytes, cudaMemcpyHostToDevice));
    }


    // --- 4. Halo Exchange for Conv1 ---
    // (Halo exchange code remains the same, including skip_halo1 label and bounds checks)
    if (haloRows1 > 0 && inputRowSizeBytes > 0) {
        MPI_Request send_req_up = MPI_REQUEST_NULL, recv_req_up = MPI_REQUEST_NULL;
        MPI_Request send_req_down = MPI_REQUEST_NULL, recv_req_down = MPI_REQUEST_NULL;
        std::vector<float> h_send_buffer_up(haloRows1 * W * C);
        std::vector<float> h_recv_buffer_up(haloRows1 * W * C);
        std::vector<float> h_send_buffer_down(haloRows1 * W * C);
        std::vector<float> h_recv_buffer_down(haloRows1 * W * C);

        int partner_up = rank - 1; int partner_down = rank + 1;
        int tag_up = 0; int tag_down = 1;
        int haloElementCount = inputHalo1SizeBytes / sizeof(float);
        if (haloElementCount <=0) goto skip_halo1;

        //fprintf(stderr, "[Rank %d] Halo1: Exchanging %d elements (haloRows1=%d).\n", rank, haloElementCount, haloRows1);

        if (hasTopHalo) { MPI_Irecv(h_recv_buffer_up.data(), haloElementCount, MPI_FLOAT, partner_up, tag_up, MPI_COMM_WORLD, &recv_req_up); }
        if (hasBotHalo) { MPI_Irecv(h_recv_buffer_down.data(), haloElementCount, MPI_FLOAT, partner_down, tag_down, MPI_COMM_WORLD, &recv_req_down); }

        if (hasBotHalo && d_input_padded1 != nullptr) {
            size_t sendOffsetElements = inputOffsetElements + (localInputSizeBytes / sizeof(float)) - haloElementCount;
            if (sendOffsetElements * sizeof(float) >= 0 && sendOffsetElements + haloElementCount <= inputPadded1Size) { // Check non-negative offset and bounds
                 //fprintf(stderr, "[Rank %d] Halo1 Send Up D->H: Offset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)sendOffsetElements, (uint64_t)haloElementCount);
                 CUDA_CHECK(cudaMemcpy(h_send_buffer_up.data(), d_input_padded1 + sendOffsetElements, inputHalo1SizeBytes, cudaMemcpyDeviceToHost));
                 MPI_Isend(h_send_buffer_up.data(), haloElementCount, MPI_FLOAT, partner_down, tag_up, MPI_COMM_WORLD, &send_req_up);
            } else { fprintf(stderr, "[Rank %d] Error: Halo1 send up offset invalid or out of bounds (Offset=%" PRId64 ", Count=%d, Size=%" PRIu64").\n", rank, (int64_t)sendOffsetElements, haloElementCount, (uint64_t)inputPadded1Size); MPI_Abort(MPI_COMM_WORLD, 1); }
        }
         if (hasTopHalo && d_input_padded1 != nullptr) {
             if (inputOffsetElements * sizeof(float) >= 0 && inputOffsetElements + haloElementCount <= inputPadded1Size) { // Check non-negative offset and bounds
                  //fprintf(stderr, "[Rank %d] Halo1 Send Down D->H: Offset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)inputOffsetElements, (uint64_t)haloElementCount);
                  CUDA_CHECK(cudaMemcpy(h_send_buffer_down.data(), d_input_padded1 + inputOffsetElements, inputHalo1SizeBytes, cudaMemcpyDeviceToHost));
                  MPI_Isend(h_send_buffer_down.data(), haloElementCount, MPI_FLOAT, partner_up, tag_down, MPI_COMM_WORLD, &send_req_down);
             } else { fprintf(stderr, "[Rank %d] Error: Halo1 send down offset invalid or out of bounds (Offset=%" PRId64 ", Count=%d, Size=%" PRIu64").\n", rank, (int64_t)inputOffsetElements, haloElementCount, (uint64_t)inputPadded1Size); MPI_Abort(MPI_COMM_WORLD, 1); }
         }

        MPI_Status status;
        if (hasTopHalo) {
            MPI_Wait(&recv_req_up, &status);
             if (d_input_padded1 != nullptr && inputHalo1SizeBytes > 0) {
                 if (inputHalo1SizeBytes > inputPadded1Size * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo1 receive H->D dest buffer too small (top).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
                 //fprintf(stderr, "[Rank %d] Halo1 Recv Up H->D: DestOffset=0, Count=%" PRIu64 "\n", rank, (uint64_t)haloElementCount);
                 CUDA_CHECK(cudaMemcpy(d_input_padded1, h_recv_buffer_up.data(), inputHalo1SizeBytes, cudaMemcpyHostToDevice));
             }
             if (send_req_down != MPI_REQUEST_NULL) MPI_Wait(&send_req_down, &status);
        }
        if (hasBotHalo) {
            MPI_Wait(&recv_req_down, &status);
            size_t bottomHaloOffsetElements = inputOffsetElements + (localInputSizeBytes / sizeof(float));
             if (d_input_padded1 != nullptr && inputHalo1SizeBytes > 0 && bottomHaloOffsetElements + haloElementCount <= inputPadded1Size) {
                  //fprintf(stderr, "[Rank %d] Halo1 Recv Down H->D: DestOffset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)bottomHaloOffsetElements, (uint64_t)haloElementCount);
                  CUDA_CHECK(cudaMemcpy(d_input_padded1 + bottomHaloOffsetElements, h_recv_buffer_down.data(), inputHalo1SizeBytes, cudaMemcpyHostToDevice));
             } else if (inputHalo1SizeBytes > 0) { fprintf(stderr, "[Rank %d] Error: Halo1 receive down offset out of bounds or null buffer (Offset=%" PRId64 ", Count=%d, Size=%" PRIu64").\n", rank, (int64_t)bottomHaloOffsetElements, haloElementCount, (uint64_t)inputPadded1Size); MPI_Abort(MPI_COMM_WORLD, 1); }
             if (send_req_up != MPI_REQUEST_NULL) MPI_Wait(&send_req_up, &status);
        }
    }
skip_halo1:;


    // --- 5. Execute Block 1 Kernels ---
     if (Hc1 > 0 && Wc1 > 0 && C1 > 0 && d_conv1_out && d_input_padded1 && d_weights1 && d_biases1) {
         //fprintf(stderr, "[Rank %d] Launching Conv1: H=%d, W=%d, C=%d -> K=%d, F=%d, S=%d, P=%d\n", rank, paddedH1, W, C, p1.K, p1.F, p1.S, p1.P);
         cudaConvLayer(d_conv1_out, d_input_padded1, d_weights1, d_biases1, paddedH1, W, C, p1.K, p1.F, p1.S, p1.P);
         cudaReluLayer(d_conv1_out, conv1OutSize);
     }
     if (Hp1 > 0 && Wp1 > 0 && C1 > 0 && d_pool1_out && d_conv1_out) {
         //fprintf(stderr, "[Rank %d] Launching Pool1: H=%d, W=%d, C=%d -> Fp=%d, Sp=%d\n", rank, Hc1, Wc1, C1, p1.F_pool, p1.S_pool);
         cudaMaxPoolLayer(d_pool1_out, d_conv1_out, Hc1, Wc1, C1, p1.F_pool, p1.S_pool);
     }


    // --- 6. Halo Exchange for Conv2 ---
    size_t pool1RowSizeBytes = (Wp1 > 0 && C1 > 0) ? (size_t)Wp1 * C1 * sizeof(float) : 0;
    if (haloRows2 > 0 && validHp1 > 0 && pool1RowSizeBytes > 0 && d_pool1_out) {
        MPI_Request send_req_up = MPI_REQUEST_NULL, recv_req_up = MPI_REQUEST_NULL;
        MPI_Request send_req_down = MPI_REQUEST_NULL, recv_req_down = MPI_REQUEST_NULL;

        size_t pool1HaloSizeBytes = (size_t)haloRows2 * pool1RowSizeBytes;
        if (haloRows2 > 0 && pool1RowSizeBytes > SIZE_MAX / (unsigned long)haloRows2) { fprintf(stderr, "[Rank %d] Error: pool1HaloSizeBytes calculation overflow.\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
        if (pool1HaloSizeBytes == 0) goto skip_halo2;

        std::vector<float> h_send_buffer_up(pool1HaloSizeBytes / sizeof(float));
        std::vector<float> h_recv_buffer_up(pool1HaloSizeBytes / sizeof(float));
        std::vector<float> h_send_buffer_down(pool1HaloSizeBytes / sizeof(float));
        std::vector<float> h_recv_buffer_down(pool1HaloSizeBytes / sizeof(float));

        int partner_up = rank - 1; int partner_down = rank + 1;
        int tag_up = 2; int tag_down = 3;
        int haloElementCount = pool1HaloSizeBytes / sizeof(float);
        if (haloElementCount <= 0) goto skip_halo2;

        //fprintf(stderr, "[Rank %d] Halo2: Exchanging %d elements (haloRows2=%d).\n", rank, haloElementCount, haloRows2);

        size_t pool1ValidDataOffsetBytes = (size_t)trimTop1 * pool1RowSizeBytes;
        if (pool1ValidDataOffsetBytes > pool1OutSize * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 trimTop1 offset (%" PRIu64 ") out of bounds (%" PRIu64 ").\n", rank, (uint64_t)pool1ValidDataOffsetBytes, (uint64_t)(pool1OutSize * sizeof(float))); MPI_Abort(MPI_COMM_WORLD, 1); }
        float* d_pool1_valid_start = d_pool1_out + pool1ValidDataOffsetBytes / sizeof(float);
        size_t pool1ValidDataSizeBytes = (size_t)validHp1 * pool1RowSizeBytes;
         if (pool1ValidDataOffsetBytes + pool1ValidDataSizeBytes > pool1OutSize * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 valid data region (offset %" PRIu64 ", size %" PRIu64 ") exceeds buffer bounds (%" PRIu64 ").\n", rank, (uint64_t)pool1ValidDataOffsetBytes, (uint64_t)pool1ValidDataSizeBytes, (uint64_t)(pool1OutSize * sizeof(float))); MPI_Abort(MPI_COMM_WORLD, 1); }

        if (hasTopHalo2) { MPI_Irecv(h_recv_buffer_up.data(), haloElementCount, MPI_FLOAT, partner_up, tag_up, MPI_COMM_WORLD, &recv_req_up); }
        if (hasBotHalo2) { MPI_Irecv(h_recv_buffer_down.data(), haloElementCount, MPI_FLOAT, partner_down, tag_down, MPI_COMM_WORLD, &recv_req_down); }

        if (hasBotHalo2) { // Send my bottom rows up
            size_t sendOffsetBytes = pool1ValidDataSizeBytes - pool1HaloSizeBytes;
             if (pool1HaloSizeBytes > pool1ValidDataSizeBytes) { // || sendOffsetBytes > pool1ValidDataSizeBytes // Offset check implicit in next check
                 fprintf(stderr, "[Rank %d] Error: Halo2 send offset calculation error (bottom). ValidSize=%" PRIu64 ", HaloSize=%" PRIu64 "\n", rank, (uint64_t)pool1ValidDataSizeBytes, (uint64_t)pool1HaloSizeBytes); MPI_Abort(MPI_COMM_WORLD, 1);
             }
             if (pool1ValidDataOffsetBytes + sendOffsetBytes + pool1HaloSizeBytes > pool1OutSize * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 send D->H source out of bounds (bottom).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
            //fprintf(stderr, "[Rank %d] Halo2 Send Up D->H: Offset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)(pool1ValidDataOffsetBytes/sizeof(float) + sendOffsetBytes/sizeof(float)), (uint64_t)haloElementCount);
            CUDA_CHECK(cudaMemcpy(h_send_buffer_up.data(), d_pool1_valid_start + sendOffsetBytes / sizeof(float), pool1HaloSizeBytes, cudaMemcpyDeviceToHost));
            MPI_Isend(h_send_buffer_up.data(), haloElementCount, MPI_FLOAT, partner_down, tag_up, MPI_COMM_WORLD, &send_req_up);
        }
         if (hasTopHalo2) { // Send my top rows down
              if (pool1HaloSizeBytes > pool1ValidDataSizeBytes) { fprintf(stderr, "[Rank %d] Error: Halo2 send D->H source size error (top - halo > valid). ValidSize=%" PRIu64 ", HaloSize=%" PRIu64 "\n", rank, (uint64_t)pool1ValidDataSizeBytes, (uint64_t)pool1HaloSizeBytes); MPI_Abort(MPI_COMM_WORLD, 1); }
               if (pool1ValidDataOffsetBytes + pool1HaloSizeBytes > pool1OutSize * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 send D->H source out of bounds (top - offset+halo > total).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
             //fprintf(stderr, "[Rank %d] Halo2 Send Down D->H: Offset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)(pool1ValidDataOffsetBytes/sizeof(float)), (uint64_t)haloElementCount);
             CUDA_CHECK(cudaMemcpy(h_send_buffer_down.data(), d_pool1_valid_start, pool1HaloSizeBytes, cudaMemcpyDeviceToHost));
             MPI_Isend(h_send_buffer_down.data(), haloElementCount, MPI_FLOAT, partner_up, tag_down, MPI_COMM_WORLD, &send_req_down);
         }

        size_t input2CurrentOffsetBytes = 0;
        if (hasTopHalo2) {
             MPI_Wait(&recv_req_up, MPI_STATUS_IGNORE);
             if (d_input_padded2 != nullptr && pool1HaloSizeBytes > 0 && pool1HaloSizeBytes <= inputPadded2Size * sizeof(float)) {
                  //fprintf(stderr, "[Rank %d] Halo2 Recv Up H->D: DestOffset=0, Count=%" PRIu64 "\n", rank, (uint64_t)haloElementCount);
                  CUDA_CHECK(cudaMemcpy(d_input_padded2, h_recv_buffer_up.data(), pool1HaloSizeBytes, cudaMemcpyHostToDevice));
             } else if (pool1HaloSizeBytes > 0) { fprintf(stderr, "[Rank %d] Error: Halo2 receive H->D dest out of bounds or null buffer (top).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
             input2CurrentOffsetBytes += pool1HaloSizeBytes;
             if (send_req_down != MPI_REQUEST_NULL) MPI_Wait(&send_req_down, MPI_STATUS_IGNORE);
        }
        if (validHp1 > 0 && pool1ValidDataSizeBytes > 0 && d_input_padded2 != nullptr) {
            if (input2CurrentOffsetBytes + pool1ValidDataSizeBytes > inputPadded2Size * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 D->D copy destination out of bounds. Offset=%" PRIu64 ", CopySize=%" PRIu64 ", BufferSize=%" PRIu64 "\n", rank, (uint64_t)input2CurrentOffsetBytes, (uint64_t)pool1ValidDataSizeBytes, (uint64_t)(inputPadded2Size * sizeof(float))); MPI_Abort(MPI_COMM_WORLD, 1); }
             //fprintf(stderr, "[Rank %d] Halo2 Copy Valid D->D: DestOffset=%" PRIu64 ", SrcOffset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)(input2CurrentOffsetBytes / sizeof(float)), (uint64_t)(pool1ValidDataOffsetBytes / sizeof(float)), (uint64_t)(pool1ValidDataSizeBytes / sizeof(float)));
             CUDA_CHECK(cudaMemcpy(d_input_padded2 + input2CurrentOffsetBytes / sizeof(float), d_pool1_valid_start, pool1ValidDataSizeBytes, cudaMemcpyDeviceToDevice));
             input2CurrentOffsetBytes += pool1ValidDataSizeBytes;
        }
        if (hasBotHalo2) {
            MPI_Wait(&recv_req_down, MPI_STATUS_IGNORE);
             if (d_input_padded2 != nullptr && pool1HaloSizeBytes > 0 && input2CurrentOffsetBytes + pool1HaloSizeBytes <= inputPadded2Size * sizeof(float)) {
                  //fprintf(stderr, "[Rank %d] Halo2 Recv Down H->D: DestOffset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)(input2CurrentOffsetBytes / sizeof(float)), (uint64_t)haloElementCount);
                  CUDA_CHECK(cudaMemcpy(d_input_padded2 + input2CurrentOffsetBytes / sizeof(float), h_recv_buffer_down.data(), pool1HaloSizeBytes, cudaMemcpyHostToDevice));
             } else if (pool1HaloSizeBytes > 0) { fprintf(stderr, "[Rank %d] Error: Halo2 receive H->D dest out of bounds or null buffer (bottom).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
             if (send_req_up != MPI_REQUEST_NULL) MPI_Wait(&send_req_up, MPI_STATUS_IGNORE);
        }

    } else if (validHp1 > 0 && pool1RowSizeBytes > 0 && d_pool1_out && d_input_padded2) {
        // Case: No halo exchange needed, copy valid data directly D->D
        size_t pool1ValidDataOffsetBytes = (size_t)trimTop1 * pool1RowSizeBytes;
        float* d_pool1_valid_start = d_pool1_out + pool1ValidDataOffsetBytes / sizeof(float);
        size_t pool1ValidDataSizeBytes = (size_t)validHp1 * pool1RowSizeBytes;
         if (pool1ValidDataOffsetBytes + pool1ValidDataSizeBytes > pool1OutSize * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 D->D source bounds error (no halo case).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
         if (pool1ValidDataSizeBytes > inputPadded2Size * sizeof(float)) { fprintf(stderr, "[Rank %d] Error: Halo2 D->D destination bounds error (no halo case).\n", rank); MPI_Abort(MPI_COMM_WORLD, 1); }
        if (pool1ValidDataSizeBytes > 0) {
            //fprintf(stderr, "[Rank %d] Halo2 Copy Valid D->D (No Halo): DestOffset=0, SrcOffset=%" PRIu64 ", Count=%" PRIu64 "\n", rank, (uint64_t)(pool1ValidDataOffsetBytes / sizeof(float)), (uint64_t)(pool1ValidDataSizeBytes / sizeof(float)));
            CUDA_CHECK(cudaMemcpy(d_input_padded2, d_pool1_valid_start, pool1ValidDataSizeBytes, cudaMemcpyDeviceToDevice));
        }
    } else if (d_input_padded2 && inputPadded2Size > 0) {
        // Case: No valid input data, zero out the buffer for layer 2
        //fprintf(stderr, "[Rank %d] Halo2: Zeroing d_input_padded2 (size %" PRIu64 ").\n", rank, (uint64_t)inputPadded2Size);
        CUDA_CHECK(cudaMemset(d_input_padded2, 0, inputPadded2Size * sizeof(float)));
    }
skip_halo2:;


    // --- 7. Execute Block 2 Kernels ---
     if (Hc2 > 0 && Wc2 > 0 && C2 > 0 && d_conv2_out && d_input_padded2 && d_weights2 && d_biases2) {
        //fprintf(stderr, "[Rank %d] Launching Conv2: H=%d, W=%d, C=%d -> K=%d, F=%d, S=%d, P=%d\n", rank, paddedH2, Wp1, C1, p2.K, p2.F, p2.S, p2.P);
        cudaConvLayer(d_conv2_out, d_input_padded2, d_weights2, d_biases2, paddedH2, Wp1, C1, p2.K, p2.F, p2.S, p2.P);
        cudaReluLayer(d_conv2_out, conv2OutSize);
    }
     if (Hp2 > 0 && Wp2 > 0 && C2 > 0 && d_pool2_out && d_conv2_out) {
        //fprintf(stderr, "[Rank %d] Launching Pool2: H=%d, W=%d, C=%d -> Fp=%d, Sp=%d\n", rank, Hc2, Wc2, C2, p2.F_pool, p2.S_pool);
        cudaMaxPoolLayer(d_pool2_out, d_conv2_out, Hc2, Wc2, C2, p2.F_pool, p2.S_pool);
     }
     if (Hp2 > 0 && Wp2 > 0 && C2 > 0 && d_lrn2_out && d_pool2_out) {
        //fprintf(stderr, "[Rank %d] Launching LRN2: H=%d, W=%d, C=%d\n", rank, Hp2, Wp2, C2);
        cudaLRNLayer(d_lrn2_out, d_pool2_out, Hp2, Wp2, C2, p2.N_lrn, p2.alpha, p2.beta, p2.k_lrn);
     }


    // --- 8. Copy Final Result Device -> Host ---
    size_t finalRowSizeBytes = (Wp2 > 0 && C2 > 0) ? (size_t)Wp2 * C2 * sizeof(float) : 0;
    size_t finalLocalSizeBytes = (size_t)finalLocalH * finalRowSizeBytes;

    if (finalLocalSizeBytes > 0 && finalLocalH > 0) {
         // Check for potential overflow before resize
        if ((unsigned long)finalLocalH > SIZE_MAX / ((unsigned long)Wp2*C2) ) {
             fprintf(stderr, "[Rank %d] Error: Final output size calculation overflow before resize.\n", rank); MPI_Abort(MPI_COMM_WORLD, 1);
        }
         h_localOutput.resize(finalLocalSizeBytes / sizeof(float));
    } else {
         h_localOutput.clear();
         finalLocalSizeBytes = 0;
    }

    if (finalLocalH > 0 && finalLocalSizeBytes > 0 && d_lrn2_out != nullptr) {
        size_t finalDataOffsetBytes = (size_t)trimTop2 * finalRowSizeBytes;
        if (finalDataOffsetBytes + finalLocalSizeBytes > lrn2OutSize * sizeof(float)) {
             fprintf(stderr, "[Rank %d] Error: Final D->H copy source (offset %" PRIu64 ", size %" PRIu64 ") out of bounds (%" PRIu64 "). finalLocalH=%d, trimTop2=%d, Hp2=%d, Wp2=%d, C2=%d\n", rank, (uint64_t)finalDataOffsetBytes, (uint64_t)finalLocalSizeBytes, (uint64_t)(lrn2OutSize * sizeof(float)), finalLocalH, trimTop2, Hp2, Wp2, C2); MPI_Abort(MPI_COMM_WORLD, 1);
          }
        //fprintf(stderr, "[Rank %d] Final D->H: DestSize=%" PRIu64 ", SrcOffset=%" PRIu64 ", CopySize=%" PRIu64 "\n", rank, (uint64_t)h_localOutput.size(), (uint64_t)(finalDataOffsetBytes / sizeof(float)), (uint64_t)(finalLocalSizeBytes / sizeof(float)));
        CUDA_CHECK(cudaMemcpy(h_localOutput.data(), d_lrn2_out + finalDataOffsetBytes / sizeof(float), finalLocalSizeBytes, cudaMemcpyDeviceToHost));
    }

    // --- 9. Free Device Memory ---
    if (d_input_padded1) cudaFree(d_input_padded1);
    if (d_conv1_out)     cudaFree(d_conv1_out);
    if (d_pool1_out)     cudaFree(d_pool1_out);
    if (d_input_padded2) cudaFree(d_input_padded2);
    if (d_conv2_out)     cudaFree(d_conv2_out);
    if (d_pool2_out)     cudaFree(d_pool2_out);
    if (d_lrn2_out)      cudaFree(d_lrn2_out);
    if (d_weights1)      cudaFree(d_weights1);
    if (d_biases1)       cudaFree(d_biases1);
    if (d_weights2)      cudaFree(d_weights2);
    if (d_biases2)       cudaFree(d_biases2);

} // End of alexnetForwardPassMPI_CUDA definition

================================================================================

=== FILE: final_project/v4_mpi_cuda/src/layers_mpi_cuda.cu ===

#include <cstdio>         // For fprintf, stderr in CUDA_CHECK
#include <cstdlib>        // For exit in CUDA_CHECK
#include <cmath>          // For fmaxf, powf, max, min
#include <cuda_runtime.h>
#include <mpi.h>          // Needed for MPI_Abort in CUDA_CHECK
#include <cstddef>        // Include for size_t

#include "../include/layers.hpp" // Function prototypes being implemented

// Added definitions for convOutDim and poolOutDim for CUDA kernels
__host__ __device__ inline int convOutDim(int D, int F, int P, int S) {
    return (D + 2 * P - F) / S + 1;
}

__host__ __device__ inline int poolOutDim(int D, int F, int S) {
    return (D - F) / S + 1;
}

// Macro to check CUDA calls - ABORTS using MPI for coordinated shutdown
#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      int rank_for_error; MPI_Comm_rank(MPI_COMM_WORLD, &rank_for_error); \
      fprintf(stderr,                                       \
        "[Rank %d] CUDA error in %s:%d: %s (%d)\n",         \
         rank_for_error, __FILE__, __LINE__, cudaGetErrorString(err), err); \
      fflush(stderr); /* Ensure message prints before abort */ \
      MPI_Abort(MPI_COMM_WORLD, err);                       \
    }                                                       \
  } while(0)

// --- Helper Functions (Device) ---
// Optional: Define __device__ helpers if needed by kernels, e.g., index calculation

// --- Kernel Implementations ---
// These kernels are identical to V3, with the pooling index fix

// Conv kernel (one thread per output element: Ho * Wo * K)
__global__ void convKernel(
    float* __restrict__ out, const float* __restrict__ in,
    const float* __restrict__ w, const float* __restrict__ b,
    int H, int W, int C, int K, int F, int S, int P,
    int Ho, int Wo)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_outputs = Ho * Wo * K;

    if (idx >= total_outputs) return;

    // Calculate output coordinates (k, y, x) from global thread index
    int k = idx % K;          // Output channel index
    int temp = idx / K;
    int x = temp % Wo;        // Output column index
    int y = temp / Wo;        // Output row index

    float sum = 0.0f; // Use float for intermediate sum

    // Corresponding input top-left corner coordinates
    int start_y = y * S - P;
    int start_x = x * S - P;

    // Iterate over input channels and filter dimensions
    for (int c = 0; c < C; ++c) {
        for (int fy = 0; fy < F; ++fy) {
            int current_y = start_y + fy;
            for (int fx = 0; fx < F; ++fx) {
                int current_x = start_x + fx;

                // Check bounds: only process valid input pixels
                if (current_y >= 0 && current_y < H && current_x >= 0 && current_x < W) {
                    // Calculate 1D indices
                    // Input index: (batch=0 implicitly) height, width, channel
                    // Use size_t for intermediate calculations to avoid overflow with large dimensions
                    size_t in_idx = ((size_t)current_y * W + current_x) * C + c;
                    // Weight index: output_channel, input_channel, filter_y, filter_x
                    size_t w_idx = (((size_t)k * C + c) * F + fy) * F + fx;

                    // Accumulate product
                    sum += in[in_idx] * w[w_idx];
                }
                // Pixels outside bounds (due to padding) contribute 0, so no `else` needed
            }
        }
    }

    // Add bias (once per output element) and store result
    out[idx] = sum + b[k];
}


// ReLU kernel (elementwise, in-place)
__global__ void reluKernel(float* data, size_t N) {
    size_t i = static_cast<size_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if (i < N) {
        data[i] = fmaxf(0.0f, data[i]);
    }
}

// Max-pooling kernel (one thread per output element: Hp * Wp * C)
__global__ void poolKernel(
    float* __restrict__ out, const float* __restrict__ in,
    int H, int W, int C,  // Input dimensions
    int Fp, int Sp,       // Pooling parameters (Filter size, Stride)
    int Hp, int Wp)       // Output dimensions
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_outputs = Hp * Wp * C;

    if (idx >= total_outputs) return;

    // Calculate output coordinates (c, y, x)
    int c = idx % C;
    int temp = idx / C;
    // *** Corrected y calculation ***
    int y = temp / Wp; // Integer division gives row index
    int x = temp % Wp; // Modulo gives column index


    // Find top-left corner of the pooling window in the input
    int start_y = y * Sp;
    int start_x = x * Sp;

    float max_val = -INFINITY; // Initialize with negative infinity

    // Iterate over the pooling window (Fp x Fp)
    for (int fy = 0; fy < Fp; ++fy) {
        int current_y = start_y + fy;
        // Ensure we don't go out of input bounds vertically
        if (current_y >= H) continue;

        for (int fx = 0; fx < Fp; ++fx) {
            int current_x = start_x + fx;
             // Ensure we don't go out of input bounds horizontally
             if (current_x >= W) continue;

            // Calculate 1D index in the input tensor
            // Use size_t for intermediate calculations
            size_t in_idx = ((size_t)current_y * W + current_x) * C + c;

            // Update maximum value
            max_val = fmaxf(max_val, in[in_idx]);
        }
    }

    // Store the maximum value found in the output tensor
    out[idx] = max_val;
}


// LRN kernel (naive cross-channel)
__global__ void lrnKernel(
    float* __restrict__ out, const float* __restrict__ in,
    int H, int W, int C, // Input dimensions
    int N_lrn, float alpha, float beta, float k) // LRN parameters
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = H * W * C;

    if (idx >= total_elements) return;

    // Calculate coordinates (c, y, x)
    int c = idx % C;
    int temp = idx / C;
    int x = temp % W;
    int y = temp / W;

    // Calculate the sum of squares within the local neighborhood across channels
    float sum_sq = 0.0f;
    int half_N = N_lrn / 2;
    int c_start = max(0, c - half_N);
    int c_end = min(C - 1, c + half_N); // Inclusive end

    for (int channel_i = c_start; channel_i <= c_end; ++channel_i) {
         // Use size_t for intermediate calculations
        size_t neighbor_idx = ((size_t)y * W + x) * C + channel_i;
        float val = in[neighbor_idx];
        sum_sq += val * val;
    }

    // Calculate the normalization factor
    // Using the common implementation approach (alpha not divided by N)
     float scale = k + alpha * sum_sq;

    // Apply normalization: out = in / (scale ^ beta)
    // Avoid division by zero or negative scale if beta is fractional
    if (scale <= 0.0f && beta > 0.0f && beta != 1.0f) {
         // Handle error case: e.g., set output to 0 or clamp scale
         out[idx] = 0.0f; // Or potentially in[idx] depending on desired behavior
    } else {
         out[idx] = in[idx] * powf(scale, -beta); // Use powf for float exponentiation
    }
}


// --- Kernel Launcher Functions ---
// These are host functions that set up grid/block dimensions and launch kernels.
// ** Function Definitions **

// Definition for cudaConvLayer
void cudaConvLayer(
    float* d_output,
    const float* d_input,
    const float* d_weights,
    const float* d_biases,
    int H, int W, int C,
    const int K, const int F, const int S, const int P)
{
    // Calculate output dimensions using helper (robust check)
    int Ho = convOutDim(H, F, P, S);
    int Wo = convOutDim(W, F, P, S);

    if (Ho <= 0 || Wo <= 0 || K <= 0) {
         // Output dimensions are zero or negative, or no filters. Nothing to compute.
         // Ensure output buffer is zeroed if necessary, or just return.
         // If d_output was allocated based on these dims, it might be nullptr or size 0.
         // Optionally print a warning:
         // fprintf(stderr, "Warning: ConvLayer output size is zero or negative (Ho=%d, Wo=%d, K=%d). Skipping kernel launch.\n", Ho, Wo, K);
         return;
     }

    int total_outputs = Ho * Wo * K;
    int threads_per_block = 256; // Common choice, tune based on GPU architecture
    // Calculate grid size, ensuring it covers all outputs
    int blocks_per_grid = (total_outputs + threads_per_block - 1) / threads_per_block;

    convKernel<<<blocks_per_grid, threads_per_block>>>(
        d_output, d_input, d_weights, d_biases,
        H, W, C, K, F, S, P, Ho, Wo);
    CUDA_CHECK(cudaGetLastError()); // Check for launch configuration errors
    // Optional: Synchronize if needed immediately after kernel
    // CUDA_CHECK(cudaDeviceSynchronize());
}

// Definition for cudaReluLayer
void cudaReluLayer(float* d_data, size_t N) {
     if (N == 0 || d_data == nullptr) return; // Nothing to do
    int threads_per_block = 256;
    // Use size_t for N, cast carefully for grid calculation if N is very large
    // Standard approach assumes N fits within reasonable limits for grid calculation
    int blocks_per_grid = (N + threads_per_block - 1) / threads_per_block;

    reluKernel<<<blocks_per_grid, threads_per_block>>>(d_data, N);
    CUDA_CHECK(cudaGetLastError());
    // CUDA_CHECK(cudaDeviceSynchronize());
}

// Definition for cudaMaxPoolLayer
void cudaMaxPoolLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int F_pool, int S_pool)
{
     // Calculate output dimensions using helper (robust check)
    int Hp = poolOutDim(H, F_pool, S_pool);
    int Wp = poolOutDim(W, F_pool, S_pool);


    if (Hp <= 0 || Wp <= 0 || C <= 0) {
         // Output dimensions are zero or negative, or no channels. Nothing to compute.
         // fprintf(stderr, "Warning: MaxPoolLayer output size is zero or negative (Hp=%d, Wp=%d, C=%d). Skipping kernel launch.\n", Hp, Wp, C);
         return;
     }

    int total_outputs = Hp * Wp * C;
    int threads_per_block = 256;
    int blocks_per_grid = (total_outputs + threads_per_block - 1) / threads_per_block;

    poolKernel<<<blocks_per_grid, threads_per_block>>>(
        d_output, d_input,
        H, W, C,
        F_pool, S_pool,
        Hp, Wp);
    CUDA_CHECK(cudaGetLastError());
    // CUDA_CHECK(cudaDeviceSynchronize());
}

// Definition for cudaLRNLayer
void cudaLRNLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int N, float alpha, float beta, float k)
{
    if (H <= 0 || W <= 0 || C <= 0 || N <= 0) {
         // Invalid dimensions or LRN window size.
         // fprintf(stderr, "Warning: LRNLayer input size or N is zero or negative. Skipping kernel launch.\n");
         // If output must match input size, consider copying input to output here.
         if (d_output != d_input && H*W*C > 0) { // Avoid self-copy
             CUDA_CHECK(cudaMemcpy(d_output, d_input, (size_t)H*W*C*sizeof(float), cudaMemcpyDeviceToDevice));
         }
         return;
     }
    int total_elements = H * W * C;
    int threads_per_block = 256;
    int blocks_per_grid = (total_elements + threads_per_block - 1) / threads_per_block;

    lrnKernel<<<blocks_per_grid, threads_per_block>>>(
        d_output, d_input,
        H, W, C, N, alpha, beta, k);
    CUDA_CHECK(cudaGetLastError());
    // CUDA_CHECK(cudaDeviceSynchronize());
}

================================================================================
