
================================================================================

=== FILE: final_project/v1_serial/Makefile ===

# Makefile for Serial Version (V1) - Use g++
# Access shared data/docs using relative paths like '../data/' from src files.

CXX = g++
# Add -O3 for optimization, -g for debugging if needed
CXXFLAGS = -Wall -std=c++11 -O3 #-g
# Add -lm if using math functions like sqrt, exp etc.
LDFLAGS = -lm
INCLUDES = -I./include

# --- List the final .cpp source files ---
SRCS = src/main.cpp src/alexnet_serial.cpp src/layers_serial.cpp

OBJS = $(SRCS:.cpp=.o)

TARGET = template

.PHONY: all clean

all: $(TARGET)

$(TARGET): $(OBJS)
	$(CXX) $(LDFLAGS) -o $@ $^ $(LDFLAGS) # Linker flags often go at the end

%.o: %.cpp $(wildcard include/*.hpp) # Recompile .o if corresponding .cpp or *any* .hpp changes
	$(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@

clean:
	rm -f $(TARGET) $(OBJS)

================================================================================

=== FILE: final_project/v1_serial/include/alexnet.hpp ===

#ifndef ALEXNET_HPP
#define ALEXNET_HPP

#include <vector>
#include <string>
#include <cstddef> // Include for size_t

// Structure to hold layer parameters
struct LayerParams {
    // Convolution Params
    std::vector<float> weights;
    std::vector<float> biases;
    int K, F, S, P; // K=NumFilters, F=FilterSize, S=Stride, P=Padding

    // Pooling Params (Associated with the preceding Conv layer for convenience)
    int F_pool; // Pooling Filter Size
    int S_pool; // Pooling Stride

    // LRN Params (Associated with the layer needing LRN)
    int N_lrn;  // LRN Window Size
    float alpha;
    float beta;
    float k_lrn;
};

// Corrected function prototype name
void alexnetForwardPass(
    std::vector<float>& input_data, // Input image data (flattened)
    const LayerParams& paramsConv1, // Includes Pool1 params
    const LayerParams& paramsConv2, // Includes Pool2 & LRN2 params
    int H, int W, int C // Initial image dimensions
);

// Helper function to initialize data (e.g., random)
void initializeData(std::vector<float>& data, size_t size);
void initializeWeights(std::vector<float>& weights, size_t size);
void initializeBiases(std::vector<float>& biases, size_t size);

// Helper function to print dimensions (for debugging)
void printDimensions(const std::string& layer_name, int H, int W, int C);

#endif // ALEXNET_HPP

================================================================================

=== FILE: final_project/v1_serial/include/layers.hpp ===

#ifndef LAYERS_HPP
#define LAYERS_HPP

#include <vector> // Use vectors for simplicity, or raw pointers if preferred

// --- Serial Layer Function Prototypes ---

// Naive Serial Convolution Layer
void serialConvLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    const std::vector<float>& weights,
    const std::vector<float>& biases, // Added biases
    int H, int W, int C, // Input dimensions (Height, Width, Channels)
    int K, // Number of filters (Output channels)
    int F, // Filter size (FxF)
    int S, // Stride
    int P  // Padding
);

// Serial ReLU Activation Layer (in-place or out-of-place)
void serialReluLayer(std::vector<float>& data); // Example: In-place

// Serial Max Pooling Layer
void serialMaxPoolLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    int H, int W, int C, // Input dimensions
    int F, // Filter size (pooling window size)
    int S  // Stride
);

// Serial Local Response Normalization (LRN) Layer
void serialLRNLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    int H, int W, int C, // Input dimensions
    int N,        // Size of the normalization window (across channels)
    float alpha,  // LRN parameter
    float beta,   // LRN parameter
    float k       // LRN parameter
);

#endif // LAYERS_HPP

================================================================================

=== FILE: final_project/v1_serial/src/main.cpp ===

#include <iostream>
#include <vector>
#include <cstdlib> // For rand(), srand()
#include <ctime>   // For time()
#include <stdexcept> // For exceptions
#include <cstddef> // Include for size_t

#include "alexnet.hpp" // Include the header for the forward pass function

int main(int argc, char* argv[]) {
    // --- Basic Setup ---
    srand(time(0)); // Seed random number generator

    std::cout << "--- AlexNet Serial CPU Version (V1) ---" << std::endl;

    // --- Define Network Dimensions and Parameters (EXAMPLE VALUES) ---
    // Input Image
    int H = 227, W = 227, C = 3; // Example AlexNet input size

    // Conv1 Parameters
    LayerParams paramsConv1;
    paramsConv1.K = 96;  // Num filters
    paramsConv1.F = 11;  // Filter size
    paramsConv1.S = 4;   // Stride
    paramsConv1.P = 0;   // Padding
    // Pool1 (often associated with Conv1 params)
    paramsConv1.F_pool = 3;
    paramsConv1.S_pool = 2;

    // Conv2 Parameters
    LayerParams paramsConv2;
    paramsConv2.K = 256; // Num filters
    paramsConv2.F = 5;   // Filter size
    paramsConv2.S = 1;   // Stride
    paramsConv2.P = 2;   // Padding
    // Pool2
    paramsConv2.F_pool = 3;
    paramsConv2.S_pool = 2;
    // LRN2
    paramsConv2.N_lrn = 5;
    paramsConv2.alpha = 0.0001f;
    paramsConv2.beta = 0.75f;
    paramsConv2.k_lrn = 2.0f;


    // --- Allocate and Initialize Data & Weights (Host Memory) ---
    std::cout << "Initializing data and parameters..." << std::endl;

    // Input Data
    std::vector<float> h_inputData;
    initializeData(h_inputData, static_cast<size_t>(H) * W * C); // Use size_t

    // Conv1 Weights & Biases
    initializeWeights(paramsConv1.weights, static_cast<size_t>(paramsConv1.K) * C * paramsConv1.F * paramsConv1.F);
    initializeBiases(paramsConv1.biases, static_cast<size_t>(paramsConv1.K));


    // Conv2 Weights & Biases
    // Input channels to Conv2 is output channels of Conv1/Pool1
    int C_conv2_input = paramsConv1.K;
    initializeWeights(paramsConv2.weights, static_cast<size_t>(paramsConv2.K) * C_conv2_input * paramsConv2.F * paramsConv2.F);
    initializeBiases(paramsConv2.biases, static_cast<size_t>(paramsConv2.K));

    std::cout << "Initialization complete." << std::endl;


    // --- Perform Forward Pass ---
    try {
        alexnetForwardPass(h_inputData, paramsConv1, paramsConv2, H, W, C);
    } catch (const std::exception& e) {
        std::cerr << "Error during forward pass: " << e.what() << std::endl;
        return 1;
    }

    std::cout << "--- Serial execution finished ---" << std::endl;

    return 0;
}

================================================================================

=== FILE: final_project/v1_serial/src/alexnet_serial.cpp ===

#include <vector>
#include <iostream>
#include <chrono>    // For timing
#include <algorithm> // For std::min, std::swap
#include <cmath>     // For std::max, std::pow // LRN might use pow
#include <string>    // For std::string
#include <cstdlib>   // For rand(), RAND_MAX
#include <cstddef>   // For size_t
#include <limits>    // For numeric_limits if needed by layers

// Include necessary headers ONCE correctly
#include "../include/alexnet.hpp" // Correct relative path for LayerParams
#include "../include/layers.hpp"  // Correct relative path for serial layer functions

// --- Helper function definitions belong here (or a separate utils.cpp) ---
// --- DO NOT duplicate these definitions in layers_serial.cpp ---

// Helper to calculate output dimensions (using reference parameters)
void calculate_conv_output_dims(int& outH, int& outW, int H, int W, int F, int S, int P) {
    if (S <= 0) {
        std::cerr << "Error: Stride (S) must be positive." << std::endl;
        outH = 0; outW = 0; return;
    }
    outH = (H - F + 2 * P) / S + 1;
    outW = (W - F + 2 * P) / S + 1;
}

// Helper to calculate pooling output dimensions
void calculate_pool_output_dims(int& outH, int& outW, int H, int W, int F, int S) {
    if (S <= 0) {
        std::cerr << "Error: Stride (S) must be positive." << std::endl;
        outH = 0; outW = 0; return;
    }
    outH = (H - F) / S + 1; // Standard pooling, assumes P=0
    outW = (W - F) / S + 1;
}

// Helper to initialize input data
void initializeData(std::vector<float>& data, size_t size) {
    data.resize(size);
    for (size_t i = 0; i < size; ++i) {
        data[i] = static_cast<float>(rand()) / static_cast<float>(RAND_MAX) * 0.1f;
    }
}

// Helper to initialize weights
void initializeWeights(std::vector<float>& weights, size_t size) {
     weights.resize(size);
    for (size_t i = 0; i < size; ++i) {
        weights[i] = (static_cast<float>(rand()) / static_cast<float>(RAND_MAX) - 0.5f) * 0.02f;
    }
}

// Helper to initialize biases
void initializeBiases(std::vector<float>& biases, size_t size) {
    biases.assign(size, 0.1f);
}

// Helper to print dimensions
void printDimensions(const std::string& layer_name, int H, int W, int C) {
    std::cout << "  [" << layer_name << "] Dimensions: H=" << H << ", W=" << W << ", C=" << C << std::endl;
}

// --- AlexNet Forward Pass Implementation ---
// --- Definition belongs ONLY here ---

void alexnetForwardPass(
    std::vector<float>& input_data,
    const LayerParams& paramsConv1,
    const LayerParams& paramsConv2,
    int H, int W, int C)
{
    std::cout << "Starting AlexNet Serial Forward Pass..." << std::endl;
    auto start_time = std::chrono::high_resolution_clock::now();

    std::vector<float> buffer1, buffer2;
    std::vector<float>* current_input = &input_data;
    std::vector<float>* current_output = &buffer1;
    current_output->clear();

    int currentH = H;
    int currentW = W;
    int currentC = C;
    int nextH, nextW, nextC;

    // --- Block 1 ---
    printDimensions("Input", currentH, currentW, currentC);

    // Conv1
    std::cout << "Applying Conv1..." << std::endl;
    calculate_conv_output_dims(nextH, nextW, currentH, currentW, paramsConv1.F, paramsConv1.S, paramsConv1.P);
    nextC = paramsConv1.K;
    size_t next_size_conv1 = static_cast<size_t>(nextH) * nextW * nextC; // Cast first dim to size_t
    current_output->resize(next_size_conv1);
    serialConvLayer(*current_output, *current_input, paramsConv1.weights, paramsConv1.biases,
                    currentH, currentW, currentC, paramsConv1.K, paramsConv1.F, paramsConv1.S, paramsConv1.P);
    std::swap(current_input, current_output);
    currentH = nextH; currentW = nextW; currentC = nextC;
    printDimensions("After Conv1", currentH, currentW, currentC);

    // ReLU1
    std::cout << "Applying ReLU1..." << std::endl;
    serialReluLayer(*current_input);
    printDimensions("After ReLU1", currentH, currentW, currentC);

    // Pool1
    std::cout << "Applying MaxPool1..." << std::endl;
    calculate_pool_output_dims(nextH, nextW, currentH, currentW, paramsConv1.F_pool, paramsConv1.S_pool);
    nextC = currentC;
    size_t next_size_pool1 = static_cast<size_t>(nextH) * nextW * nextC;
    current_output->resize(next_size_pool1);
    serialMaxPoolLayer(*current_output, *current_input,
                       currentH, currentW, currentC, paramsConv1.F_pool, paramsConv1.S_pool);
    std::swap(current_input, current_output);
    currentH = nextH; currentW = nextW; currentC = nextC;
    printDimensions("After Pool1", currentH, currentW, currentC);

    // --- Block 2 ---

    // Conv2
    std::cout << "Applying Conv2..." << std::endl;
    calculate_conv_output_dims(nextH, nextW, currentH, currentW, paramsConv2.F, paramsConv2.S, paramsConv2.P);
    nextC = paramsConv2.K;
    size_t next_size_conv2 = static_cast<size_t>(nextH) * nextW * nextC;
    // Resize buffer if necessary
    if (current_output->size() != next_size_conv2) {
         current_output->resize(next_size_conv2);
    }
    serialConvLayer(*current_output, *current_input, paramsConv2.weights, paramsConv2.biases,
                    currentH, currentW, currentC, paramsConv2.K, paramsConv2.F, paramsConv2.S, paramsConv2.P);
    std::swap(current_input, current_output);
    currentH = nextH; currentW = nextW; currentC = nextC;
    printDimensions("After Conv2", currentH, currentW, currentC);

    // ReLU2
    std::cout << "Applying ReLU2..." << std::endl;
    serialReluLayer(*current_input);
    printDimensions("After ReLU2", currentH, currentW, currentC);

    // Pool2
    std::cout << "Applying MaxPool2..." << std::endl;
    calculate_pool_output_dims(nextH, nextW, currentH, currentW, paramsConv2.F_pool, paramsConv2.S_pool);
    nextC = currentC;
    size_t next_size_pool2 = static_cast<size_t>(nextH) * nextW * nextC;
    if (current_output->size() != next_size_pool2) {
        current_output->resize(next_size_pool2);
    }
    serialMaxPoolLayer(*current_output, *current_input,
                       currentH, currentW, currentC, paramsConv2.F_pool, paramsConv2.S_pool);
    std::swap(current_input, current_output);
    currentH = nextH; currentW = nextW; currentC = nextC;
    printDimensions("After Pool2", currentH, currentW, currentC);

    // LRN2
    std::cout << "Applying LRN2..." << std::endl;
    nextH = currentH; nextW = currentW; nextC = currentC; // LRN doesn't change dimensions
    size_t next_size_lrn2 = static_cast<size_t>(nextH) * nextW * nextC;
    if (current_output->size() != next_size_lrn2) {
        current_output->resize(next_size_lrn2);
    }
    serialLRNLayer(*current_output, *current_input,
                   currentH, currentW, currentC, paramsConv2.N_lrn, paramsConv2.alpha, paramsConv2.beta, paramsConv2.k_lrn);
    std::swap(current_input, current_output);
    printDimensions("After LRN2", currentH, currentW, currentC);

    // --- Copy final result back to input_data if necessary ---
    if (current_input != &input_data) {
         std::cout << "Copying final result back to original buffer..." << std::endl;
         input_data = *current_input; // Vector assignment handles copy/resize
    } else {
         std::cout << "Final result is already in the original buffer." << std::endl;
    }

    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
    std::cout << "AlexNet Serial Forward Pass completed in " << duration.count() << " ms" << std::endl;

    // Output first few values
    std::cout << "Final Output (first 10 values): ";
    // Use size_t for loop counter when comparing with vector::size()
    size_t print_count = std::min(static_cast<size_t>(10), input_data.size());
    for(size_t i = 0; i < print_count; ++i) {
        std::cout << input_data[i] << (i == print_count - 1 ? "" : " ");
    }
    std::cout << (input_data.size() > 10 ? "..." : "") << std::endl;
}

================================================================================

=== FILE: final_project/v1_serial/src/layers_serial.cpp ===

#include <vector>
#include <cmath>     // For std::max, std::pow, std::fmax? (check usage)
#include <algorithm> // For std::max, std::min
#include <limits>    // For std::numeric_limits
#include <cstddef>   // For size_t
#include <iostream>  // For error checking (optional)

// Include necessary headers
#include "layers.hpp"  // Should declare the functions being implemented here
#include "alexnet.hpp" // Needed for LayerParams if used directly (conv layer signature needs adjustment)

// --- 3D Index Helper ---
// Make it static or put in an anonymous namespace if only used in this file
namespace { // Anonymous namespace limits scope to this file
    inline size_t idx3D(int h, int w, int c, int W, int C) {
        // Add checks for h, w, c boundaries if needed for robustness
        return (static_cast<size_t>(h) * W + w) * C + c;
    }
    // Helper to calculate output dimensions (needed by layers implementation)
    // Can be defined here (static/anon namespace) or declared in layers.hpp and defined once elsewhere.
    // Let's keep it local here for now.
    inline int calculateConvOutputDim(int D, int F, int P, int S) {
        if (S <= 0) return 0; // Avoid division by zero
        return (D - F + 2 * P) / S + 1;
    }
    inline int calculatePoolOutputDim(int D, int F, int S) {
        if (S <= 0) return 0; // Avoid division by zero
        return (D - F) / S + 1; // Assumes P=0 for pool
    }
} // End anonymous namespace


// --- Serial Layer Function Implementations ---

// Naive Serial Convolution Layer
// Signature matches layers.hpp
void serialConvLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    const std::vector<float>& weights,
    const std::vector<float>& biases,
    int H, int W, int C,
    int K, int F, int S, int P)
{
    int Ho = calculateConvOutputDim(H, F, P, S);
    int Wo = calculateConvOutputDim(W, F, P, S);

    // Pre-check output size (optional but good practice)
    if (output.size() != static_cast<size_t>(Ho) * Wo * K) {
         std::cerr << "Warning: Output vector size mismatch in serialConvLayer. Resizing." << std::endl;
         output.resize(static_cast<size_t>(Ho) * Wo * K);
    }

    // Parallelizing this loop is the main goal of MPI/CUDA versions
    for (int k = 0; k < K; ++k) { // For each output channel (filter)
        for (int ho = 0; ho < Ho; ++ho) { // For each output row
            for (int wo = 0; wo < Wo; ++wo) { // For each output column
                float sum = biases[k]; // Start with bias
                // Apply filter
                for (int c = 0; c < C; ++c) { // For each input channel
                    for (int fh = 0; fh < F; ++fh) { // For each filter row
                        for (int fw = 0; fw < F; ++fw) { // For each filter column
                            int hi = ho * S - P + fh; // Input row index
                            int wi = wo * S - P + fw; // Input column index

                            // Check bounds (convolution with padding)
                            if (hi >= 0 && hi < H && wi >= 0 && wi < W) {
                                size_t input_idx = idx3D(hi, wi, c, W, C);
                                // Weight index: OutputChannel(k), InputChannel(c), FilterRow(fh), FilterCol(fw)
                                size_t weight_idx = (((static_cast<size_t>(k) * C + c) * F + fh) * F) + fw;
                                sum += input[input_idx] * weights[weight_idx];
                            }
                            // else: contribution is 0 (implicitly, due to padding)
                        }
                    }
                }
                output[idx3D(ho, wo, k, Wo, K)] = sum; // Store result
            }
        }
    }
}

// Serial ReLU Activation Layer (in-place)
// Signature matches layers.hpp
void serialReluLayer(std::vector<float>& data) {
    for (float& val : data) {
        val = std::max(0.0f, val);
    }
    // Can be optimized using std::transform or OpenMP for trivial parallelization on CPU
}

// Serial Max Pooling Layer
// Signature matches layers.hpp
void serialMaxPoolLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    int H, int W, int C,
    int F, // Filter size (pooling window size)
    int S) // Stride
{
    int Ho = calculatePoolOutputDim(H, F, S);
    int Wo = calculatePoolOutputDim(W, F, S);

    // Pre-check output size
     if (output.size() != static_cast<size_t>(Ho) * Wo * C) {
         std::cerr << "Warning: Output vector size mismatch in serialMaxPoolLayer. Resizing." << std::endl;
         output.resize(static_cast<size_t>(Ho) * Wo * C);
     }

    for (int c = 0; c < C; ++c) { // For each channel (pooling is usually done per-channel)
        for (int ho = 0; ho < Ho; ++ho) {
            for (int wo = 0; wo < Wo; ++wo) {
                float max_val = -std::numeric_limits<float>::infinity();
                // Find max in the FxF window
                for (int fh = 0; fh < F; ++fh) {
                    for (int fw = 0; fw < F; ++fw) {
                        int hi = ho * S + fh;
                        int wi = wo * S + fw;
                        // Bounds check (should ideally not be needed if Ho, Wo calculated correctly)
                        if (hi >= 0 && hi < H && wi >= 0 && wi < W) {
                            max_val = std::max(max_val, input[idx3D(hi, wi, c, W, C)]);
                        }
                    }
                }
                output[idx3D(ho, wo, c, Wo, C)] = max_val;
            }
        }
    }
}

// Serial Local Response Normalization (LRN) Layer
// Signature matches layers.hpp
void serialLRNLayer(
    std::vector<float>& output,
    const std::vector<float>& input,
    int H, int W, int C,
    int N, float alpha, float beta, float k)
{
     // Pre-check output size
     if (output.size() != input.size()) {
         std::cerr << "Warning: Output vector size mismatch in serialLRNLayer. Resizing." << std::endl;
         output.resize(input.size());
     }
     if (N <= 0) {
         std::cerr << "Error: LRN window size (N) must be positive." << std::endl;
         // Optionally copy input to output or handle error differently
         std::copy(input.begin(), input.end(), output.begin());
         return;
     }

    int half_N = N / 2;
    float alpha_over_N = alpha / static_cast<float>(N); // Precompute

    for (int h = 0; h < H; ++h) {
        for (int w = 0; w < W; ++w) {
            for (int c = 0; c < C; ++c) { // For each channel at this (h, w) position
                // Calculate the sum of squares across the channel window
                float sum_sq = 0.0f;
                int c_start = std::max(0, c - half_N);
                int c_end = std::min(C - 1, c + half_N);
                for (int i = c_start; i <= c_end; ++i) {
                    float val = input[idx3D(h, w, i, W, C)];
                    sum_sq += val * val;
                }

                // Calculate the normalization factor
                float norm_factor = std::pow(k + alpha_over_N * sum_sq, beta);

                // Apply normalization
                size_t current_idx = idx3D(h, w, c, W, C);
                output[current_idx] = input[current_idx] / norm_factor;
            }
        }
    }
}

================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/Makefile ===

# MPI‑only (Scatter+Halo) build
CXX       := mpicxx
CXXFLAGS  := -std=c++11 -O3 -Wall -Wextra -pedantic \
             -Wno-cast-function-type
LDFLAGS   := -lm
INCLUDES  := -I./include

SRCS      := src/main.cpp \
             src/alexnet_mpi.cpp \
             src/layers_mpi.cpp

OBJS      := $(SRCS:.cpp=.o)
TARGET    := template

.PHONY: all clean

all: $(TARGET)

$(TARGET): $(OBJS)
	$(CXX) $(CXXFLAGS) $^ -o $@ $(LDFLAGS)

%.o: %.cpp $(wildcard include/*.hpp)
	$(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@

clean:
	rm -f $(OBJS) $(TARGET)


================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/include/alexnet.hpp ===

#pragma once
#include <vector>

/* -------- Layer hyper‑parameters -------- */
struct LayerParams {
    std::vector<float> weights;   // flattened (K x C x F x F)
    std::vector<float> biases;    // length K
    int K = 0;    // # filters / output channels
    int F = 0;    // filter size
    int S = 1;    // stride
    int P = 0;    // padding

    /* pool */
    int F_pool  = 0;  // pool window
    int S_pool  = 1;  // pool stride

    /* local response norm */
    int   N_lrn = 0;
    float alpha = 0.0f;
    float beta  = 0.0f;
    float k_lrn = 1.0f;
};

/* --------------- API --------------- */
// Needs definition in alexnet_mpi.cpp
void alexnetForwardPassMPI(std::vector<float>& input,
                           const LayerParams& conv1,
                           const LayerParams& conv2,
                           int H, int W, int C,
                           std::vector<float>& output);

/* -------- Helpers -------- */
// Defined inline so they can be included in multiple places without linker errors

inline int convOutDim(int dim, int F, int P, int S) {
    if (S <= 0) return 0; // Avoid division by zero
    return (dim - F + 2 * P) / S + 1;
}

// *** ADD THIS INLINE DEFINITION ***
inline int poolOutDim(int dim, int F, int S) {
    if (S <= 0) return 0; // Avoid division by zero
    return (dim - F) / S + 1; // Assumes P=0 for pooling
}

================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/include/layers.hpp ===

#pragma once
#include <vector>
#include "alexnet.hpp"

/* ----- layer kernels (CPU, identical to V1 serial) ----- */
void serialConvLayer(std::vector<float>& out,
                     const std::vector<float>& in,
                     const LayerParams& p,
                     int H, int W, int C);

void serialReluLayer(std::vector<float>& data);

void serialMaxPoolLayer(std::vector<float>& out,
                        const std::vector<float>& in,
                        int H, int W, int C,
                        int F_pool, int S_pool);

void serialLRNLayer(std::vector<float>& out,
                    const std::vector<float>& in,
                    int H, int W, int C,
                    int N, float alpha, float beta, float k);


================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/src/main.cpp ===

// final_project/v2_mpi_only/2.2_scatter_halo/src/main.cpp
#include <mpi.h>
#include <iostream>
#include <vector>
#include <algorithm>
#include <numeric>
#include <iomanip> // For std::fixed, std::setprecision if needed
#include <cmath>   // Needed for std::max
#include <chrono>  // For timing

#include "../include/alexnet.hpp" // Includes LayerParams and inline helpers
#include "../include/layers.hpp"  // Includes serial layer function prototypes

// 3D index helper (local definition ok)
inline size_t idx3D(int h, int w, int c, int W, int C) {
    // Add checks for h, w, c boundaries if needed for robustness
    return (static_cast<size_t>(h) * W + w) * C + c;
}

// Note: convOutDim and poolOutDim are now defined inline in alexnet.hpp


int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 1) Common setup
    int H = 227, W = 227, C = 3;
    LayerParams conv1, conv2;
    std::vector<float> input, finalOut;
    int finalH = -1, finalW = -1, finalC = -1;

    if (rank == 0) {
        // initialize input & params
        input.assign((size_t)H * W * C, 1.0f);
        conv1.K = 96; conv1.F = 11; conv1.S = 4; conv1.P = 0;
        conv1.F_pool = 3; conv1.S_pool = 2;
        conv1.weights.assign((size_t)conv1.K * C * conv1.F * conv1.F, 0.01f);
        conv1.biases.assign(conv1.K, 0.0f);
        conv2.K = 256; conv2.F = 5; conv2.S = 1; conv2.P = 2;
        conv2.F_pool = 3; conv2.S_pool = 2;
        conv2.N_lrn = 5; conv2.alpha = 1e-4f; conv2.beta = 0.75f; conv2.k_lrn = 2.0f;
        int C_conv2_input = conv1.K;
        conv2.weights.assign((size_t)conv2.K * C_conv2_input * conv2.F * conv2.F, 0.01f);
        conv2.biases.assign(conv2.K, 0.0f);

        // Calculate expected final dimensions (using inline helpers from alexnet.hpp)
        int H1 = convOutDim(H, conv1.F, conv1.P, conv1.S);
        int W1 = convOutDim(W, conv1.F, conv1.P, conv1.S);
        H1 = poolOutDim(H1, conv1.F_pool, conv1.S_pool);
        W1 = poolOutDim(W1, conv1.F_pool, conv1.S_pool);
        int H2 = convOutDim(H1, conv2.F, conv2.P, conv2.S);
        int W2 = convOutDim(W1, conv2.F, conv2.P, conv2.S);
        H2 = poolOutDim(H2, conv2.F_pool, conv2.S_pool);
        W2 = poolOutDim(W2, conv2.F_pool, conv2.S_pool);
        finalH = H2; finalW = W2; finalC = conv2.K;
    }

    // 2) Broadcast sizes & params
    auto bcastInt = [&](int& x){ MPI_Bcast(&x,1,MPI_INT,0,MPI_COMM_WORLD); };
    bcastInt(H); bcastInt(W); bcastInt(C);
    bcastInt(conv1.K); bcastInt(conv1.F); bcastInt(conv1.S); bcastInt(conv1.P);
    bcastInt(conv1.F_pool); bcastInt(conv1.S_pool);
    bcastInt(conv2.K); bcastInt(conv2.F); bcastInt(conv2.S); bcastInt(conv2.P);
    bcastInt(conv2.F_pool); bcastInt(conv2.S_pool);
    bcastInt(conv2.N_lrn);
    bcastInt(finalH); bcastInt(finalW); bcastInt(finalC);

    if (rank != 0) {
        input.resize((size_t)H * W * C);
        conv1.weights.resize((size_t)conv1.K * C * conv1.F * conv1.F);
        conv1.biases.resize(conv1.K);
        int C_conv2_input = conv1.K;
        conv2.weights.resize((size_t)conv2.K * C_conv2_input * conv2.F * conv2.F);
        conv2.biases.resize(conv2.K);
    }
    float lrnArr[3];
    if (rank == 0) { lrnArr[0] = conv2.alpha; lrnArr[1] = conv2.beta; lrnArr[2] = conv2.k_lrn; }
    MPI_Bcast(lrnArr,3,MPI_FLOAT,0,MPI_COMM_WORLD);
    if (rank != 0) { conv2.alpha = lrnArr[0]; conv2.beta = lrnArr[1]; conv2.k_lrn = lrnArr[2]; }
    MPI_Bcast(input.data(),           static_cast<int>(input.size()),           MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(conv1.weights.data(),   static_cast<int>(conv1.weights.size()),   MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(conv1.biases.data(),    static_cast<int>(conv1.biases.size()),    MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(conv2.weights.data(),   static_cast<int>(conv2.weights.size()),   MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(conv2.biases.data(),    static_cast<int>(conv2.biases.size()),    MPI_FLOAT,0,MPI_COMM_WORLD);

    // --- Barrier before starting main computation (optional) ---
    MPI_Barrier(MPI_COMM_WORLD);
    auto t_start_compute = std::chrono::high_resolution_clock::now(); // *** Start Compute Timer ***

    // Path A: broadcast-all if only one rank (np=1 case)
    if (size == 1) {
        // Definition is in alexnet_mpi.cpp
        alexnetForwardPassMPI(input, conv1, conv2, H, W, C, finalOut);
    }
    // Path B: scatter + halo (np > 1 case)
    else {
        // 3) Scatter input
        std::vector<int> sendCnt(size), sendDisp(size);
        int base = H / size, rem = H % size;
        if (rank == 0) {
            sendDisp[0] = 0;
            for (int i = 0; i < size; ++i) {
                int rows = base + (i < rem ? 1 : 0);
                sendCnt[i]  = rows * W * C;
                if (i > 0) { sendDisp[i] = sendDisp[i - 1] + sendCnt[i - 1]; }
            }
        }
        int localCnt;
        MPI_Scatter(sendCnt.data(), 1, MPI_INT, &localCnt, 1, MPI_INT, 0, MPI_COMM_WORLD);
        std::vector<float> localIn(localCnt);
        MPI_Scatterv(input.data(), sendCnt.data(), sendDisp.data(), MPI_FLOAT,
                     localIn.data(), localCnt, MPI_FLOAT, 0, MPI_COMM_WORLD);
        int localH = (W*C > 0) ? localCnt / (W * C) : 0;

        // 4) Halo for Conv1
        const int pad1 = conv1.F / 2;
        int slice1 = 0; if (pad1 > 0 && W > 0 && C > 0) slice1 = pad1 * W * C;
        std::vector<float> topHalo(slice1), botHalo(slice1);
        MPI_Request send_req_up = MPI_REQUEST_NULL, recv_req_up = MPI_REQUEST_NULL;
        MPI_Request send_req_down = MPI_REQUEST_NULL, recv_req_down = MPI_REQUEST_NULL;
        if (pad1 > 0) {
            if (rank > 0) {
                MPI_Irecv(topHalo.data(), slice1, MPI_FLOAT, rank - 1, 1, MPI_COMM_WORLD, &recv_req_up);
                MPI_Isend(localIn.data(), slice1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, &send_req_up);
            } else { std::fill(topHalo.begin(), topHalo.end(), 0.0f); }
            if (rank < size - 1) {
                MPI_Irecv(botHalo.data(), slice1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, &recv_req_down);
                MPI_Isend(localIn.data() + (size_t)std::max(0, localH - pad1) * W * C, slice1, MPI_FLOAT, rank + 1, 1, MPI_COMM_WORLD, &send_req_down);
            } else { std::fill(botHalo.begin(), botHalo.end(), 0.0f); }
            MPI_Wait(&recv_req_up, MPI_STATUS_IGNORE); MPI_Wait(&send_req_up, MPI_STATUS_IGNORE);
            MPI_Wait(&recv_req_down, MPI_STATUS_IGNORE); MPI_Wait(&send_req_down, MPI_STATUS_IGNORE);
        }

        // 5) Conv1 → ReLU1 → Pool1
        int pH1 = localH + 2 * pad1;
        std::vector<float> padded1((size_t)std::max(0,pH1) * W * C);
        if (pad1 > 0 && localIn.size() > 0) { // Check localIn isn't empty
             if (slice1 <= static_cast<int>(padded1.size())) std::copy(topHalo.begin(), topHalo.end(), padded1.begin());
             if (slice1 + localIn.size() <= padded1.size()) std::copy(localIn.begin(), localIn.end(), padded1.begin() + slice1);
             if (slice1 + localIn.size() + slice1 <= padded1.size()) std::copy(botHalo.begin(), botHalo.end(), padded1.begin() + slice1 + localIn.size());
        } else { padded1 = localIn; }

        int Hc1 = convOutDim(pH1, conv1.F, conv1.P, conv1.S);
        int Wc1 = convOutDim(W,   conv1.F, conv1.P, conv1.S);
        std::vector<float> c1out((size_t)std::max(0,Hc1) * Wc1 * conv1.K);
        // Ensure layers are defined (layers_mpi.cpp) - V2 uses different layer signatures!
        serialConvLayer(c1out, padded1, conv1, pH1, W, C); // V2 uses LayerParams struct
        serialReluLayer(c1out);

        int Hp1 = poolOutDim(Hc1, conv1.F_pool, conv1.S_pool);
        int Wp1 = poolOutDim(Wc1, conv1.F_pool, conv1.S_pool);
        std::vector<float> pool1_p((size_t)std::max(0,Hp1) * Wp1 * conv1.K);
        serialMaxPoolLayer(pool1_p, c1out, Hc1, Wc1, conv1.K, conv1.F_pool, conv1.S_pool);


        // 6) Asymmetric trim of pool1_p (Corrected Logic)
        int trim_top1 = 0, trim_bot1 = 0;
        if (pad1 > 0 && conv1.S > 0 && conv1.S_pool > 0) {
            int rows_affected_by_pad_after_conv1 = (pad1 + conv1.S - 1) / conv1.S;
            int trim_amount1 = (rows_affected_by_pad_after_conv1 + conv1.S_pool - 1) / conv1.S_pool;
            trim_top1 = (rank > 0        ) ? trim_amount1 : 0;
            trim_bot1 = (rank < size - 1 ) ? trim_amount1 : 0;
        }
        if (trim_top1 + trim_bot1 >= Hp1 && Hp1 > 0) { if(rank==0) std::cerr<<"E1 "; MPI_Abort(MPI_COMM_WORLD, 1); }
        int realHp1 = Hp1 - trim_top1 - trim_bot1;
        std::vector<float> pool1Out((size_t)std::max(0, realHp1) * Wp1 * conv1.K);
        if (realHp1 > 0) {
            for (int r = 0; r < realHp1; ++r) {
                 size_t src_off = (size_t)(r + trim_top1) * Wp1 * conv1.K; size_t dst_off = (size_t)r * Wp1 * conv1.K; size_t count = (size_t)Wp1 * conv1.K;
                 if (src_off + count <= pool1_p.size() && dst_off + count <= pool1Out.size()) { std::copy_n(pool1_p.data() + src_off, count, pool1Out.data() + dst_off); }
                 else { if(rank==0) std::cerr<<"E2 "; MPI_Abort(MPI_COMM_WORLD, 1); }
            }
        } else { pool1Out.clear(); realHp1 = 0; }

        // 7) Halo Exchange for Conv2
        int C1 = conv1.K; int pad2 = conv2.F / 2; int slice2 = 0; if(pad2 > 0 && Wp1 > 0 && C1 > 0) slice2 = pad2 * Wp1 * C1;
        std::vector<float> topHalo2(slice2), botHalo2(slice2);
        MPI_Request send_req2_up = MPI_REQUEST_NULL, recv_req2_up = MPI_REQUEST_NULL;
        MPI_Request send_req2_down = MPI_REQUEST_NULL, recv_req2_down = MPI_REQUEST_NULL;
        if (pad2 > 0 && realHp1 > 0) {
            if (rank > 0) { MPI_Irecv(topHalo2.data(), slice2, MPI_FLOAT, rank - 1, 3, MPI_COMM_WORLD, &recv_req2_up); int send_count_up = std::min(slice2, (int)pool1Out.size()); if(send_count_up > 0) MPI_Isend(pool1Out.data(), send_count_up, MPI_FLOAT, rank - 1, 2, MPI_COMM_WORLD, &send_req2_up); else send_req2_up = MPI_REQUEST_NULL;} else { std::fill(topHalo2.begin(), topHalo2.end(), 0.0f); }
            if (rank < size - 1) { MPI_Irecv(botHalo2.data(), slice2, MPI_FLOAT, rank + 1, 2, MPI_COMM_WORLD, &recv_req2_down); size_t send_offset_down = (size_t)std::max(0, realHp1 - pad2) * Wp1 * C1; int send_count_down = std::min(slice2, (int)((size_t)realHp1*Wp1*C1 - send_offset_down)); if (send_count_down > 0) { MPI_Isend(pool1Out.data() + send_offset_down, send_count_down, MPI_FLOAT, rank + 1, 3, MPI_COMM_WORLD, &send_req2_down); } else { send_req2_down = MPI_REQUEST_NULL; } } else { std::fill(botHalo2.begin(), botHalo2.end(), 0.0f); }
            MPI_Wait(&recv_req2_up, MPI_STATUS_IGNORE); MPI_Wait(&send_req2_up, MPI_STATUS_IGNORE); MPI_Wait(&recv_req2_down, MPI_STATUS_IGNORE); MPI_Wait(&send_req2_down, MPI_STATUS_IGNORE);
        } else { std::fill(topHalo2.begin(), topHalo2.end(), 0.0f); std::fill(botHalo2.begin(), botHalo2.end(), 0.0f); }

        int pH2 = realHp1 + 2 * pad2;
        std::vector<float> padded2((size_t)std::max(0,pH2) * Wp1 * C1, 0.0f);
        if (realHp1 > 0 && pool1Out.size() > 0) { // Check pool1Out not empty
             if (pad2 > 0) {
                 if (slice2 <= static_cast<int>(padded2.size())) std::copy(topHalo2.begin(), topHalo2.end(), padded2.begin());
                 if (slice2 + pool1Out.size() <= padded2.size()) std::copy(pool1Out.begin(), pool1Out.end(), padded2.begin() + slice2);
                 if (slice2 + pool1Out.size() + slice2 <= padded2.size()) std::copy(botHalo2.begin(), botHalo2.end(), padded2.begin() + slice2 + pool1Out.size());
             } else { padded2 = pool1Out; }
        }

        int Hc2 = convOutDim(pH2, conv2.F, conv2.P, conv2.S);
        int Wc2 = convOutDim(Wp1, conv2.F, conv2.P, conv2.S);
        std::vector<float> c2out((size_t)std::max(0,Hc2) * Wc2 * conv2.K);
        serialConvLayer(c2out, padded2, conv2, pH2, Wp1, C1); // V2 uses LayerParams struct
        serialReluLayer(c2out);

        int Hp2 = poolOutDim(Hc2, conv2.F_pool, conv2.S_pool);
        int Wp2 = poolOutDim(Wc2, conv2.F_pool, conv2.S_pool);
        std::vector<float> p2out((size_t)std::max(0,Hp2) * Wp2 * conv2.K);
        serialMaxPoolLayer(p2out, c2out, Hc2, Wc2, conv2.K, conv2.F_pool, conv2.S_pool);

        std::vector<float> l2out(p2out.size());
        serialLRNLayer(l2out, p2out, Hp2, Wp2, conv2.K, conv2.N_lrn, conv2.alpha, conv2.beta, conv2.k_lrn);

        // 8) Trim output of LRN2 (Corrected Logic)
        int trim_top2 = 0, trim_bot2 = 0;
        if (pad2 > 0 && conv2.S > 0 && conv2.S_pool > 0) {
            int rows_affected_by_pad_after_conv2 = (pad2 + conv2.S - 1) / conv2.S;
            int trim_amount2 = (rows_affected_by_pad_after_conv2 + conv2.S_pool - 1) / conv2.S_pool;
            trim_top2 = (rank > 0        ) ? trim_amount2 : 0;
            trim_bot2 = (rank < size - 1 ) ? trim_amount2 : 0;
        }
        if (trim_top2 + trim_bot2 >= Hp2 && Hp2 > 0) { if(rank==0) std::cerr<<"E3 "; MPI_Abort(MPI_COMM_WORLD, 1); }
        int realHp2 = Hp2 - trim_top2 - trim_bot2;
        std::vector<float> localOut((size_t)std::max(0, realHp2) * Wp2 * conv2.K);
         if (realHp2 > 0) {
            for (int r = 0; r < realHp2; ++r) {
                 size_t src_off = (size_t)(r + trim_top2) * Wp2 * conv2.K; size_t dst_off = (size_t)r * Wp2 * conv2.K; size_t count = (size_t)Wp2 * conv2.K;
                 if (src_off + count <= l2out.size() && dst_off + count <= localOut.size()) { std::copy_n(l2out.data() + src_off, count, localOut.data() + dst_off); }
                 else { if(rank==0) std::cerr<<"E4 "; MPI_Abort(MPI_COMM_WORLD, 1); }
            }
         } else { localOut.clear(); realHp2 = 0; }

        // Gather sizes
        int outCnt = static_cast<int>(localOut.size());
        std::vector<int> recvCnt(size);
        MPI_Gather(&outCnt, 1, MPI_INT, recvCnt.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);

        // Calculate displacements and total size on Rank 0
        std::vector<int> recvDisp;
        int totalFinalSize = 0; // Define here for rank 0 scope
        if (rank == 0) {
            recvDisp.resize(size); recvDisp[0] = 0; totalFinalSize = recvCnt[0];
            for (int i = 1; i < size; ++i) { recvDisp[i] = recvDisp[i-1] + recvCnt[i-1]; totalFinalSize += recvCnt[i]; }
            finalOut.resize(totalFinalSize);
        }

        // Gather final data
        MPI_Gatherv(localOut.data(), outCnt, MPI_FLOAT,
                    finalOut.data(), recvCnt.data(), recvDisp.data(),
                    MPI_FLOAT, 0, MPI_COMM_WORLD);
        // --- End of Scatter/Halo Path Computation ---
    } // End else (size > 1)

    // --- Barrier after all computation and gathering ---
    MPI_Barrier(MPI_COMM_WORLD);
    auto t_end = std::chrono::high_resolution_clock::now(); // End Timer here

    // 9) Final detailed summary and time (on rank 0)
    if (rank == 0) {
        // Calculate duration
        double duration_ms = std::chrono::duration<double, std::milli>(t_end - t_start_compute).count();

        // Verify final dimensions if size > 1 (check against broadcasted finalH/W/C)
        int expectedTotalSize = finalH * finalW * finalC;
        // Need totalFinalSize if size > 1; it's only calculated in the else block.
        // Recalculate or retrieve totalFinalSize if needed here. Simpler: check finalOut size.
        if (finalOut.size() != static_cast<size_t>(expectedTotalSize) && size > 1) {
              // Note: This check might fail if finalH/W/C were incorrectly calculated initially.
              // It's mainly a sanity check for the gather process.
             std::cerr << "WARNING: Gathered size (" << finalOut.size()
                       << ") does not match expected final size ("
                       << finalH << "x" << finalW << "x" << finalC << " = " << expectedTotalSize
                       << "). Check trim/halo/layer logic." << std::endl;
        }

        // Print shape and sample values
        std::cout << "shape: " << finalH << "x" << finalW << "x" << finalC << std::endl;
        std::cout << "Sample values: ";
        int num_to_print = std::min((size_t)5, finalOut.size());
        for (int i = 0; i < num_to_print; ++i) {
            std::cout << finalOut[i] << (i == num_to_print - 1 ? "" : " ");
        }
        std::cout << std::endl;

         // Print time in expected format
        std::cout << "Execution Time: " << duration_ms << " ms" << std::endl;
    }

    MPI_Finalize();
    return 0;
}

================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/src/alexnet_mpi.cpp ===

#include "../include/alexnet.hpp"
#include "../include/layers.hpp"

void alexnetForwardPassMPI(std::vector<float>& input,
                           const LayerParams& conv1,
                           const LayerParams& conv2,
                           int H, int W, int C,
                           std::vector<float>& output)
{
    int Hc1 = convOutDim(H, conv1.F, conv1.P, conv1.S);
    int Wc1 = convOutDim(W, conv1.F, conv1.P, conv1.S);
    std::vector<float> conv1Out(Hc1 * Wc1 * conv1.K);

    serialConvLayer(conv1Out, input, conv1, H, W, C);
    serialReluLayer(conv1Out);

    int Hp1 = convOutDim(Hc1, conv1.F_pool, 0, conv1.S_pool);
    int Wp1 = convOutDim(Wc1, conv1.F_pool, 0, conv1.S_pool);
    std::vector<float> pool1Out(Hp1 * Wp1 * conv1.K);
    serialMaxPoolLayer(pool1Out, conv1Out, Hc1, Wc1, conv1.K,
                       conv1.F_pool, conv1.S_pool);

    int Hc2 = convOutDim(Hp1, conv2.F, conv2.P, conv2.S);
    int Wc2 = convOutDim(Wp1, conv2.F, conv2.P, conv2.S);
    std::vector<float> conv2Out(Hc2 * Wc2 * conv2.K);
    serialConvLayer(conv2Out, pool1Out, conv2, Hp1, Wp1, conv1.K);
    serialReluLayer(conv2Out);

    int Hp2 = convOutDim(Hc2, conv2.F_pool, 0, conv2.S_pool);
    int Wp2 = convOutDim(Wc2, conv2.F_pool, 0, conv2.S_pool);
    std::vector<float> pool2Out(Hp2 * Wp2 * conv2.K);
    serialMaxPoolLayer(pool2Out, conv2Out, Hc2, Wc2, conv2.K,
                       conv2.F_pool, conv2.S_pool);

    output.resize(pool2Out.size());
    serialLRNLayer(output, pool2Out, Hp2, Wp2, conv2.K,
                   conv2.N_lrn, conv2.alpha, conv2.beta, conv2.k_lrn);
}


================================================================================

=== FILE: final_project/v2_mpi_only/2.2_scatter_halo/src/layers_mpi.cpp ===

#include <algorithm>
#include <cmath>
#include <limits>
#include "../include/layers.hpp"

inline size_t idx3D(int h,int w,int c,int W,int C){
    return (static_cast<size_t>(h)*W + w)*C + c;
}

void serialConvLayer(std::vector<float>& out,
                     const std::vector<float>& in,
                     const LayerParams& p,
                     int H, int W, int C)
{
    int Ho = convOutDim(H, p.F, p.P, p.S);
    int Wo = convOutDim(W, p.F, p.P, p.S);
    for (int k = 0; k < p.K; ++k) {
        for (int ho = 0; ho < Ho; ++ho) {
            for (int wo = 0; wo < Wo; ++wo) {
                float sum = p.biases[k];
                for (int c = 0; c < C; ++c) {
                    for (int fh = 0; fh < p.F; ++fh) {
                        for (int fw = 0; fw < p.F; ++fw) {
                            int hi = ho*p.S - p.P + fh;
                            int wi = wo*p.S - p.P + fw;
                            if (hi<0||hi>=H||wi<0||wi>=W) continue;
                            sum += in[idx3D(hi,wi,c,W,C)] *
                                   p.weights[(((k*C + c)*p.F + fh)*p.F) + fw];
                        }
                    }
                }
                out[idx3D(ho,wo,k,Wo,p.K)] = sum;
            }
        }
    }
}

void serialReluLayer(std::vector<float>& data)
{
    for (auto &v : data) v = std::max(0.0f, v);
}

void serialMaxPoolLayer(std::vector<float>& out,
                        const std::vector<float>& in,
                        int H, int W, int C,
                        int F_pool, int S_pool)
{
    int Ho = convOutDim(H, F_pool, 0, S_pool);
    int Wo = convOutDim(W, F_pool, 0, S_pool);
    for (int h = 0; h < Ho; ++h) {
        for (int w = 0; w < Wo; ++w) {
            for (int c = 0; c < C; ++c) {
                float mx = -std::numeric_limits<float>::infinity();
                for (int fh = 0; fh < F_pool; ++fh) {
                    for (int fw = 0; fw < F_pool; ++fw) {
                        int hi = h*S_pool + fh;
                        int wi = w*S_pool + fw;
                        mx = std::max(mx, in[idx3D(hi,wi,c,W,C)]);
                    }
                }
                out[idx3D(h,w,c,Wo,C)] = mx;
            }
        }
    }
}

void serialLRNLayer(std::vector<float>& out,
                    const std::vector<float>& in,
                    int H, int W, int C,
                    int N, float alpha, float beta, float k)
{
    int half = N/2;
    for (int h = 0; h < H; ++h) {
        for (int w = 0; w < W; ++w) {
            for (int c = 0; c < C; ++c) {
                float sumSq = 0.0f;
                for (int i = std::max(0,c-half); i <= std::min(C-1,c+half); ++i) {
                    float v = in[idx3D(h,w,i,W,C)];
                    sumSq += v*v;
                }
                float denom = std::pow(k + alpha * sumSq / N, beta);
                out[idx3D(h,w,c,W,C)] =
                    in[idx3D(h,w,c,W,C)] / denom;
            }
        }
    }
}


================================================================================

=== FILE: final_project/v3_cuda_only/Makefile ===

# ───────────────────────────────────────────────────────────────
# AlexNet V3 – CUDA‑only build
# Tested with CUDA 11‑14 (nvcc >= 11.0)
# ───────────────────────────────────────────────────────────────

# Allow overriding from script, otherwise default to a common set including sm_86
ifndef HOST_CUDA_ARCH_FLAGS
  # Default for better portability, ensure sm_86 for your RTX 3090
  # You can add more architectures if you plan to run on diverse hardware
  # compute_X0 means PTX for X.0 and forwards compatibility
  EFFECTIVE_CUDA_ARCH := \
                        -gencode arch=compute_75,code=sm_75 \
                        -gencode arch=compute_86,code=sm_86 \
                        -gencode arch=compute_86,code=compute_86
else
  EFFECTIVE_CUDA_ARCH := $(HOST_CUDA_ARCH_FLAGS)
endif

NVCCFLAGS     := -std=c++11 -O3 \
                 -Xcompiler="-Wall -Wextra" \
                 -cudart=shared \
                 --cudadevrt=none        

INCLUDES      := -I./include
LDFLAGS       := -lm                      # cudart is linked automatically via -cudart=shared

SRC_DIR       := src
OBJ_DIR       := $(SRC_DIR)
SRCS          := $(SRC_DIR)/alexnet_cuda.cu \
                 $(SRC_DIR)/layers_cuda.cu \
                 $(SRC_DIR)/main_cuda.cpp
OBJS          := $(SRCS:.cu=.o)
OBJS          := $(OBJS:.cpp=.o)

TARGET        := template

# default rule
all: $(TARGET)

$(TARGET): $(OBJS)
	@echo '--- Linking V3 Target ---'
	@echo 'NVCC Command: nvcc $(NVCCFLAGS) $(EFFECTIVE_CUDA_ARCH) -o $@ $^ $(LDFLAGS)'
	@nvcc $(NVCCFLAGS) $(EFFECTIVE_CUDA_ARCH) -o $@ $^ $(LDFLAGS)

# pattern rules
$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cu
	@echo 'Compiling (CUDA): $<'
	@nvcc $(NVCCFLAGS) $(EFFECTIVE_CUDA_ARCH) $(INCLUDES) -c $< -o $@

$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cpp
	@echo 'Compiling (C++): $<'
	@nvcc $(NVCCFLAGS) $(EFFECTIVE_CUDA_ARCH) $(INCLUDES) -c $< -o $@

# housekeeping
clean:
	rm -f $(OBJ_DIR)/*.o $(TARGET)

.PHONY: all clean


================================================================================

=== FILE: final_project/v3_cuda_only/include/alexnet.hpp ===

#ifndef ALEXNET_CUDA_HPP
#define ALEXNET_CUDA_HPP

#include <vector>

// Holds layer parameters (same as V1)
struct LayerParams {
    std::vector<float> weights;
    std::vector<float> biases;
    int K, F, S, P;      // Conv params
    int F_pool, S_pool;  // Pooling
    int N_lrn;           // LRN window
    float alpha, beta, k_lrn;
};

// Runs full forward pass on GPU
// input: flattened H×W×C
// out: flattened final feature map
void alexnetForwardPassCUDA(
    const std::vector<float>& input_host,
    const LayerParams& paramsConv1,
    const LayerParams& paramsConv2,
    int H, int W, int C,
    std::vector<float>& output_host);

#endif // ALEXNET_CUDA_HPP


================================================================================

=== FILE: final_project/v3_cuda_only/include/layers.hpp ===

#ifndef LAYERS_CUDA_HPP
#define LAYERS_CUDA_HPP

#include <vector>

// Launches conv kernel: output size (Ho×Wo×K)
void cudaConvLayer(
    float* d_output,
    const float* d_input,
    const float* d_weights,
    const float* d_biases,
    int H, int W, int C,
    const int K, const int F, const int S, const int P);

// Elementwise ReLU in‐place
void cudaReluLayer(float* d_data, int N);

// Max‐pool in one kernel
void cudaMaxPoolLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int F_pool, int S_pool);

// Local response normalization
void cudaLRNLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int N, float alpha, float beta, float k);

#endif // LAYERS_CUDA_HPP


================================================================================

=== FILE: final_project/v3_cuda_only/src/main_cuda.cpp ===

#include <iostream>
#include <vector>
#include <chrono>
#include <cuda_runtime.h>
#include "../include/alexnet.hpp"

// CPU helper for computing output dims
inline int convOutDim(int D, int F, int P, int S) {
    return (D + 2*P - F) / S + 1;
}

int main() {
    // 1) Setup identical to V1
    int H=227, W=227, C=3;
    LayerParams conv1, conv2;
    std::vector<float> input(H*W*C,1.0f), output;

    conv1.K=96; conv1.F=11; conv1.S=4; conv1.P=0;
    conv1.F_pool=3; conv1.S_pool=2;
    conv1.weights.assign(conv1.K*C*conv1.F*conv1.F,0.01f);
    conv1.biases.assign(conv1.K,0.0f);

    conv2.K=256; conv2.F=5; conv2.S=1; conv2.P=2;
    conv2.F_pool=3; conv2.S_pool=2;
    conv2.N_lrn=5; conv2.alpha=1e-4f; conv2.beta=0.75f; conv2.k_lrn=2.0f;
    conv2.weights.assign(conv2.K*conv1.K*conv2.F*conv2.F,0.01f);
    conv2.biases.assign(conv2.K,0.0f);

    // 2) Run on GPU
    auto t0 = std::chrono::high_resolution_clock::now();
    alexnetForwardPassCUDA(input, conv1, conv2, H, W, C, output);
    auto t1 = std::chrono::high_resolution_clock::now();

    // 3) Print timing & sample
    double ms = std::chrono::duration<double,std::milli>(t1-t0).count();
    std::cout<<"AlexNet CUDA Forward Pass completed in "<<ms<<" ms\n";

    std::cout<<"Final Output (first 10 values):";
    for(int i=0;i<10 && i<(int)output.size();++i)
        std::cout<<" "<<output[i];
    std::cout<<"\n";

    return 0;
}


================================================================================

=== FILE: final_project/v3_cuda_only/src/alexnet_cuda.cu ===

#include <cstdio>         // for fprintf, stderr
#include <cstdlib>        // for exit
#include <cmath>          // for powf
#include <cuda_runtime.h>
#include <vector>
#include "../include/alexnet.hpp"
#include "../include/layers.hpp"

// Helper macro for checking CUDA errors
#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      std::fprintf(stderr,                                  \
        "CUDA error %s:%d '%s'\n",                         \
        __FILE__, __LINE__, cudaGetErrorString(err));       \
      std::exit(1);                                         \
    }                                                       \
  } while(0)

// Runs the sequence of Conv1→ReLU→Pool1→Conv2→ReLU→Pool2→LRN2 on device
void alexnetForwardPassCUDA(
    const std::vector<float>& input_host,
    const LayerParams& p1,
    const LayerParams& p2,
    int H,int W,int C,
    std::vector<float>& output_host)
{
    // Allocate device buffers
    float *d_input, *d_c1out, *d_p1out,
          *d_c2out, *d_p2out, *d_l2out;
    float *d_w1,*d_b1,*d_w2,*d_b2;

    // Conv1 dims
    int Hc1 = (H + 2*p1.P - p1.F)/p1.S + 1;
    int Wc1 = (W + 2*p1.P - p1.F)/p1.S + 1;
    int Hp1 = (Hc1 - p1.F_pool)/p1.S_pool + 1;
    int Wp1 = (Wc1 - p1.F_pool)/p1.S_pool + 1;

    // Conv2 dims
    int Hc2 = (Hp1 + 2*p2.P - p2.F)/p2.S + 1;
    int Wc2 = (Wp1 + 2*p2.P - p2.F)/p2.S + 1;
    int Hp2 = (Hc2 - p2.F_pool)/p2.S_pool + 1;
    int Wp2 = (Wc2 - p2.F_pool)/p2.S_pool + 1;

    // Sizes
    size_t in_sz   = (size_t)H*W*C;
    size_t c1_sz   = (size_t)Hc1*Wc1*p1.K;
    size_t p1_sz   = (size_t)Hp1*Wp1*p1.K;
    size_t c2_sz   = (size_t)Hc2*Wc2*p2.K;
    size_t p2_sz   = (size_t)Hp2*Wp2*p2.K;
    size_t l2_sz   = p2_sz;

    CUDA_CHECK(cudaMalloc(&d_input, in_sz * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_c1out, c1_sz * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_p1out, p1_sz * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_c2out, c2_sz * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_p2out, p2_sz * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_l2out, l2_sz * sizeof(float)));

    CUDA_CHECK(cudaMalloc(&d_w1, p1.weights.size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_b1, p1.biases .size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_w2, p2.weights.size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_b2, p2.biases .size()*sizeof(float)));

    // Copy host→device
    CUDA_CHECK(cudaMemcpy(d_input, input_host.data(),   in_sz*sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_w1,   p1.weights.data(),    p1.weights.size()*sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b1,   p1.biases.data(),     p1.biases .size()*sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_w2,   p2.weights.data(),    p2.weights.size()*sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b2,   p2.biases.data(),     p2.biases .size()*sizeof(float), cudaMemcpyHostToDevice));

    // 1) Conv1→ReLU→Pool1
    cudaConvLayer(d_c1out, d_input, d_w1, d_b1, H, W, C, p1.K, p1.F, p1.S, p1.P);
    cudaReluLayer(d_c1out, (int)c1_sz);
    cudaMaxPoolLayer(d_p1out, d_c1out, Hc1, Wc1, p1.K, p1.F_pool, p1.S_pool);

    // 2) Conv2→ReLU→Pool2
    cudaConvLayer(d_c2out, d_p1out, d_w2, d_b2, Hp1, Wp1, p1.K, p2.K, p2.F, p2.S, p2.P);
    cudaReluLayer(d_c2out, (int)c2_sz);
    cudaMaxPoolLayer(d_p2out, d_c2out, Hc2, Wc2, p2.K, p2.F_pool, p2.S_pool);

    // 3) LRN2
    cudaLRNLayer(d_l2out, d_p2out, Hp2, Wp2, p2.K, p2.N_lrn, p2.alpha, p2.beta, p2.k_lrn);

    // Copy result back
    output_host.resize(l2_sz);
    CUDA_CHECK(cudaMemcpy(output_host.data(), d_l2out, l2_sz*sizeof(float), cudaMemcpyDeviceToHost));

    // Free
    cudaFree(d_input);
    cudaFree(d_c1out); cudaFree(d_p1out);
    cudaFree(d_c2out); cudaFree(d_p2out); cudaFree(d_l2out);
    cudaFree(d_w1); cudaFree(d_b1); cudaFree(d_w2); cudaFree(d_b2);
}


================================================================================

=== FILE: final_project/v3_cuda_only/src/layers_cuda.cu ===

#include <cstdio>         // for fprintf, stderr
#include <cstdlib>        // for exit
#include <cmath>          // for fmaxf, powf
#include <cuda_runtime.h>
#include "../include/layers.hpp"

// Macro to check CUDA calls
#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      std::fprintf(stderr,                                  \
        "CUDA error %s:%d '%s'\n",                         \
        __FILE__, __LINE__, cudaGetErrorString(err));       \
      std::exit(1);                                         \
    }                                                       \
  } while(0)

// Conv kernel (one thread per output element)
__global__ void convKernel(
    float* out, const float* in, const float* w, const float* b,
    int H, int W, int C, int K, int F, int S, int P,
    int Ho, int Wo)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(idx >= Ho*Wo*K) return;

    int k = idx % K;
    int tmp = idx / K;
    int x = tmp % Wo;
    int y = tmp / Wo;

    float sum = 0.0f;
    for(int c=0;c<C;++c)
    for(int fy=0;fy<F;++fy)
    for(int fx=0;fx<F;++fx) {
        int in_y = y*S + fy - P;
        int in_x = x*S + fx - P;
        if(in_y>=0 && in_y<H && in_x>=0 && in_x<W) {
            int in_idx = ((in_y*W)+in_x)*C + c;
            int w_idx  = ((k*C + c)*F + fy)*F + fx;
            sum += in[in_idx] * w[w_idx];
        }
    }
    out[idx] = sum + b[k];
}

void cudaConvLayer(
    float* d_output,
    const float* d_input,
    const float* d_weights,
    const float* d_biases,
    int H, int W, int C,
    const int K, const int F, const int S, const int P)
{
    int Ho = (H + 2*P - F)/S + 1;
    int Wo = (W + 2*P - F)/S + 1;
    int total = Ho*Wo*K;
    int block = 256, grid = (total+block-1)/block;
    convKernel<<<grid,block>>>(d_output,d_input,d_weights,d_biases,
        H,W,C,K,F,S,P,Ho,Wo);
    CUDA_CHECK(cudaGetLastError());
}

// ReLU
__global__ void reluKernel(float* data, int N) {
    int i = blockIdx.x*blockDim.x + threadIdx.x;
    if(i<N) data[i] = fmaxf(0.0f, data[i]);
}

void cudaReluLayer(float* d_data, int N) {
    int block = 256, grid = (N+block-1)/block;
    reluKernel<<<grid,block>>>(d_data,N);
    CUDA_CHECK(cudaGetLastError());
}

// Max‐pool
__global__ void poolKernel(
    float* out, const float* in,
    int H, int W, int C,
    int Fp, int Sp,
    int Hp, int Wp)
{
    int idx = blockIdx.x*blockDim.x + threadIdx.x;
    if(idx>=Hp*Wp*C) return;

    int c = idx % C;
    int tmp = idx / C;
    int x = tmp % Wp;
    int y = tmp / Wp;

    float mx = -1e20f;
    for(int fy=0;fy<Fp;++fy)
    for(int fx=0;fx<Fp;++fx){
        int in_y = y*Sp + fy;
        int in_x = x*Sp + fx;
        int in_idx = ((in_y*W)+in_x)*C + c;
        mx = fmaxf(mx, in[in_idx]);
    }
    out[idx] = mx;
}

void cudaMaxPoolLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int F_pool, int S_pool)
{
    int Hp = (H - F_pool)/S_pool + 1;
    int Wp = (W - F_pool)/S_pool + 1;
    int total = Hp*Wp*C;
    int block = 256, grid = (total+block-1)/block;
    poolKernel<<<grid,block>>>(d_output,d_input,
        H,W,C,F_pool,S_pool,Hp, Wp);
    CUDA_CHECK(cudaGetLastError());
}

// LRN (naive cross‐channel)
__global__ void lrnKernel(
    float* out, const float* in,
    int H, int W, int C, int N,
    float alpha, float beta, float k)
{
    int idx = blockIdx.x*blockDim.x + threadIdx.x;
    if(idx>=H*W*C) return;

    int c = idx % C;
    int tmp = idx / C;
    int x = tmp % W;
    int y = tmp / W;

    float sum = 0.0f;
    int half = N/2;
    for(int cc = max(0,c-half); cc<=min(C-1,c+half); ++cc) {
        int ii = ((y*W)+x)*C + cc;
        sum += in[ii]*in[ii];
    }
    out[idx] = in[idx] / powf(k + alpha*sum, beta);
}

void cudaLRNLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int N, float alpha, float beta, float k)
{
    int total = H*W*C;
    int block = 256, grid = (total+block-1)/block;
    lrnKernel<<<grid,block>>>(d_output,d_input,
        H,W,C,N,alpha,beta,k);
    CUDA_CHECK(cudaGetLastError());
}


================================================================================

=== FILE: final_project/v4_mpi_cuda/Makefile ===

# ────────────────────────────────────────────────────────────────
# Version‑4  (MPI + CUDA)  Makefile
# Tested on: CUDA 12.x  •  Open MPI 5.x  •  NixOS 24.05
# Fixes:
#   • filters out all “‑Wl,*” flags nvcc rejects
#   • disables dev‑runtime, uses shared cudart
# ────────────────────────────────────────────────────────────────

DIAG_FLAGS := -fno-diagnostics-show-caret -fno-diagnostics-show-option -fmessage-length=0

NVCC    := nvcc
CXX_MPI := mpicxx

MPI_CFLAGS  := $(shell $(CXX_MPI) --showme:compile 2>/dev/null)
MPI_LDFLAGS := $(filter-out -Wl%,$(shell $(CXX_MPI) --showme:link 2>/dev/null))

# Allow overriding from script, otherwise default
ifndef HOST_CUDA_ARCH_FLAGS
  EFFECTIVE_GPU_ARCH_FLAGS := \
                        -gencode arch=compute_75,code=sm_75 \
                        -gencode arch=compute_86,code=sm_86 \
                        -gencode arch=compute_86,code=compute_86
else
  EFFECTIVE_GPU_ARCH_FLAGS := $(HOST_CUDA_ARCH_FLAGS)
endif

NVCC_FLAGS := -std=c++17 -O3 \
              -ccbin=$(CXX_MPI) \
              -Xcompiler="-Wall -Wextra $(DIAG_FLAGS) $(MPI_CFLAGS)" \
              $(EFFECTIVE_GPU_ARCH_FLAGS) \
              -cudart=shared \
              --cudadevrt=none \
              -Xcudafe "--diag_suppress=186"

$(info EFFECTIVE_GPU_ARCH_FLAGS is [$(EFFECTIVE_GPU_ARCH_FLAGS)])
$(info HOST_CUDA_ARCH_FLAGS is [$(HOST_CUDA_ARCH_FLAGS)])

INCLUDES := -I./include
TARGET   := template

SRCS_CPP := src/main_mpi_cuda.cpp
SRCS_CU  := src/alexnet_mpi_cuda.cu src/layers_mpi_cuda.cu
OBJS_CPP := $(SRCS_CPP:.cpp=.o)
OBJS_CU  := $(SRCS_CU:.cu=.o)
OBJS     := $(OBJS_CPP) $(OBJS_CU)

LDFLAGS := $(MPI_LDFLAGS) -lm

.PHONY: all clean
all: $(TARGET)

$(OBJS_CU): %.o: %.cu $(wildcard include/*.hpp) Makefile
	@echo "Compiling .cu file: $<"
	@echo "Command: $(NVCC) $(NVCC_FLAGS) $(INCLUDES) -dc $< -o $@"
	@$(NVCC) $(NVCC_FLAGS) $(INCLUDES) -dc $< -o $@

$(OBJS_CPP): %.o: %.cpp $(wildcard include/*.hpp) Makefile		
	@echo "Compiling .cpp file: $<"
	@echo "Command: $(NVCC) $(NVCC_FLAGS) $(INCLUDES) -c  $< -o $@"
	@$(NVCC) $(NVCC_FLAGS) $(INCLUDES) -c  $< -o $@

$(TARGET): $(OBJS)
	@echo "Linking target: $@"
	@echo "Command: $(NVCC) $(NVCC_FLAGS) $^ -o $@ $(LDFLAGS)"
	@$(NVCC) $(NVCC_FLAGS) $^ -o $@ $(LDFLAGS)
  
clean:
	rm -f $(TARGET) $(OBJS)


================================================================================

=== FILE: final_project/v4_mpi_cuda/include/alexnet.hpp ===

// === final_project/v4_mpi_cuda/include/alexnet.hpp ===
// (only the **new forward-declaration** added)
#ifndef ALEXNET_MPI_CUDA_HPP
#define ALEXNET_MPI_CUDA_HPP
#include <vector>
#include <cstddef>

struct LayerParams {
    std::vector<float> weights, biases;
    int K,F,S,P, F_pool,S_pool, N_lrn;
    float alpha,beta,k_lrn;
};


void alexnetForwardPassMPI_CUDA(const std::vector<float>& h_localInput,int localH,
                                int H,int W,int C,
                                const LayerParams& p1,const LayerParams& p2,
                                std::vector<float>& h_localOutput,
                                int rank,int size);

// -------------- NEW: single-GPU tile helper -----------------
void alexnetTileForwardCUDA(const float* d_input,
                            const LayerParams& p1,const LayerParams& p2,
                            int H,int W,int C,
                            float* d_output);

// size helpers (unchanged)
inline int convOutDim(int D,int F,int P,int S){
    return (S<=0||F>D+2*P) ? 0 : (D+2*P-F)/S+1;
}
inline int poolOutDim(int D,int F,int S){
    return (S<=0||D<F) ? 0 : (D-F)/S+1;
}
#endif


================================================================================

=== FILE: final_project/v4_mpi_cuda/include/layers.hpp ===

// final_project/v4_mpi_cuda/include/layers.hpp
#ifndef LAYERS_MPI_CUDA_HPP
#define LAYERS_MPI_CUDA_HPP

#include <cstddef> // Include for size_t

// CUDA Kernel Launcher Declarations
// These are host functions defined in layers_mpi_cuda.cu
// Ensure these exactly match the definitions.

void cudaConvLayer(
    float* d_output,
    const float* d_input,
    const float* d_weights,
    const float* d_biases,
    int H, int W, int C,
    const int K, const int F, const int S, const int P);

void cudaReluLayer(
    float* d_data,
    size_t N); // Use size_t consistently

void cudaMaxPoolLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int F_pool, int S_pool);

void cudaLRNLayer(
    float* d_output,
    const float* d_input,
    int H, int W, int C,
    int N, float alpha, float beta, float k);

#endif // LAYERS_MPI_CUDA_HPP

================================================================================

=== FILE: final_project/v4_mpi_cuda/src/main_mpi_cuda.cpp ===

// === final_project/v4_mpi_cuda/src/main_mpi_cuda.cpp ===
// Clean release version – fixed trim logic (no over‑trimming)
// Implements Block 1–2 AlexNet forward on MPI ranks + CUDA

#include "alexnet.hpp"
#include <mpi.h>
#include <cuda_runtime.h>
#include <vector>
#include <iostream>
#include <algorithm>
#include <chrono>
#include <iomanip>

#define CUDA_CHECK(x) do{cudaError_t e=(x);                     \
  if(e!=cudaSuccess){int r;MPI_Comm_rank(MPI_COMM_WORLD,&r);    \
    std::cerr<<"[Rank "<<r<<"] CUDA "<<cudaGetErrorString(e)<<"\n";  \
    MPI_Abort(MPI_COMM_WORLD,1);} }while(0)

/* ======================================================================== */
static void runHybridMPI(int H,int W,int C,
                         LayerParams& p1,LayerParams& p2)
{
    int rank,size; MPI_Comm_rank(MPI_COMM_WORLD,&rank);
    MPI_Comm_size(MPI_COMM_WORLD,&size);

    /* ---------------- synthetic data & params --------------------------- */
    std::vector<float> full;
    if(rank==0){
        full.assign(static_cast<size_t>(H)*W*C,1.f);
        p1.weights.assign(static_cast<size_t>(p1.K)*C*p1.F*p1.F,0.01f);
        p1.biases .assign(p1.K,0.f);
        p2.weights.assign(static_cast<size_t>(p2.K)*p1.K*p2.F*p2.F,0.01f);
        p2.biases .assign(p2.K,0.f);
    }

    auto bI=[&](int& v){ MPI_Bcast(&v,1,MPI_INT ,0,MPI_COMM_WORLD); };
    auto bF=[&](float& v){ MPI_Bcast(&v,1,MPI_FLOAT,0,MPI_COMM_WORLD); };
    bI(p1.K);bI(p1.F);bI(p1.S);bI(p1.P);bI(p1.F_pool);bI(p1.S_pool);
    bI(p2.K);bI(p2.F);bI(p2.S);bI(p2.P);bI(p2.F_pool);bI(p2.S_pool);
    bI(p2.N_lrn);bF(p2.alpha);bF(p2.beta);bF(p2.k_lrn);
    if(rank!=0){
        p1.weights.resize(static_cast<size_t>(p1.K)*C*p1.F*p1.F);
        p1.biases .resize(p1.K);
        p2.weights.resize(static_cast<size_t>(p2.K)*p1.K*p2.F*p2.F);
        p2.biases .resize(p2.K);
    }
    MPI_Bcast(p1.weights.data(),static_cast<int>(p1.weights.size()),MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(p1.biases .data(),static_cast<int>(p1.biases .size()),MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(p2.weights.data(),static_cast<int>(p2.weights.size()),MPI_FLOAT,0,MPI_COMM_WORLD);
    MPI_Bcast(p2.biases .data(),static_cast<int>(p2.biases .size()),MPI_FLOAT,0,MPI_COMM_WORLD);

    /* ---------------- scatter rows  ------------------------------------ */
    std::vector<int> rows(size),cnt(size),disp(size);
    if(rank==0){
        int base=H/size,rem=H%size,off=0;
        for(int r=0;r<size;++r){ rows[r]=base+(r<rem); cnt[r]=rows[r]*W*C; disp[r]=off; off+=cnt[r]; }
    }
    int myRows=0; MPI_Scatter(rows.data(),1,MPI_INT,&myRows,1,MPI_INT,0,MPI_COMM_WORLD);

    std::vector<float> myIn(static_cast<size_t>(myRows)*W*C);
    MPI_Scatterv(full.data(),cnt.data(),disp.data(),MPI_FLOAT,
                 myIn.data(),static_cast<int>(myIn.size()),MPI_FLOAT,0,MPI_COMM_WORLD);

    /* ---------------- halo exchange (Conv1) ----------------------------- */
    int halo = p1.F/2;            // rows
    int rowSz= W*C;               // elems per row
    if(halo){
        std::vector<float> recvT(static_cast<size_t>(halo)*rowSz),
                           recvB(static_cast<size_t>(halo)*rowSz);
        MPI_Request reqs[4]; int q=0;
        if(rank>0)        MPI_Irecv(recvT.data(),halo*rowSz,MPI_FLOAT,rank-1,0,MPI_COMM_WORLD,&reqs[q++]);
        if(rank<size-1)   MPI_Irecv(recvB.data(),halo*rowSz,MPI_FLOAT,rank+1,1,MPI_COMM_WORLD,&reqs[q++]);
        if(rank>0)        MPI_Isend(myIn.data(),halo*rowSz,MPI_FLOAT,rank-1,1,MPI_COMM_WORLD,&reqs[q++]);
        if(rank<size-1)   MPI_Isend(myIn.data()+static_cast<long>(myIn.size())-halo*rowSz,halo*rowSz,MPI_FLOAT,
                                    rank+1,0,MPI_COMM_WORLD,&reqs[q++]);
        MPI_Waitall(q,reqs,MPI_STATUSES_IGNORE);
        if(rank>0)        myIn.insert(myIn.begin(),recvT.begin(),recvT.end());
        if(rank<size-1)   myIn.insert(myIn.end()  ,recvB.begin(),recvB.end());
    }

    /* ---------------- device copy -------------------------------------- */
    float *d_in=nullptr; CUDA_CHECK(cudaMalloc(&d_in,myIn.size()*sizeof(float)));
    CUDA_CHECK(cudaMemcpy(d_in,myIn.data(),myIn.size()*sizeof(float),cudaMemcpyHostToDevice));

    /* ---------------- output dimensions --------------------------------- */
    int paddedH = static_cast<int>(myIn.size())/rowSz;
    int Hc1 = convOutDim(paddedH , p1.F, p1.P, p1.S);
    int Wc1 = convOutDim(W       , p1.F, p1.P, p1.S);
    int Hp1 = poolOutDim(Hc1, p1.F_pool, p1.S_pool);
    int Wp1 = poolOutDim(Wc1, p1.F_pool, p1.S_pool);
    int Hc2 = convOutDim(Hp1, p2.F, p2.P, p2.S);
    int Wc2 = convOutDim(Wp1, p2.F, p2.P, p2.S);
    int Hp2 = poolOutDim(Hc2, p2.F_pool, p2.S_pool);
    int Wp2 = poolOutDim(Wc2, p2.F_pool, p2.S_pool);

    /* ---------------- GPU forward pass ---------------------------------- */
    std::vector<float> tileOut(static_cast<size_t>(Hp2)*Wp2*p2.K);
    float* d_out=nullptr; CUDA_CHECK(cudaMalloc(&d_out,tileOut.size()*sizeof(float)));
    alexnetTileForwardCUDA(d_in,p1,p2,paddedH,W,C,d_out);
    CUDA_CHECK(cudaMemcpy(tileOut.data(),d_out,tileOut.size()*sizeof(float),cudaMemcpyDeviceToHost));

    /* ---------------- accurate halo rows after 2 layers ----------------- */
    int halo_pool1 = poolOutDim(halo, p1.F_pool, p1.S_pool);  // stride‑2
    int halo_pool2 = poolOutDim(halo_pool1, p2.F_pool, p2.S_pool); // second pool

    int trim_top    = (rank>0)        ? halo_pool1 + halo_pool2 : 0;
    int trim_bottom = (rank<size-1)   ? halo_pool1 + halo_pool2 : 0;

    // Clamp so at least one row remains
    int keep_rows = Hp2 - trim_top - trim_bottom;
    if(keep_rows <= 0){
        int give = 1 - keep_rows;          // rows we must give back
        int give_bottom = std::min(give, trim_bottom);
        trim_bottom -= give_bottom; give -= give_bottom;
        trim_top    -= give;               // give remaining from top (safe)
    }

    int start = trim_top;
    int stop  = Hp2 - trim_bottom; // exclusive

    std::vector<float> local(tileOut.begin()+static_cast<long>(start)*Wp2*p2.K,
                             tileOut.begin()+static_cast<long>(stop )*Wp2*p2.K);

    /* ---------------- gather ------------------------------------------- */
    int sendN=static_cast<int>(local.size());
    std::vector<int> recN(size),dispG(size);
    MPI_Gather(&sendN,1,MPI_INT,recN.data(),1,MPI_INT,0,MPI_COMM_WORLD);
    if(rank==0){ dispG[0]=0; for(int i=1;i<size;++i) dispG[i]=dispG[i-1]+recN[i-1]; full.resize(static_cast<size_t>(dispG.back()+recN.back())); }
    MPI_Gatherv(local.data(),sendN,MPI_FLOAT,
                full.data(),recN.data(),dispG.data(),MPI_FLOAT,0,MPI_COMM_WORLD);

    if(rank==0){
        std::cout<<"Final Output Shape: "<<full.size()/(Wp2*p2.K)<<"x"<<Wp2<<"x"<<p2.K<<"\n";
        std::cout<<"Final Output (first 10 values): ";
        std::cout<<std::setprecision(6);
        for(int i=0;i<10 && i<static_cast<int>(full.size());++i) std::cout<<full[i]<<(i<9?" ":"\n");
    }

    cudaFree(d_in); cudaFree(d_out);
}
/* ======================================================================== */
int main(int argc,char** argv)
{
    MPI_Init(&argc,&argv);
    LayerParams b1,b2;
    b1.K=96 ; b1.F=11; b1.S=4; b1.P=0; b1.F_pool=3; b1.S_pool=2;
    b2.K=256; b2.F=5 ; b2.S=1; b2.P=2; b2.F_pool=3; b2.S_pool=2;
    b2.N_lrn=5; b2.alpha=1e-4f; b2.beta=0.75f; b2.k_lrn=2.f;

    const int H=227,W=227,C=3;
    MPI_Barrier(MPI_COMM_WORLD);
    auto t0=std::chrono::high_resolution_clock::now();
    runHybridMPI(H,W,C,b1,b2);
    MPI_Barrier(MPI_COMM_WORLD);
    if(int rank; !MPI_Comm_rank(MPI_COMM_WORLD,&rank) && rank==0){
        auto t1=std::chrono::high_resolution_clock::now();
        std::cout<<"AlexNet MPI+CUDA Forward Pass completed in "
                 <<std::chrono::duration<double,std::milli>(t1-t0).count()
                 <<" ms\n";
    }
    MPI_Finalize();
    return 0;
}


================================================================================

=== FILE: final_project/v4_mpi_cuda/src/alexnet_mpi_cuda.cu ===

// final_project/v4_mpi_cuda/src/alexnet_mpi_cuda.cu
#include <cuda_runtime.h>
#include <mpi.h>
#include <vector>
#include <cmath>
#include <algorithm>
#include <iostream>
#include <cstdio>
#include <cstdlib>
#include <climits>
#include <cinttypes>
#ifndef SIZE_MAX
#define SIZE_MAX ((size_t)-1)
#endif

#include "../include/alexnet.hpp" // Includes LayerParams, dim helpers
#include "../include/layers.hpp"  // Includes CUDA kernel launchers

#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      int rank_for_error; MPI_Comm_rank(MPI_COMM_WORLD, &rank_for_error); \
      fprintf(stderr, "[Rank %d] CUDA error in %s:%d: %s (%d)\n", rank_for_error, __FILE__, __LINE__, cudaGetErrorString(err), err); \
      fflush(stderr); MPI_Abort(MPI_COMM_WORLD, err); } } while(0)

inline int ceil_div(int numerator, int denominator) {
    if (denominator <= 0) { return 0; }
    return (numerator + denominator - 1) / denominator;
}
inline int mapRangeStart(int in_start, int F, int P, int S) {
    if (S <= 0) return 0; int numerator = in_start - F + 1 + P; return ceil_div(numerator, S);
}
inline int mapRangeEnd(int in_end, int /*F*/, int P, int S) {
    if (S <= 0) return -1; int numerator = in_end + P;
    if (numerator >= 0) { return numerator / S; }
    else { return (numerator / S) - ((numerator % S != 0) ? 1 : 0); }
}

void alexnetForwardPassMPI_CUDA(
    const std::vector<float>& h_localInput, int localH,
    int H, int W, int C, const LayerParams& p1, const LayerParams& p2,
    std::vector<float>& h_localOutput, int rank, int size ) {

    if (localH <= 0 || H <= 0 || W <= 0 || C <= 0) { h_localOutput.clear(); return; }

    int haloRows1 = (p1.F > 1) ? p1.F / 2 : 0; int haloRows2 = (p2.F > 1) ? p2.F / 2 : 0;
    bool hasTopHalo = (rank > 0 && haloRows1 > 0); bool hasBotHalo = (rank < size - 1 && haloRows1 > 0);
    int paddedH1 = localH + (hasTopHalo ? haloRows1 : 0) + (hasBotHalo ? haloRows1 : 0);
    int Hc1 = convOutDim(paddedH1, p1.F, p1.P, p1.S); int Wc1 = convOutDim(W, p1.F, p1.P, p1.S);
    int Hp1 = poolOutDim(Hc1, p1.F_pool, p1.S_pool); int Wp1 = poolOutDim(Wc1, p1.F_pool, p1.S_pool); int C1 = p1.K;

    int myGlobalStartRow = 0;
    if (size > 0 && H > 0) { int baseRows = H / size; int extraRows = H % size; for(int r=0; r<rank; ++r) { myGlobalStartRow += baseRows + (r < extraRows ? 1 : 0); } } else if (H <= 0) { fprintf(stderr, "[Rank %d] Error: H invalid (%d).\n", rank, H); MPI_Abort(MPI_COMM_WORLD, 1); }
    int myGlobalEndRow = myGlobalStartRow + localH - 1;
    if (myGlobalEndRow >= H) { fprintf(stderr, "[Rank %d] Error: Global end row (%d) >= H (%d).\n", rank, myGlobalEndRow, H); MPI_Abort(MPI_COMM_WORLD, 1); }

    int conv1_start_ho = mapRangeStart(myGlobalStartRow, p1.F, p1.P, p1.S); int conv1_end_ho = mapRangeEnd(myGlobalEndRow, p1.F, p1.P, p1.S);
    int pool1_start_ho = mapRangeStart(conv1_start_ho, p1.F_pool, 0, p1.S_pool); int pool1_end_ho = mapRangeEnd(conv1_end_ho, p1.F_pool, 0, p1.S_pool);
    int validHp1 = (pool1_end_ho >= pool1_start_ho) ? (pool1_end_ho - pool1_start_ho + 1) : 0;
    int global_conv1_start_for_row0 = mapRangeStart(0, p1.F, p1.P, p1.S); int global_pool1_start_for_row0 = mapRangeStart(global_conv1_start_for_row0, p1.F_pool, 0, p1.S_pool);
    int trimTop1 = pool1_start_ho - global_pool1_start_for_row0;
    if (trimTop1 < 0) trimTop1 = 0;
    if (trimTop1 >= Hp1) { trimTop1 = (Hp1 > 0 ? Hp1 : 0); validHp1 = 0; } else if (Hp1 > 0) { if (trimTop1 + validHp1 > Hp1) { validHp1 = std::max(0, Hp1 - trimTop1); } } else { validHp1 = 0; trimTop1 = 0; }
    if (validHp1 < 0) validHp1 = 0;
    fprintf(stderr, "[Rank %d] Pool1 Trim: Hp1=%d, global_pool1_range=[%d, %d], validHp1=%d, trimTop1=%d\n", rank, Hp1, pool1_start_ho, pool1_end_ho, validHp1, trimTop1);

    bool hasTopHalo2 = (rank > 0 && haloRows2 > 0 && validHp1 > 0); bool hasBotHalo2 = (rank < size - 1 && haloRows2 > 0 && validHp1 > 0);
    int paddedH2 = validHp1 + (hasTopHalo2 ? haloRows2 : 0) + (hasBotHalo2 ? haloRows2 : 0);
    int Hc2 = convOutDim(paddedH2, p2.F, p2.P, p2.S); int Wc2 = convOutDim(Wp1, p2.F, p2.P, p2.S);
    int Hp2 = poolOutDim(Hc2, p2.F_pool, p2.S_pool); int Wp2 = poolOutDim(Wc2, p2.F_pool, p2.S_pool); int C2 = p2.K;
    //fprintf(stderr, "[Rank %d] Calc2: validHp1=%d, paddedH2=%d, Hc2=%d, Wc2=%d, Hp2=%d, Wp2=%d, C2=%d\n", rank, validHp1, paddedH2, Hc2, Wc2, Hp2, Wp2, C2);

    int global_pool1_start = pool1_start_ho; int global_pool1_end = pool1_end_ho;
    int conv2_start_ho = mapRangeStart(global_pool1_start, p2.F, p2.P, p2.S); int conv2_end_ho = mapRangeEnd(global_pool1_end, p2.F, p2.P, p2.S);
    int pool2_start_ho = mapRangeStart(conv2_start_ho, p2.F_pool, 0, p2.S_pool); int pool2_end_ho = mapRangeEnd(conv2_end_ho, p2.F_pool, 0, p2.S_pool);
    int finalLocalH = (pool2_end_ho >= pool2_start_ho) ? (pool2_end_ho - pool2_start_ho + 1) : 0;
    int global_conv2_start_for_row0 = mapRangeStart(global_pool1_start_for_row0, p2.F, p2.P, p2.S); int global_pool2_start_for_row0 = mapRangeStart(global_conv2_start_for_row0, p2.F_pool, 0, p2.S_pool);
    int trimTop2 = pool2_start_ho - global_pool2_start_for_row0;
    if (trimTop2 < 0) trimTop2 = 0;
    if (trimTop2 >= Hp2) { trimTop2 = (Hp2 > 0 ? Hp2 : 0); finalLocalH = 0; } else if (Hp2 > 0) { if (trimTop2 + finalLocalH > Hp2) { finalLocalH = std::max(0, Hp2 - trimTop2); } } else { trimTop2 = 0; finalLocalH = 0; }
    if (finalLocalH < 0) finalLocalH = 0;
    fprintf(stderr, "[Rank %d] Pool2 Trim: Hp2=%d, global_pool2_range=[%d, %d], finalLocalH=%d, trimTop2=%d\n", rank, Hp2, pool2_start_ho, pool2_end_ho, finalLocalH, trimTop2);

    float *d_input_padded1=nullptr, *d_conv1_out=nullptr, *d_pool1_out=nullptr, *d_input_padded2=nullptr, *d_conv2_out=nullptr, *d_pool2_out=nullptr, *d_lrn2_out=nullptr;
    float *d_weights1=nullptr, *d_biases1=nullptr, *d_weights2=nullptr, *d_biases2=nullptr;
    size_t inputPadded1Size=(size_t)paddedH1*W*C, conv1OutSize=(size_t)Hc1*Wc1*C1, pool1OutSize=(size_t)Hp1*Wp1*C1, inputPadded2Size=(size_t)paddedH2*Wp1*C1, conv2OutSize=(size_t)Hc2*Wc2*C2, pool2OutSize=(size_t)Hp2*Wp2*C2, lrn2OutSize=pool2OutSize;
    size_t w1Size=p1.weights.size(), b1Size=p1.biases.size(), w2Size=p2.weights.size(), b2Size=p2.biases.size();
    if(inputPadded1Size>0) CUDA_CHECK(cudaMalloc(&d_input_padded1, inputPadded1Size*sizeof(float))); if(conv1OutSize>0) CUDA_CHECK(cudaMalloc(&d_conv1_out, conv1OutSize*sizeof(float))); if(pool1OutSize>0) CUDA_CHECK(cudaMalloc(&d_pool1_out, pool1OutSize*sizeof(float))); if(inputPadded2Size>0) CUDA_CHECK(cudaMalloc(&d_input_padded2, inputPadded2Size*sizeof(float))); if(conv2OutSize>0) CUDA_CHECK(cudaMalloc(&d_conv2_out, conv2OutSize*sizeof(float))); if(pool2OutSize>0) CUDA_CHECK(cudaMalloc(&d_pool2_out, pool2OutSize*sizeof(float))); if(lrn2OutSize>0) CUDA_CHECK(cudaMalloc(&d_lrn2_out, lrn2OutSize*sizeof(float)));
    if(w1Size>0) CUDA_CHECK(cudaMalloc(&d_weights1, w1Size*sizeof(float))); if(b1Size>0) CUDA_CHECK(cudaMalloc(&d_biases1, b1Size*sizeof(float))); if(w2Size>0) CUDA_CHECK(cudaMalloc(&d_weights2, w2Size*sizeof(float))); if(b2Size>0) CUDA_CHECK(cudaMalloc(&d_biases2, b2Size*sizeof(float)));

    if(w1Size>0 && !p1.weights.empty() && d_weights1) CUDA_CHECK(cudaMemcpy(d_weights1,p1.weights.data(),w1Size*sizeof(float),cudaMemcpyHostToDevice)); if(b1Size>0 && !p1.biases.empty() && d_biases1) CUDA_CHECK(cudaMemcpy(d_biases1,p1.biases.data(),b1Size*sizeof(float),cudaMemcpyHostToDevice)); if(w2Size>0 && !p2.weights.empty() && d_weights2) CUDA_CHECK(cudaMemcpy(d_weights2,p2.weights.data(),w2Size*sizeof(float),cudaMemcpyHostToDevice)); if(b2Size>0 && !p2.biases.empty() && d_biases2) CUDA_CHECK(cudaMemcpy(d_biases2,p2.biases.data(),b2Size*sizeof(float),cudaMemcpyHostToDevice));
    size_t localInputSizeBytes=h_localInput.size()*sizeof(float); size_t inputRowSizeBytes=(W>0&&C>0)?(size_t)W*C*sizeof(float):0; size_t inputHalo1SizeBytes=(size_t)haloRows1*inputRowSizeBytes; size_t inputOffsetElements=(hasTopHalo?inputHalo1SizeBytes:0)/sizeof(float);
    if(localInputSizeBytes>0 && d_input_padded1){ if(inputOffsetElements*sizeof(float)+localInputSizeBytes > inputPadded1Size*sizeof(float)){fprintf(stderr,"[Rank %d] Error: Initial H->D copy bounds.\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} CUDA_CHECK(cudaMemcpy(d_input_padded1+inputOffsetElements,h_localInput.data(),localInputSizeBytes,cudaMemcpyHostToDevice));}

    if(haloRows1>0 && inputRowSizeBytes>0){
        MPI_Request s_up=MPI_REQUEST_NULL, r_up=MPI_REQUEST_NULL, s_down=MPI_REQUEST_NULL, r_down=MPI_REQUEST_NULL;
        std::vector<float> hs_up(haloRows1*W*C),hr_up(haloRows1*W*C),hs_down(haloRows1*W*C),hr_down(haloRows1*W*C);
        int p_up=rank-1, p_down=rank+1; int t_up=0,t_down=1; int h_count=inputHalo1SizeBytes/sizeof(float); if(h_count<=0) goto skip_halo1;
        //fprintf(stderr,"[Rank %d] Halo1: Exchanging %d elem.\n",rank,h_count);
        if(hasTopHalo){MPI_Irecv(hr_up.data(),h_count,MPI_FLOAT,p_up,t_up,MPI_COMM_WORLD,&r_up);} if(hasBotHalo){MPI_Irecv(hr_down.data(),h_count,MPI_FLOAT,p_down,t_down,MPI_COMM_WORLD,&r_down);}
        if(hasBotHalo && d_input_padded1){ size_t offset=inputOffsetElements+(localInputSizeBytes/sizeof(float))-h_count; if(offset*sizeof(float)>=0 && offset+h_count<=inputPadded1Size){ /*fprintf(stderr,"[R%d] H1 S_UP D->H Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)offset,h_count);*/ CUDA_CHECK(cudaMemcpy(hs_up.data(),d_input_padded1+offset,inputHalo1SizeBytes,cudaMemcpyDeviceToHost)); MPI_Isend(hs_up.data(),h_count,MPI_FLOAT,p_down,t_up,MPI_COMM_WORLD,&s_up); }else{fprintf(stderr,"[R%d] Err H1 S_UP Off/Bnds\n",rank); MPI_Abort(MPI_COMM_WORLD,1);}}
        if(hasTopHalo && d_input_padded1){ size_t offset=inputOffsetElements; if(offset*sizeof(float)>=0 && offset+h_count<=inputPadded1Size){ /*fprintf(stderr,"[R%d] H1 S_DN D->H Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)offset,h_count);*/ CUDA_CHECK(cudaMemcpy(hs_down.data(),d_input_padded1+offset,inputHalo1SizeBytes,cudaMemcpyDeviceToHost)); MPI_Isend(hs_down.data(),h_count,MPI_FLOAT,p_up,t_down,MPI_COMM_WORLD,&s_down); }else{fprintf(stderr,"[R%d] Err H1 S_DN Off/Bnds\n",rank); MPI_Abort(MPI_COMM_WORLD,1);}}
        MPI_Status status;
        if(hasTopHalo){/*fprintf(stderr,"[R%d] H1 Wait RecvUp...\n",rank);*/ MPI_Wait(&r_up,&status); /*fprintf(stderr,"[R%d] H1 Wait RecvUp DONE.\n",rank);*/ if(d_input_padded1 && inputHalo1SizeBytes>0){if(inputHalo1SizeBytes > inputPadded1Size*sizeof(float)){fprintf(stderr,"[R%d] Err H1 RcvUp Dest Small\n",rank); MPI_Abort(MPI_COMM_WORLD,1);} /*fprintf(stderr,"[R%d] H1 RecvUp H->D Off=0 Cnt=%d\n",rank,h_count);*/ CUDA_CHECK(cudaMemcpy(d_input_padded1,hr_up.data(),inputHalo1SizeBytes,cudaMemcpyHostToDevice));} if(s_down!=MPI_REQUEST_NULL){/*fprintf(stderr,"[R%d] H1 Wait SendDn...\n",rank);*/ MPI_Wait(&s_down,&status); /*fprintf(stderr,"[R%d] H1 Wait SendDn DONE.\n",rank);*/}}
        if(hasBotHalo){/*fprintf(stderr,"[R%d] H1 Wait RecvDn...\n",rank);*/ MPI_Wait(&r_down,&status); /*fprintf(stderr,"[R%d] H1 Wait RecvDn DONE.\n",rank);*/ size_t offset=inputOffsetElements+(localInputSizeBytes/sizeof(float)); if(d_input_padded1 && inputHalo1SizeBytes>0 && offset+h_count<=inputPadded1Size){ /*fprintf(stderr,"[R%d] H1 RecvDn H->D Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)offset,h_count);*/ CUDA_CHECK(cudaMemcpy(d_input_padded1+offset,hr_down.data(),inputHalo1SizeBytes,cudaMemcpyHostToDevice));}else if(inputHalo1SizeBytes>0){fprintf(stderr,"[R%d] Err H1 RcvDn Off/Bnds\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} if(s_up!=MPI_REQUEST_NULL){/*fprintf(stderr,"[R%d] H1 Wait SendUp...\n",rank);*/ MPI_Wait(&s_up,&status); /*fprintf(stderr,"[R%d] H1 Wait SendUp DONE.\n",rank);*/}}
    } skip_halo1:;

    if(Hc1>0 && Wc1>0 && C1>0 && d_conv1_out && d_input_padded1 && d_weights1 && d_biases1){ /*fprintf(stderr,"[R%d] Launch Conv1\n",rank);*/ cudaConvLayer(d_conv1_out,d_input_padded1,d_weights1,d_biases1,paddedH1,W,C,p1.K,p1.F,p1.S,p1.P); cudaReluLayer(d_conv1_out,conv1OutSize);}
    if(Hp1>0 && Wp1>0 && C1>0 && d_pool1_out && d_conv1_out){ /*fprintf(stderr,"[R%d] Launch Pool1\n",rank);*/ cudaMaxPoolLayer(d_pool1_out,d_conv1_out,Hc1,Wc1,C1,p1.F_pool,p1.S_pool);}

    size_t pool1RowSizeBytes=(Wp1>0 && C1>0)?(size_t)Wp1*C1*sizeof(float):0;
    if(haloRows2>0 && validHp1>0 && pool1RowSizeBytes>0 && d_pool1_out){
        MPI_Request s_up=MPI_REQUEST_NULL, r_up=MPI_REQUEST_NULL, s_down=MPI_REQUEST_NULL, r_down=MPI_REQUEST_NULL;
        size_t pool1HaloSizeBytes=(size_t)haloRows2*pool1RowSizeBytes; if(haloRows2>0 && pool1RowSizeBytes>SIZE_MAX/(unsigned long)haloRows2){fprintf(stderr,"[R%d] Err H2 Size Ovflw\n",rank); MPI_Abort(MPI_COMM_WORLD,1);} if(pool1HaloSizeBytes==0) goto skip_halo2;
        std::vector<float> hs_up(pool1HaloSizeBytes/sizeof(float)),hr_up(pool1HaloSizeBytes/sizeof(float)),hs_down(pool1HaloSizeBytes/sizeof(float)),hr_down(pool1HaloSizeBytes/sizeof(float));
        int p_up=rank-1, p_down=rank+1; int t_up=2,t_down=3; int h_count=pool1HaloSizeBytes/sizeof(float); if(h_count<=0) goto skip_halo2;
        //fprintf(stderr,"[R%d] Halo2: Exchanging %d elem.\n",rank,h_count);
        size_t offsetBytes=(size_t)trimTop1*pool1RowSizeBytes; if(offsetBytes>pool1OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err H2 Trim Offset\n",rank);MPI_Abort(MPI_COMM_WORLD,1);}
        float* valid_start=d_pool1_out+offsetBytes/sizeof(float); size_t validSizeBytes=(size_t)validHp1*pool1RowSizeBytes; if(offsetBytes+validSizeBytes > pool1OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err H2 Valid Region Bnds\n",rank);MPI_Abort(MPI_COMM_WORLD,1);}

        if(hasTopHalo2){MPI_Irecv(hr_up.data(),h_count,MPI_FLOAT,p_up,t_up,MPI_COMM_WORLD,&r_up);} if(hasBotHalo2){MPI_Irecv(hr_down.data(),h_count,MPI_FLOAT,p_down,t_down,MPI_COMM_WORLD,&r_down);}
        if(hasBotHalo2){size_t offset=validSizeBytes-pool1HaloSizeBytes; if(pool1HaloSizeBytes>validSizeBytes){fprintf(stderr,"[R%d] Err H2 S_UP Size %" PRIu64 ">%" PRIu64 "\n",rank,(uint64_t)pool1HaloSizeBytes,(uint64_t)validSizeBytes); MPI_Abort(MPI_COMM_WORLD,1);} if(offsetBytes+offset+pool1HaloSizeBytes > pool1OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err H2 S_UP Src Bnds\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} /*fprintf(stderr,"[R%d] H2 S_UP D->H Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)(offsetBytes/sizeof(float)+offset/sizeof(float)),h_count);*/ CUDA_CHECK(cudaMemcpy(hs_up.data(),valid_start+offset/sizeof(float),pool1HaloSizeBytes,cudaMemcpyDeviceToHost)); MPI_Isend(hs_up.data(),h_count,MPI_FLOAT,p_down,t_up,MPI_COMM_WORLD,&s_up);}
        if(hasTopHalo2){if(pool1HaloSizeBytes>validSizeBytes){fprintf(stderr,"[R%d] Err H2 S_DN Size %" PRIu64 ">%" PRIu64 "\n",rank,(uint64_t)pool1HaloSizeBytes,(uint64_t)validSizeBytes);MPI_Abort(MPI_COMM_WORLD,1);} if(offsetBytes+pool1HaloSizeBytes > pool1OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err H2 S_DN Src Bnds\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} /*fprintf(stderr,"[R%d] H2 S_DN D->H Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)(offsetBytes/sizeof(float)),h_count);*/ CUDA_CHECK(cudaMemcpy(hs_down.data(),valid_start,pool1HaloSizeBytes,cudaMemcpyDeviceToHost)); MPI_Isend(hs_down.data(),h_count,MPI_FLOAT,p_up,t_down,MPI_COMM_WORLD,&s_down);}

        size_t currentOffsetBytes=0;
        if(hasTopHalo2){MPI_Wait(&r_up,MPI_STATUS_IGNORE); if(d_input_padded2 && pool1HaloSizeBytes>0 && pool1HaloSizeBytes<=inputPadded2Size*sizeof(float)){/*fprintf(stderr,"[R%d] H2 RcvUp H->D Off=0 Cnt=%d\n",rank,h_count);*/ CUDA_CHECK(cudaMemcpy(d_input_padded2,hr_up.data(),pool1HaloSizeBytes,cudaMemcpyHostToDevice));}else if(pool1HaloSizeBytes>0){fprintf(stderr,"[R%d] Err H2 RcvUp Dest Bnds/Null\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} currentOffsetBytes+=pool1HaloSizeBytes; if(s_down!=MPI_REQUEST_NULL){MPI_Wait(&s_down,MPI_STATUS_IGNORE);}}
        if(validHp1>0 && validSizeBytes>0 && d_input_padded2){if(currentOffsetBytes+validSizeBytes > inputPadded2Size*sizeof(float)){fprintf(stderr,"[R%d] Err H2 D->D Dest Bnds\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} /*fprintf(stderr,"[R%d] H2 Copy D->D DestOff=%" PRIu64 " SrcOff=%" PRIu64 " Cnt=%" PRIu64 "\n",rank,(uint64_t)(currentOffsetBytes/sizeof(float)),(uint64_t)(offsetBytes/sizeof(float)),(uint64_t)(validSizeBytes/sizeof(float)));*/ CUDA_CHECK(cudaMemcpy(d_input_padded2+currentOffsetBytes/sizeof(float),valid_start,validSizeBytes,cudaMemcpyDeviceToDevice)); currentOffsetBytes+=validSizeBytes;}
        if(hasBotHalo2){MPI_Wait(&r_down,MPI_STATUS_IGNORE); if(d_input_padded2 && pool1HaloSizeBytes>0 && currentOffsetBytes+pool1HaloSizeBytes <= inputPadded2Size*sizeof(float)){/*fprintf(stderr,"[R%d] H2 RcvDn H->D Off=%" PRIu64 " Cnt=%d\n",rank,(uint64_t)(currentOffsetBytes/sizeof(float)),h_count);*/ CUDA_CHECK(cudaMemcpy(d_input_padded2+currentOffsetBytes/sizeof(float),hr_down.data(),pool1HaloSizeBytes,cudaMemcpyHostToDevice));}else if(pool1HaloSizeBytes>0){fprintf(stderr,"[R%d] Err H2 RcvDn Dest Bnds/Null\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} if(s_up!=MPI_REQUEST_NULL){MPI_Wait(&s_up,MPI_STATUS_IGNORE);}}

    }else if(validHp1>0 && pool1RowSizeBytes>0 && d_pool1_out && d_input_padded2){
        size_t offsetBytes=(size_t)trimTop1*pool1RowSizeBytes; float* valid_start=d_pool1_out+offsetBytes/sizeof(float); size_t validSizeBytes=(size_t)validHp1*pool1RowSizeBytes;
        if(offsetBytes+validSizeBytes > pool1OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err H2 D->D Src Bnds (NoHalo)\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} if(validSizeBytes > inputPadded2Size*sizeof(float)){fprintf(stderr,"[R%d] Err H2 D->D Dest Bnds (NoHalo)\n",rank);MPI_Abort(MPI_COMM_WORLD,1);}
        if(validSizeBytes>0){/*fprintf(stderr,"[R%d] H2 Copy D->D (NoHalo) DestOff=0 SrcOff=%" PRIu64 " Cnt=%" PRIu64 "\n",rank,(uint64_t)(offsetBytes/sizeof(float)),(uint64_t)(validSizeBytes/sizeof(float)));*/ CUDA_CHECK(cudaMemcpy(d_input_padded2,valid_start,validSizeBytes,cudaMemcpyDeviceToDevice));}
    }else if(d_input_padded2 && inputPadded2Size>0){/*fprintf(stderr,"[R%d] H2 Zeroing\n",rank);*/ CUDA_CHECK(cudaMemset(d_input_padded2,0,inputPadded2Size*sizeof(float)));}
    skip_halo2:;

    if(Hc2>0 && Wc2>0 && C2>0 && d_conv2_out && d_input_padded2 && d_weights2 && d_biases2){/*fprintf(stderr,"[R%d] Launch Conv2\n",rank);*/ cudaConvLayer(d_conv2_out,d_input_padded2,d_weights2,d_biases2,paddedH2,Wp1,C1,p2.K,p2.F,p2.S,p2.P); cudaReluLayer(d_conv2_out,conv2OutSize);}
    if(Hp2>0 && Wp2>0 && C2>0 && d_pool2_out && d_conv2_out){/*fprintf(stderr,"[R%d] Launch Pool2\n",rank);*/ cudaMaxPoolLayer(d_pool2_out,d_conv2_out,Hc2,Wc2,C2,p2.F_pool,p2.S_pool);}
    if(Hp2>0 && Wp2>0 && C2>0 && d_lrn2_out && d_pool2_out){/*fprintf(stderr,"[R%d] Launch LRN2\n",rank);*/ cudaLRNLayer(d_lrn2_out,d_pool2_out,Hp2,Wp2,C2,p2.N_lrn,p2.alpha,p2.beta,p2.k_lrn);}

    size_t finalRowSizeBytes=(Wp2>0 && C2>0)?(size_t)Wp2*C2*sizeof(float):0; size_t finalLocalSizeBytes=(size_t)finalLocalH*finalRowSizeBytes;
    if(finalLocalSizeBytes>0 && finalLocalH>0){if((unsigned long)finalLocalH > SIZE_MAX/((unsigned long)Wp2*C2)){fprintf(stderr,"[R%d] Err Final Size Ovflw\n",rank);MPI_Abort(MPI_COMM_WORLD,1);} h_localOutput.resize(finalLocalSizeBytes/sizeof(float));} else {h_localOutput.clear(); finalLocalSizeBytes=0;}
    if(finalLocalH>0 && finalLocalSizeBytes>0 && d_lrn2_out){
        size_t offsetBytes=(size_t)trimTop2*finalRowSizeBytes;
        if(offsetBytes+finalLocalSizeBytes > lrn2OutSize*sizeof(float)){fprintf(stderr,"[R%d] Err Final D->H Src Bnds Off=%" PRIu64 " Sz=%" PRIu64 " BufSz=%" PRIu64 " finalH=%d trimT2=%d Hp2=%d Wp2=%d C2=%d\n",rank,(uint64_t)offsetBytes,(uint64_t)finalLocalSizeBytes,(uint64_t)(lrn2OutSize*sizeof(float)),finalLocalH,trimTop2,Hp2,Wp2,C2); MPI_Abort(MPI_COMM_WORLD,1);}
        //fprintf(stderr,"[R%d] Final D->H DestSz=%" PRIu64 " SrcOff=%" PRIu64 " CpySz=%" PRIu64 "\n",rank,(uint64_t)h_localOutput.size(),(uint64_t)(offsetBytes/sizeof(float)),(uint64_t)(finalLocalSizeBytes/sizeof(float)));
        CUDA_CHECK(cudaMemcpy(h_localOutput.data(),d_lrn2_out+offsetBytes/sizeof(float),finalLocalSizeBytes,cudaMemcpyDeviceToHost));
    }

    if(d_input_padded1) cudaFree(d_input_padded1); if(d_conv1_out) cudaFree(d_conv1_out); if(d_pool1_out) cudaFree(d_pool1_out); if(d_input_padded2) cudaFree(d_input_padded2); if(d_conv2_out) cudaFree(d_conv2_out); if(d_pool2_out) cudaFree(d_pool2_out); if(d_lrn2_out) cudaFree(d_lrn2_out);
    if(d_weights1) cudaFree(d_weights1); if(d_biases1) cudaFree(d_biases1); if(d_weights2) cudaFree(d_weights2); if(d_biases2) cudaFree(d_biases2);
    //fprintf(stderr, "[Rank %d] alexnetForwardPassMPI_CUDA finished.\n", rank);
}

// === append to the end of final_project/v4_mpi_cuda/src/alexnet_mpi_cuda.cu ===
void alexnetTileForwardCUDA(const float* d_input,
                            const LayerParams& p1,const LayerParams& p2,
                            int H,int W,int C,float* d_output)
{
    int Hc1=convOutDim(H ,p1.F,p1.P,p1.S);
    int Wc1=convOutDim(W ,p1.F,p1.P,p1.S);
    int Hp1=poolOutDim(Hc1,p1.F_pool,p1.S_pool);
    int Wp1=poolOutDim(Wc1,p1.F_pool,p1.S_pool);
    int C1 =p1.K;

    int Hc2=convOutDim(Hp1,p2.F,p2.P,p2.S);
    int Wc2=convOutDim(Wp1,p2.F,p2.P,p2.S);
    int Hp2=poolOutDim(Hc2,p2.F_pool,p2.S_pool);
    int Wp2=poolOutDim(Wc2,p2.F_pool,p2.S_pool);
    int C2 =p2.K;

    size_t nConv1=(size_t)Hc1*Wc1*C1, nPool1=(size_t)Hp1*Wp1*C1,
           nConv2=(size_t)Hc2*Wc2*C2, nPool2=(size_t)Hp2*Wp2*C2;

    float *d_c1,*d_p1,*d_c2,*d_p2;
    CUDA_CHECK(cudaMalloc(&d_c1,nConv1*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_p1,nPool1*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_c2,nConv2*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_p2,nPool2*sizeof(float)));

    float *dw1,*db1,*dw2,*db2;
    CUDA_CHECK(cudaMalloc(&dw1,p1.weights.size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&db1,p1.biases .size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&dw2,p2.weights.size()*sizeof(float)));
    CUDA_CHECK(cudaMalloc(&db2,p2.biases .size()*sizeof(float)));

    CUDA_CHECK(cudaMemcpy(dw1,p1.weights.data(),p1.weights.size()*sizeof(float),cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(db1,p1.biases .data(),p1.biases .size()*sizeof(float),cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(dw2,p2.weights.data(),p2.weights.size()*sizeof(float),cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(db2,p2.biases .data(),p2.biases .size()*sizeof(float),cudaMemcpyHostToDevice));

    cudaConvLayer(d_c1,d_input ,dw1,db1,H ,W ,C ,p1.K,p1.F,p1.S,p1.P);
    cudaReluLayer(d_c1,nConv1);
    cudaMaxPoolLayer(d_p1,d_c1,Hc1,Wc1,C1,p1.F_pool,p1.S_pool);

    cudaConvLayer(d_c2,d_p1,dw2,db2,Hp1,Wp1,C1,p2.K,p2.F,p2.S,p2.P);
    cudaReluLayer(d_c2,nConv2);
    cudaMaxPoolLayer(d_p2,d_c2,Hc2,Wc2,C2,p2.F_pool,p2.S_pool);

    cudaLRNLayer(d_output,d_p2,Hp2,Wp2,C2,p2.N_lrn,p2.alpha,p2.beta,p2.k_lrn);

    cudaFree(d_c1); cudaFree(d_p1); cudaFree(d_c2); cudaFree(d_p2);
    cudaFree(dw1);  cudaFree(db1);  cudaFree(dw2);  cudaFree(db2);
}


================================================================================

=== FILE: final_project/v4_mpi_cuda/src/layers_mpi_cuda.cu ===

// final_project/v4_mpi_cuda/src/layers_mpi_cuda.cu
#include <cstdio>
#include <cstdlib>
#include <cmath>
#include <cuda_runtime.h>
#include <mpi.h>
#include <cstddef>
#include <algorithm> // Include for std::max, std::min used by device kernels

#include "../include/layers.hpp"
#include "../include/alexnet.hpp" // Include alexnet.hpp for convOutDim/poolOutDim

// Macro to check CUDA calls - ABORTS using MPI for coordinated shutdown
#define CUDA_CHECK(call)                                    \
  do {                                                      \
    cudaError_t err = call;                                 \
    if(err != cudaSuccess) {                                \
      int rank_for_error=-1; MPI_Comm_rank(MPI_COMM_WORLD, &rank_for_error); \
      fprintf(stderr, "[Rank %d] CUDA error in %s:%d: %s (%d)\n", rank_for_error, __FILE__, __LINE__, cudaGetErrorString(err), err); \
      fflush(stderr); MPI_Abort(MPI_COMM_WORLD, err); } } while(0)


// --- Kernel Implementations ---

__global__ void convKernel(
    float* __restrict__ out, const float* __restrict__ in,
    const float* __restrict__ w, const float* __restrict__ b,
    int H, int W, int C, int K, int F, int S, int P,
    int Ho, int Wo)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_outputs = Ho * Wo * K; if (idx >= total_outputs) return;
    int k = idx % K; int temp = idx / K; int x = temp % Wo; int y = temp / Wo;
    float sum = b[k]; // Initialize with bias
    int start_y = y * S - P; int start_x = x * S - P;
    for (int c = 0; c < C; ++c) {
        for (int fy = 0; fy < F; ++fy) { int current_y = start_y + fy;
            for (int fx = 0; fx < F; ++fx) { int current_x = start_x + fx;
                if (current_y >= 0 && current_y < H && current_x >= 0 && current_x < W) {
                    size_t in_idx = ((size_t)current_y * W + current_x) * C + c;
                    size_t w_idx = (((size_t)k * C + c) * F + fy) * F + fx;
                    sum += in[in_idx] * w[w_idx];
                }
            }
        }
    } out[idx] = sum;
}

__global__ void reluKernel(float* data, size_t N) {
    size_t i = static_cast<size_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if (i < N) { data[i] = fmaxf(0.0f, data[i]); }
}

__global__ void poolKernel(
    float* __restrict__ out, const float* __restrict__ in,
    int H, int W, int C, int Fp, int Sp, int Hp, int Wp)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_outputs = Hp * Wp * C; if (idx >= total_outputs) return;
    int c = idx % C; int temp = idx / C; int y = temp / Wp; int x = temp % Wp;
    int start_y = y * Sp; int start_x = x * Sp;
    float max_val = -INFINITY;
    for (int fy = 0; fy < Fp; ++fy) { int current_y = start_y + fy; if (current_y >= H) continue;
        for (int fx = 0; fx < Fp; ++fx) { int current_x = start_x + fx; if (current_x >= W) continue;
            size_t in_idx = ((size_t)current_y * W + current_x) * C + c;
            max_val = fmaxf(max_val, in[in_idx]);
        }
    } out[idx] = max_val;
}

__global__ void lrnKernel(
    float* __restrict__ out, const float* __restrict__ in,
    int H, int W, int C, int N_lrn, float alpha, float beta, float k)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = H * W * C; if (idx >= total_elements) return;
    int c = idx % C; int temp = idx / C; int x = temp % W; int y = temp / W;
    float sum_sq = 0.0f; int half_N = N_lrn / 2;
    // Use std:: namespace for max/min inside kernel requires including <algorithm> and potentially modifying build flags if not already handled
    // Using standard max/min functions which should be available in CUDA context
    int c_start = max(0, c - half_N); int c_end = min(C - 1, c + half_N);
    for (int channel_i = c_start; channel_i <= c_end; ++channel_i) {
        size_t neighbor_idx = ((size_t)y * W + x) * C + channel_i;
        float val = in[neighbor_idx]; sum_sq += val * val;
    }
    float scale = k + alpha * sum_sq;
    if (scale <= 0.0f && beta > 0.0f && beta != 1.0f) { out[idx] = 0.0f; }
    else { out[idx] = in[idx] * powf(scale, -beta); }
}


// --- Kernel Launcher Functions ---

void cudaConvLayer(
    float* d_output, const float* d_input, const float* d_weights, const float* d_biases,
    int H, int W, int C, const int K, const int F, const int S, const int P)
{
    int Ho = convOutDim(H, F, P, S); int Wo = convOutDim(W, F, P, S);
    if (Ho <= 0 || Wo <= 0 || K <= 0) { return; }
    int total_outputs = Ho * Wo * K; int threads = 256; int blocks = (total_outputs + threads - 1) / threads;
    convKernel<<<blocks, threads>>>(d_output, d_input, d_weights, d_biases, H, W, C, K, F, S, P, Ho, Wo);
    CUDA_CHECK(cudaGetLastError());
}

void cudaReluLayer(float* d_data, size_t N) {
     if (N == 0 || d_data == nullptr) return;
    int threads = 256; size_t blocks = (N + threads - 1) / threads;
    // Ensure blocks does not exceed device limits (though unlikely for typical N)
    if (blocks > 2147483647) blocks = 2147483647; // Max grid dim for older devices
    reluKernel<<< (unsigned int)blocks, threads >>>(d_data, N); // Cast blocks for safety
    CUDA_CHECK(cudaGetLastError());
}

void cudaMaxPoolLayer(
    float* d_output, const float* d_input,
    int H, int W, int C, int F_pool, int S_pool)
{
    int Hp = poolOutDim(H, F_pool, S_pool); int Wp = poolOutDim(W, F_pool, S_pool);
    if (Hp <= 0 || Wp <= 0 || C <= 0) { return; }
    int total_outputs = Hp * Wp * C; int threads = 256; int blocks = (total_outputs + threads - 1) / threads;
    poolKernel<<<blocks, threads>>>(d_output, d_input, H, W, C, F_pool, S_pool, Hp, Wp);
    CUDA_CHECK(cudaGetLastError());
}

void cudaLRNLayer(
    float* d_output, const float* d_input,
    int H, int W, int C, int N, float alpha, float beta, float k)
{
    if (H <= 0 || W <= 0 || C <= 0 || N <= 0) {
         if (d_output != d_input && H*W*C > 0) { CUDA_CHECK(cudaMemcpy(d_output, d_input, (size_t)H*W*C*sizeof(float), cudaMemcpyDeviceToDevice)); }
         return;
     }
    int total_elements = H * W * C; int threads = 256; int blocks = (total_elements + threads - 1) / threads;
    lrnKernel<<<blocks, threads>>>(d_output, d_input, H, W, C, N, alpha, beta, k);
    CUDA_CHECK(cudaGetLastError());
}

================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/Makefile ===



================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/include/alexnet.hpp ===

*** File Not Found ***


================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/include/layers.hpp ===

*** File Not Found ***


================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/src/main.cpp ===

*** File Not Found ***


================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/src/alexnet_mpi_cuda.cu ===

*** File Not Found ***


================================================================================

=== FILE: final_project/v5_cuda_aware_mpi/src/layers_cuda.cu ===

*** File Not Found ***


================================================================================
