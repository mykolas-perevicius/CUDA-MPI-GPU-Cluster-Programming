# Combined Project Documentation (final_project)

*This file combines key project README, context, research, and discussion logs.*
*It is automatically generated by the `collect_p_docs.sh` script.*


---

## Content from: `final_project/README.md`

# CS485: GPU Cluster Programming (MPI+CUDA) – Final Project: AlexNet Inference

This directory contains the code, documentation, and resources for the final project of the CS485 GPU Cluster Programming course: an evolving MPI+CUDA implementation of AlexNet inference. The project follows a staged development approach, progressing from serial execution to advanced hybrid parallelism.

**Project Scope Clarification:** The primary focus of the V1-V5 implementation plan is on the **initial two blocks** of the AlexNet architecture (Conv1->ReLU->Pool1 and Conv2->ReLU->Pool2->LRN2). This provides a representative and computationally significant workload for learning and comparing parallelization techniques (MPI, CUDA, Hybrid). Implementing the *full* AlexNet network (including Conv3-5, FC6-8, Softmax) is considered an extension task to be undertaken only *after* the successful completion and analysis of V1-V5 for the initial subset, if time permits.

## Table of Contents
1.  [Project Overview](#1-project-overview)
2.  [Target Environment](#2-target-environment)
3.  [Repository Structure (This Directory)](#3-repository-structure-this-directory)
4.  [Project Version Implementation Plan (Blocks 1 & 2 Focus)](#4-project-version-implementation-plan-blocks-1--2-focus)
    *   [Version 1: Serial CPU](#version-1-serial-cpu---completed)
    *   [Version 2: MPI Only (CPU)](#version-2-mpi-only-cpu---completed)
        *   [Approach 2.1: Broadcast All](#approach-21-broadcast-all---implemented)
        *   [Approach 2.2: Scatter + Halo](#approach-22-scatter--halo---implemented)
    *   [Version 3: CUDA Only (Single GPU)](#version-3-cuda-only-single-gpu---completed)
    *   [Version 4: MPI + CUDA (Hybrid)](#version-4-mpi--cuda-hybrid---pending)
    *   [Version 5: CUDA-Aware MPI (Optional Optimization)](#version-5-cuda-aware-mpi-optional-optimization---pending)
5.  [Key Technologies](#5-key-technologies)
6.  [Current Implementation Status & Performance Highlights](#6-current-implementation-status--performance-highlights)
7.  [Development Workflow for Versions](#7-development-workflow-for-versions)
8.  [Build & Test Instructions per Version](#8-build--test-instructions-per-version)
9.  [Presentation Strategy](#9-presentation-strategy)
10. [Troubleshooting](#10-troubleshooting)
11. [Future Directions & Extensions](#11-future-directions--extensions)
12. [References & Resources](#12-references--resources)

## 1. Project Overview
This project implements inference for the **initial two blocks** of the AlexNet convolutional neural network (Conv1->ReLU->Pool1 and Conv2->ReLU->Pool2->LRN2). The primary goal is to learn and apply different parallel programming paradigms (MPI, CUDA, MPI+CUDA) to this representative workload and to systematically evaluate their performance impact through a structured, incremental 5-version approach. The focus is on the parallelization techniques and performance analysis, rather than building a complete end-to-end classifier.

**Core Task:** Implement and benchmark inference for AlexNet Blocks 1 & 2 across different parallelization stages (V1-V5).

**Parallelization Strategy (V4/V5):** Primarily data parallelism, where input data batches are distributed across MPI ranks, each rank utilizing its assigned GPU(s) for computation. Model weights are typically broadcast from rank 0.

## 2. Target Environment
Code must ultimately compile and run correctly under the course's specified environment:
- **OS:** Fedora 37
- **Compilers:** GCC 12 (for host code), `mpicc`/`mpicxx` (MPI wrappers), `nvcc` (CUDA compiler)
- **GPU Toolkit:** CUDA 12.x
- **MPI:** Open MPI (ideally compiled with CUDA-awareness for V5)

Local development is done in WSL2 (Ubuntu) with compatible toolchains, but final testing should target the Fedora environment.

## 3. Repository Structure (This Directory)

```
final_project/
├── v1_serial/ # V1: Serial CPU implementation (COMPLETE)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v2_mpi_only/ # V2: MPI-only (CPU cores) implementation (COMPLETE)
│ ├── 2.1_broadcast_all/ # -> Approach 2.1 (Implemented)
│ │ ├── include/
│ │ ├── src/
│ │ └── Makefile
│ └── 2.2_scatter_halo/ # -> Approach 2.2 (Implemented)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v3_cuda_only/ # V3: CUDA-only (single GPU) implementation (COMPLETE)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v4_mpi_cuda/ # V4: Baseline MPI + CUDA implementation (PENDING - Restored Baseline)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v5_cuda_aware_mpi/ # V5: Optional CUDA-aware MPI optimization (PENDING - Restored Baseline)
│ ├── include/
│ ├── src/
│ └── Makefile
├── data/ # SHARED: Input data, model weights, etc. (Accessed via ../data/ or ../../data/)
├── docs/ # SHARED: Project documentation, design notes (Accessed via ../docs/ or ../../docs/)
├── logs/ # Basic run logs (gitignored)
├── logs_extended/ # Detailed performance logs (gitignored)
├── original_backup_*/ # Backup of initial V4/V5 state (gitignored)
├── Makefile.original_v4_v5 # Reference Makefile for V4/V5 base
├── ai_context.txt # Technical context summary for AI assistant
├── discussion.md # Rolling log for professor meetings
├── project.txt # Concatenated source code dump (generated by script)
├── RESEARCH.md # Research findings, critical analysis, references
└── README.md # This file
```

**Note:** Shared resources (`data/`, `docs/`) are accessed from within versioned source code using relative paths. Adjust paths depending on whether accessing from `vX/` or `v2_mpi_only/X.Y/`.

## 4. Project Version Implementation Plan (Blocks 1 & 2 Focus)

The project progresses through five versions, focusing on Blocks 1&2 of AlexNet. V2 explored two distinct MPI strategies. The core goal is to demonstrate understanding of each parallelization paradigm using this subset.

---
### Version 1: Serial CPU - COMPLETED
*   **Directory:** `final_project/v1_serial/`
*   **Goal:** Correct, sequential implementation on a single CPU core. Established functional baseline.
*   **Implementation:** Pure C++, `std::vector`, direct loops for layers.

---
### Version 2: MPI Only (CPU) - COMPLETED
*   **Goal:** Parallelize V1 logic across multiple CPU cores using MPI.
*   **Directory Structure:** Contains subdirectories for implemented approaches.

#### Approach 2.1: Broadcast All - IMPLEMENTED
*   **Directory:** `final_project/v2_mpi_only/2.1_broadcast_all/`
*   **Strategy:** Rank 0 `MPI_Bcast`s full input/parameters. All ranks compute the full V1 layer sequence locally. Each rank extracts its assigned slice from the *final* output only. Rank 0 gathers slices via `MPI_Gatherv`.
*   **Outcome:** Simple implementation, validated basic MPI communication, but demonstrated poor scalability due to broadcast overhead and redundant computation (performance degraded with more processes). Serves as a contrast to more scalable methods.

#### Approach 2.2: Scatter + Halo - IMPLEMENTED
*   **Directory:** `final_project/v2_mpi_only/2.2_scatter_halo/`
*   **Strategy:** Rank 0 `MPI_Scatterv`s input rows. Ranks exchange halo regions using non-blocking `MPI_Isend`/`MPI_Irecv`/`MPI_Wait` before convolution layers. Parameters are broadcast. Each rank computes only on its local data (+halos). Rank 0 gathers final results via `MPI_Gatherv`. Required careful index management and handling of boundary conditions/padding.
*   **Outcome:** More complex implementation (halo management, asymmetric trimming), but demonstrated expected speedup and better scalability compared to 2.1. Represents a more realistic MPI parallelization pattern for convolutions.

---
### Version 3: CUDA Only (Single GPU) - COMPLETED
*   **Directory:** `final_project/v3_cuda_only/`
*   **Goal:** Port V1 compute logic (layers) to run on a single GPU using CUDA. Establish GPU baseline performance.
*   **Implementation:** Basic CUDA kernels implemented for Conv, ReLU, Pool, LRN (e.g., 1D grid-stride loops). Host code manages `cudaMalloc`/`cudaMemcpy`/`cudaFree`, kernel launches. Basic `CUDA_CHECK` error handling used. Uses `nvcc` compiler.
*   **Outcome:** Functional GPU implementation. Initial performance analysis indicates potential bottlenecks likely related to host-device transfer overhead or unoptimized kernels (requires profiling).

---
### Version 4: MPI + CUDA (Hybrid) - PENDING
*   **Directory:** `final_project/v4_mpi_cuda/`
*   **Goal:** Combine MPI parallelism (inter-node/rank, likely based on V2.2 Scatter+Halo logic) with CUDA parallelism (intra-node/rank GPU kernel execution from V3).
*   **Planned Actions:**
    1.  Start from restored baseline code, heavily referencing V2.2 and V3 implementations.
    2.  MPI handles overall structure: data distribution (Scatterv to host buffers), halo communication (Send/Recv on host buffers), result aggregation (Gatherv from host buffers).
    3.  Implement explicit Host<->Device data transfers (`cudaMemcpy` H2D after MPI receive/before kernel, `cudaMemcpy` D2H before MPI send/gather) - **Host Staging**.
    4.  Launch V3 CUDA kernels on device data within each rank.
    5.  Ensure correct GPU affinity (`cudaSetDevice`).
    6.  Implement synchronization between MPI and CUDA operations (`MPI_Wait`, `cudaDeviceSynchronize` or Events).
    7.  Profile to identify bottlenecks (likely host staging, MPI comm).
    8.  Build with `nvcc -ccbin=mpicxx`.

---
### Version 5: CUDA-Aware MPI (Optional Optimization) - PENDING
*   **Directory:** `final_project/v5_cuda_aware_mpi/`
*   **Goal:** Optimize V4 by using CUDA-aware MPI calls (passing GPU device pointers directly to MPI) to potentially reduce/eliminate host staging overhead.
*   **Planned Actions:**
    1.  Modify V4 code's `MPI_Scatterv`, `MPI_Isend`, `MPI_Irecv`, `MPI_Gatherv` calls to use *device* pointers.
    2.  Remove explicit H<->D `cudaMemcpy` calls related solely to MPI staging.
    3.  Verify cluster support (CUDA-aware OpenMPI build, HW support like GPUDirect RDMA). Configure environment if needed (e.g., UCX variables).
    4.  Compare performance against V4 to quantify benefit/overhead.
    5.  Build with `nvcc -ccbin=mpicxx`.

## 5. Key Technologies
- **MPI (Open MPI):** Distributed memory communication.
- **CUDA (NVIDIA):** GPU programming (`nvcc`, runtime API, kernels).
- **C++11:** Host code logic, `std::vector`.
- **Make:** Build system.
- **Bash:** Automation scripts (testing, packaging, scaffolding).

## 6. Current Implementation Status & Performance Highlights

*   **V1 (Serial):** **COMPLETED**. Runs successfully. Baseline time: **~617 ms**.
*   **V2 (MPI - 2.1 Broadcast):** **COMPLETED**. Runs (np=1,2,4). **Degraded performance** (np1:~660ms -> np4:~802ms).
*   **V2 (MPI - 2.2 Scatter+Halo):** **COMPLETED**. Runs (np=1,2,4). **Speedup observed** (np1:~491ms -> np4:~177ms).
*   **V3 (CUDA Only):** **COMPLETED**. Runs (np=1). Initial performance **~750 ms**. Needs profiling (Potential H<->D or kernel bottleneck).
*   **V4 (MPI+CUDA):** **PENDING** (Baseline code restored).
*   **V5 (CUDA-Aware):** **PENDING** (Baseline code restored).

*(Performance times recorded on local WSL2 development machine (CPU: [Your CPU], GPU: [Your GPU]) and require re-evaluation on the target Fedora cluster)*

## 7. Development Workflow for Versions
1.  **Choose Version/Approach:** Select the target (e.g., V4).
2.  **Navigate:** `cd final_project/vX_suffix[/Y.Z_approach]`
3.  **Implement:** Modify code in `src/` and `include/` based on the version's goal, likely starting from the previous validated stage.
4.  **Update Makefile:** Adjust compiler, flags, libraries, and source file list (`SRCS`).
5.  **Build:** Run `make clean && make` within the version directory.
6.  **Test:** Execute the compiled `template` executable (appropriate command for serial/MPI/CUDA).
7.  **Analyze/Profile:** Use timers (`MPI_Wtime`, CUDA Events) and profiling tools (Nsight Systems/Compute) to understand performance bottlenecks.
8.  **Commit:** Save changes frequently using Git.

## 8. Build & Test Instructions per Version

**(Run commands from within the respective subdirectory)**

*   **V1 (Serial):** `cd ../v1_serial && make clean && make && ./template`
*   **V2 (MPI - Broadcast All):** `cd ../v2_mpi_only/2.1_broadcast_all && make clean && make && mpirun -np <N> ./template`
*   **V2 (MPI - Scatter+Halo):** `cd ../v2_mpi_only/2.2_scatter_halo && make clean && make && mpirun -np <N> ./template`
*   **V3 (CUDA Only):** `cd ../v3_cuda_only && make clean && make && ./template`
*   **V4 (MPI+CUDA):** `cd ../v4_mpi_cuda && make clean && make && mpirun -np <N> ./template`
*   **V5 (CUDA-Aware MPI):** `cd ../v5_cuda_aware_mpi && make clean && make && mpirun -np <N> ./template` *(Requires correctly configured CUDA-aware MPI environment)*

*(Use `--oversubscribe` for `mpirun` if testing locally with N > physical cores. On cluster, use `-hostfile` and mapping options.)*

## 9. Presentation Strategy

The final project presentation will focus on the **journey of parallelization and performance analysis using the first two blocks of AlexNet** as the consistent workload. Key elements will include:
*   **Demonstration:** Show V1-V5 (or highest completed version) running successfully.
*   **Methodology:** Explain the parallelization techniques applied in each distinct version (Serial, MPI Broadcast, MPI Scatter+Halo, CUDA, MPI+CUDA, CUDA-Aware MPI). Justify design choices.
*   **Performance Analysis:** Present comprehensive results:
    *   Wall-clock times for each version.
    *   Speedup and Efficiency plots relative to V1.
    *   Scalability analysis (strong scaling) for MPI/Hybrid versions (V2.x, V4, V5).
    *   Timing breakdowns (computation vs communication vs H<->D transfer) using profiler data to identify bottlenecks in each stage.
*   **Conclusion:** Summarize key learnings, challenges overcome, and the effectiveness of each parallelization approach for this specific workload.

## 10. Troubleshooting
*(Section content remains the same - standard troubleshooting tips for Make, Includes, Linking, MPI Runtime, CUDA Runtime, Paths)*
- Makefile Errors: Check for TABs vs spaces...
- Include Errors: Verify include paths...
- Linker Errors: Check linked libraries...
- MPI Runtime Errors: Check `mpirun` syntax...
- CUDA Errors: Use `CUDA_CHECK` macro...
- Path Errors: Double-check relative paths...

## 11. Future Directions & Extensions
- **Complete V4/V5:** Finalize the hybrid implementations and perform thorough benchmarking.
- **Performance Optimization:** Apply advanced techniques (async overlap, kernel tuning, shared memory) to V3/V4/V5 based on profiling results.
- **Full AlexNet Implementation:** *If time permits after V1-V5*, extend the most performant version (likely V4/V5) to include Conv3-5, FC6-8, and Softmax layers. This involves implementing FC layers (matrix multiplication) and potentially adjusting parallel strategies.
- **Distributed Training:** Explore gradient synchronization (`MPI_Allreduce`, potentially via NCCL) as a significantly more advanced topic beyond inference.
- **Explore Alternative MPI Strategies:** Implement and compare V2.2 (Scatter+Halo) within the V4/V5 hybrid context.

## 12. References & Resources
- MPI Forum: [mpi-forum.org](https://mpi-forum.org/)
- NVIDIA CUDA Documentation: [docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)
- Programming Massively Parallel Processors (4th Ed.) Textbook & Companion Site
- Open MPI Documentation: [open-mpi.org](https://www.open-mpi.org/)
- LLNL HPC Tutorials: [hpc-tutorials.llnl.gov](https://hpc-tutorials.llnl.gov/)
- `final_project/RESEARCH.md` for detailed analysis and specific paper references.


---

## Content from: `final_project/AI.md`

# AI Context for CS485 Final Project: AlexNet Inference (MPI+CUDA) - V1, V2, V3 Complete

## 1. Project Scope & Status
- **Target:** Staged implementation (V1-V5) of AlexNet **Blocks 1 & 2** (Conv1->ReLU->Pool1->Conv2->ReLU->Pool2->LRN2). Full network is an extension.
- **Completed:** V1 (Serial), V2 (MPI Only - both 2.1_Broadcast & 2.2_Scatter+Halo approaches), V3 (CUDA Only).
- **Next Step:** Implement **V4 (MPI+CUDA Hybrid)**, integrating V2.2 MPI logic with V3 CUDA kernels.

## 2. Environment & Build
- **Target:** Fedora 37, GCC 12, CUDA 12.x, Open MPI (CUDA-aware needed for V5).
- **Dev:** WSL2 (Ubuntu), GCC 12, Open MPI, CUDA 12.8.
- **Build Tools:** Makefiles specific to each version/sub-version (`g++`, `mpicxx`, `nvcc`, `nvcc -ccbin=mpicxx`).

## 3. Key Code Structures & Parameters

**`LayerParams` Struct (`include/alexnet.hpp` - Used across V1-V3):**
```c++
struct LayerParams {
    std::vector<float> weights; // Host vectors
    std::vector<float> biases;  // Host vectors
    int K, F, S, P;       // Conv params
    int F_pool, S_pool;   // Pooling params
    int N_lrn;            // LRN params
    float alpha, beta, k_lrn;
    // Note: V3+ use float* device pointers for computation.
};
```

**V1 Serial (`v1_serial/`):** Pure C++, `std::vector`, direct loops. Baseline functional code.

**V2 MPI Approaches (`v2_mpi_only/`):**
*   **2.1_broadcast_all:** `MPI_Bcast` full input/params. All ranks compute full V1 sequence locally. Gather final slice via `MPI_Gatherv`. **Simple but poor scaling.**
*   **2.2_scatter_halo:** `MPI_Scatterv` input rows. Halo exchange via `MPI_Isend`/`Irecv`/`Wait`. `MPI_Bcast` params. Ranks compute on local data+halo using V1 serial logic. Gather final slice via `MPI_Gatherv`. **More complex, better scaling.**

**V3 CUDA (`v3_cuda_only/`):**
*   Kernels (`__global__`) for layers in `layers_cuda.cu` (simple 1D grid-stride).
*   Host manages GPU memory (`cudaMalloc`/`cudaMemcpy`/`cudaFree`) and kernel launches in `alexnet_cuda.cu`. `CUDA_CHECK` macro used.

**Example Parameters Used (Consistent across V1-V3):** Input H=227, W=227, C=3; Conv1 K=96, F=11, S=4, P=0; Pool1 F=3, S=2; Conv2 K=256, F=5, S=1, P=2; Pool2 F=3, S=2; LRN2 N=5, alpha=1e-4, beta=0.75, k=2.0.

## 4. Performance Summary Table (Blocks 1&2, WSL2 Dev Machine)
```
╔════════════════════════╤═══════╤═════════════╤════════════╤═════╗
║  Version                ║ Procs ║ Shape       ║ Time       ║ St  ║
╟════════════════════════┼═══════┼═════════════┼════════════┼═════╢
║ V1 Serial              ║     1 ║ 13x13x256   ║   ~617 ms  ║  ✔  ║
║ V2 2.1-broadcast-all   ║     1 ║ 13x13x256   ║  ~660 ms   ║  ✔  ║
║ V2 2.1-broadcast-all   ║     2 ║ 13x13x256   ║  ~704 ms   ║  ✔  ║
║ V2 2.1-broadcast-all   ║     4 ║ 13x13x256   ║  ~802 ms   ║  ✔  ║
║ V2 2.2-scatter-halo    ║     1 ║ 13x13x256   ║  ~491 ms   ║  ✔  ║
║ V2 2.2-scatter-halo    ║     2 ║ 13x13x256   ║  ~334 ms   ║  ✔  ║
║ V2 2.2-scatter-halo    ║     4 ║ 13x13x256   ║  ~177 ms   ║  ✔  ║
║ V3 CUDA                ║     1 ║ 13x13x256   ║  ~750 ms   ║  ✔  ║
╚════════════════════════╧═══════╧═════════════╧════════════╧═════╝
```
*(Note: V3 needs profiling; H<->D or kernel optimization likely needed.)*

## 5. Immediate Task: Implement V4 (MPI + CUDA Hybrid)
- **Location:** `final_project/v4_mpi_cuda/`
- **Goal:** Integrate **V2.2 MPI logic (Scatterv, Halo Exchange, Gatherv)** with **V3 CUDA kernels**.
- **Strategy:**
    1.  Base code: Use restored V4/V5 code, heavily referencing V2.2 and V3 implementations.
    2.  MPI handles overall structure, data distribution (Scatterv to *host* buffers), halo communication (*host* buffers), result aggregation (Gatherv from *host* buffers).
    3.  CUDA handles layer computation on GPU.
    4.  **Host Staging Required:** Explicit `cudaMemcpy H2D` needed after Scatterv/halo Recv. Explicit `cudaMemcpy D2H` needed before halo Send / final Gatherv.
    5.  Launch CUDA kernels from V3 (`layers_cuda.cu`) on device data. Use device pointers (`float*`) for kernel args.
    6.  Manage GPU affinity (`cudaSetDevice`).
    7.  Synchronize MPI/CUDA (e.g., `MPI_Wait` for halo, `cudaDeviceSynchronize` or Events before D2H).
    8.  Build with `nvcc -ccbin=mpicxx`.
- **Key Files:** `Makefile`, `include/alexnet.hpp`, `include/layers.hpp`, `src/main.cpp` (MPI focus), `src/alexnet_mpi_cuda.cu` (Hybrid orchestration, H<->D copies), `src/layers_cuda.cu` (Kernels - likely copied from V3).

## 6. Future Task: V5 (CUDA-Aware MPI)
- **Location:** `final_project/v5_cuda_aware_mpi/`
- **Goal:** Optimize V4 by removing host staging.
- **Strategy:** Modify V4 MPI calls to use *device* pointers directly. Requires CUDA-aware MPI library. Compare performance vs V4.


---

## Content from: `final_project/RESEARCH.md`

# CS485 Final Project: Research, Analysis, and Findings

**Project:** AlexNet Inference (MPI+CUDA Staged Implementation)

**Purpose:** This document captures research findings, critical analysis of the project plan, High-Performance Computing (HPC) best practices, and relevant context from academic literature and technical documentation pertaining to the CS485 final project. It serves as a rolling reference to inform design decisions and anticipate challenges throughout the V1-V5 implementation stages.

---

## Table of Contents

1.  [Executive Summary of Analysis](#1-executive-summary-of-analysis)
2.  [Pedagogical Evaluation of Staged (V1-V5) Approach](#2-pedagogical-evaluation-of-staged-v1-v5-approach)
    *   [Analysis of the Learning Curve](#21-analysis-of-the-learning-curve)
    *   [Common Student Challenges](#22-common-student-challenges)
    *   [Alignment with Course Objectives](#23-alignment-with-course-objectives)
3.  [Development Environment and Build System](#3-development-environment-and-build-system)
    *   [WSL2 vs. Native Linux (Fedora): Compatibility and Performance](#31-wsl2-vs-native-linux-fedora-compatibility-and-performance)
    *   [Build System: Makefiles and Bash Scripting](#32-build-system-makefiles-and-bash-scripting)
4.  [Analysis of MPI Parallelization Strategies (V2 Focus)](#4-analysis-of-mpi-parallelization-strategies-v2-focus)
    *   [Critique of "Broadcast All, Compute Slices" Approach (V2.1)](#41-critique-of-broadcast-all-compute-slices-approach-v21)
    *   [Comparison with Alternative CPU-MPI Schemes (e.g., V2.2 Scatter+Halo)](#42-comparison-with-alternative-cpu-mpi-schemes-eg-v22-scatterhalo)
    *   [Sub-Version Implementation Philosophy](#43-sub-version-implementation-philosophy)
    *   [Correctness Pitfalls in MPI](#44-correctness-pitfalls-in-mpi)
5.  [Layer Parallelization: AlexNet Subset & Challenges (V3 Focus)](#5-layer-parallelization-alexnet-subset--challenges-v3-focus)
    *   [Representativeness of Early AlexNet Blocks](#51-representativeness-of-early-alexnet-blocks)
    *   [Inherent Difficulties in Layer Parallelization (MPI & CUDA)](#52-inherent-difficulties-in-layer-parallelization-mpi--cuda)
    *   [Critique of `std::vector<float>` for HPC/CUDA](#53-critique-of-stdvectorfloat-for-hpccuda)
6.  [CUDA Implementation and MPI+CUDA Integration (V3-V5)](#6-cuda-implementation-and-mpicuda-integration-v3-v5)
    *   [V3 (CUDA Kernels): Best Practices and Pitfalls](#61-v3-cuda-kernels-best-practices-and-pitfalls)
    *   [V4 (MPI+CUDA): Integration Challenges](#62-v4-mpicuda-integration-challenges)
    *   [V5 (CUDA-aware MPI): Requirements, Benefits, Pitfalls](#63-v5-cuda-aware-mpi-requirements-benefits-pitfalls)
7.  [HPC Software Engineering and Development Practices](#7-hpc-software-engineering-and-development-practices)
    *   [Code Organization and Modularity](#71-code-organization-and-modularity)
    *   [Robust Error Handling (MPI & CUDA)](#72-robust-error-handling-mpi--cuda)
    *   [Debugging Techniques per Stage](#73-debugging-techniques-per-stage)
    *   [Data Precision (Single vs. Double)](#74-data-precision-single-vs-double)
8.  [Performance Analysis, Bottlenecks, and Profiling](#8-performance-analysis-bottlenecks-and-profiling)
    *   [Likely Performance Bottlenecks per Stage](#81-likely-performance-bottlenecks-per-stage)
    *   [Standard Performance Metrics](#82-standard-performance-metrics)
    *   [Suitable Profiling Tools/Techniques](#83-suitable-profiling-toolstechniques)
9.  [Advanced Topics, Libraries Context, and Potential Gaps](#9-advanced-topics-libraries-context-and-potential-gaps)
    *   [Gaps in Current Plan (Async Overlap, Load Balancing, etc.)](#91-gaps-in-current-plan-async-overlap-load-balancing-etc)
    *   [Awareness of Relevant Libraries (cuDNN, NCCL, Thrust, etc.)](#92-awareness-of-relevant-libraries-cudnn-nccl-thrust-etc)
    *   [Validation and Numerical Correctness Concerns](#93-validation-and-numerical-correctness-concerns)
10. [Research-Based Recommendations for the Project](#10-research-based-recommendations-for-the-project)
11. [References](#11-references)
12. [Rolling Research Notes & Findings](#12-rolling-research-notes--findings)

---

## 1. Executive Summary of Analysis

The CS485 final project plan, implementing AlexNet inference via a five-stage parallelization (V1-V5), is pedagogically valuable for its incremental introduction to Serial, MPI, CUDA, and Hybrid MPI+CUDA programming. However, it presents significant technical and conceptual challenges. Key concerns identified through research include:
*   **Learning Curve:** Steep difficulty increases between stages (V1->V2, V3->V4).
*   **V2 MPI Strategy:** The initial "Broadcast All" (V2.1) approach is simple but scales poorly due to communication/memory overhead; alternatives like Scatter+Halo (V2.2) are more scalable but complex.
*   **Environment:** Developing in WSL2 introduces compatibility/performance risks compared to the target Fedora cluster.
*   **Data Structures:** `std::vector<float>` is suboptimal for CUDA host-device transfers; pinned memory is crucial for V3+.
*   **V5 Feasibility:** CUDA-aware MPI benefits depend heavily on specific cluster HW/SW support (GPU Direct RDMA).
*   **Bottlenecks:** Performance limitations shift dramatically (CPU -> MPI Comm -> GPU Kernel/Transfers -> MPI+CUDA Sync/Comm -> Network).
*   **Software Engineering:** Robust error handling, modularity, and effective debugging/profiling are critical but challenging.
*   **Context:** Plan lacks emphasis on asynchronous overlap and standard libraries (cuDNN, NCCL).

Recommendations focus on standardizing the environment, refining the V2 strategy (or its framing), mandating appropriate data structures by V3, providing strong support for V4 integration, verifying V5 viability, enforcing SE practices, and contextualizing manual efforts with industry libraries/techniques.

---

## 2. Pedagogical Evaluation of Staged (V1-V5) Approach

### 2.1 Analysis of the Learning Curve

The 5-stage approach offers a structured learning path:
*   **V1 (Serial):** Establishes functional correctness and a performance baseline. Reinforces algorithm understanding.
*   **V2 (MPI):** Introduces distributed memory concepts, explicit communication (`MPI_Bcast`, `MPI_Gather`), and process management. A major conceptual shift.
*   **V3 (CUDA):** Introduces GPU architecture, SIMT execution, kernel programming, and host-device memory management. Another distinct, significant conceptual leap.
*   **V4 (MPI+CUDA):** The core integration challenge, demanding orchestration across nodes and GPUs, managing data movement between host (MPI) and device (CUDA) memory spaces, and synchronization. Often the steepest curve. Performance dominated by inter-node communication.
*   **V5 (CUDA-aware MPI):** Optimization layer potentially simplifying V4 code and improving performance via direct GPU communication (e.g., GPU Direct RDMA). Requires specific HW/SW support; pedagogical value depends on demonstrable benefits.

**Overall:** The staging isolates concepts effectively, but instructors/students must anticipate difficulty spikes, especially V1->V2 and V3->V4.

### 2.2 Common Student Challenges

*   **Conceptual:** Distinguishing distributed vs. shared memory models. Understanding MPI communicators, CUDA execution/memory hierarchies, synchronization primitives (`MPI_Barrier`, `cudaDeviceSynchronize`, `__syncthreads`).
*   **Implementation:** Debugging parallel/distributed non-deterministic code. Managing complex C++/MPI/CUDA builds. Correct API usage and error checking. Performance tuning (identifying/addressing bottlenecks).
*   **Integration (V4/V5):** Sequencing MPI calls and CUDA operations correctly. Ensuring host/device data consistency. Avoiding deadlocks. Managing GPU affinity. Configuring/verifying CUDA-aware MPI.

### 2.3 Alignment with Course Objectives

This project aligns well with typical HPC course objectives:
*   Hands-on experience with parallelization concepts (data/task parallelism, speedup, efficiency).
*   MPI standard for distributed memory programming.
*   CUDA for GPU accelerator programming.
*   Hybrid MPI+CUDA models.
*   Performance analysis and profiling.
*   Application to a relevant domain (CNN inference).

---

## 3. Development Environment and Build System

### 3.1 WSL2 vs. Native Linux (Fedora): Compatibility and Performance

Using WSL2 (Ubuntu) for development while targeting Fedora 37 presents risks:
*   **WSL2 Overview:** Runs a real Linux kernel via lightweight VM. Good compatibility but not identical to native Linux.
*   **Performance Discrepancies:** Filesystem I/O (Windows<->WSL2) can be slow. Virtualized networking may differ from native MPI performance. GPU compute usually close, but potential overheads exist.
*   **Compatibility Risks:** Differences in kernel versions, system libraries (glibc), CUDA drivers (WSL vs. native), and MPI implementation behavior (e.g., Intel MPI unsupported [10], OpenMPI quirks [5]) can cause "works on my machine" issues. Consumer GPUs in WSL2 may lack features like GPUDirect RDMA [5].
*   **Mitigation:** Minimize environment differences. Use containers (Docker/Singularity) in both dev/target environments. Perform frequent testing on the actual Fedora cluster. Match toolchain versions (GCC 12, CUDA 12.x, Open MPI) precisely.

**Conclusion:** Sole reliance on WSL2 is risky. Debugging environment-specific issues is time-consuming. Prioritize testing on or containerizing the target environment.

### 3.2 Build System: Makefiles and Bash Scripting

*   **Suitability:** Makefiles + Bash are standard HPC tools, generally sufficient for this project scope. Manage compilation of C++/CUDA and linking against MPI/CUDA libraries.
*   **Complexity Management:** Requires well-organized Makefiles (variables, pattern rules, dependencies). Robust Bash scripts with error checking are needed for automation.
*   **Alternative (CMake):** Offers better cross-platform support, dependency finding (FindMPI, FindCUDA), build configurations, and test integration. Scales better for complex projects and is a valuable skill. Represents stronger software engineering practice, though with a slightly steeper initial learning curve than basic Makefiles.

---

## 4. Analysis of MPI Parallelization Strategies (V2 Focus)

### 4.1 Critique of "Broadcast All, Compute Slices" Approach (V2.1)

This is the initially chosen strategy for V2: `MPI_Bcast` full input/parameters, each rank computes an output slice independently, `MPI_Gather`/`MPI_Gatherv` final slices.
*   **Scalability Issues:**
    *   *Communication Bottleneck:* `MPI_Bcast` cost scales poorly with process count (P) and data size. Initial broadcast dominates time as P increases. Gather also adds overhead.
    *   *Memory Inefficiency:* Each rank stores full input/parameters, negating distributed memory benefits. Limited by single-node memory.
*   **Implementation Simplicity:** Main advantage. Ranks compute independently after broadcast, simplifying logic. Avoids complex halo exchanges.
*   **Pedagogical Value:** Introduces basic collectives but fails to teach scalable communication patterns (e.g., neighbor exchange) needed for efficient distributed applications. Poor foundation for V4/V5 performance. Makes overlapping communication/computation difficult.

### 4.2 Comparison with Alternative CPU-MPI Schemes (e.g., V2.2 Scatter+Halo)

*   **Input Spatial Decomposition (Scatter + Halo Exchange):**
    *   *Description:* Divide input map spatially among ranks. Exchange boundary data (halos) with neighbors (`MPI_Sendrecv`). Replicate parameters.
    *   *Pros:* Reduces activation memory per rank. Good load balancing potential. Neighbor communication can be more scalable than broadcast. Natural fit for convolution.
    *   *Cons:* Much higher implementation complexity (halo logic, boundary handling, potential deadlocks). [1]
*   **Filter Decomposition (Model Parallelism):**
    *   *Description:* Distribute filters among ranks. Replicate input. Combine partial outputs (`MPI_Allgather`/`MPI_Allreduce`).
    *   *Pros:* Reduces parameter memory per rank. Good for many filters.
    *   *Cons:* Replicates input activation memory. Significant communication for combining outputs. Often used with data parallelism.
*   **Table 1: Comparison of Strategies (V2 Context)**
    | Strategy Name             | Communication Pattern (Primary)            | Memory/Rank                               | Scalability Potential | Implementation Complexity | Suitability (Conv) | Suitability (FC) |
    | :------------------------ | :----------------------------------------- | :---------------------------------------- | :-------------------- | :------------------------ | :----------------- | :--------------- |
    | Broadcast All/Output Slice | `MPI_Bcast`, `MPI_Gather`/`Allgather`        | High (Full Input + Full Model)            | Poor                  | Low                       | Moderate           | Moderate         |
    | Input Spatial Decomp      | Neighbor `MPI_Sendrecv` (Halo)             | Low (Input Patch + Halos + Full Model)    | Good                  | High                      | High               | Low              |
    | Filter Decomp             | `MPI_Bcast` (Input), `Allgather`/`Allreduce` | Moderate (Full Input + Partial Model) | Moderate              | Moderate                  | Moderate           | High             |

### 4.3 Sub-Version Implementation Philosophy

Recognize that multiple valid strategies (like V2.1 vs V2.2) exist for each stage.
1.  **Identify & Document:** Note different approaches in `README.md` or `RESEARCH.md`.
2.  **Implement Primary:** Implement one chosen strategy first within the main version folder (e.g., V2.1 in `v2_mpi_only/`).
3.  **Explore Alternatives (Optional):** If time allows, implement alternatives in sub-folders (e.g., `v2_mpi_only/v2.2_scatter_halo/`) or via Git branches for comparison.

### 4.4 Correctness Pitfalls in MPI

*   **Collectives:** Ensuring all ranks participate, correct counts/displacements/datatypes, correct reduction operations. Off-by-one errors in slicing.
*   **Halo Exchange (if used):** Incorrect neighbor ranks, mismatched tags/counts, buffer errors, deadlocks (blocking send/recv order), boundary conditions.
*   **Data Types:** `MPI_Datatype` must match C++ memory layout precisely.

---

## 5. Layer Parallelization: AlexNet Subset & Challenges (V3 Focus)

### 5.1 Representativeness of Early AlexNet Blocks

*   **Layers Covered:** Conv1, ReLU1, Pool1, LRN1(?), Conv2, ReLU2, Pool2, LRN2.
*   **Characteristics:** Covers compute-intensive convolution, simple element-wise ReLU, local reduction pooling, and more complex cross-channel LRN. Representative of fundamental CNN operations.
*   **Limitations:** Omits deeper Conv layers (smaller spatial size, more channels) and Fully Connected layers (matrix multiplies, different bottlenecks). May give skewed perspective on optimal strategies for the whole network. Early layers have large spatial dimensions suitable for spatial decomposition; later layers might favor filter decomposition.

### 5.2 Inherent Difficulties in Layer Parallelization (MPI & CUDA)

*   **MPI:**
    *   *Convolution:* Data distribution needs halo exchange (if spatial decomp.) or input replication (if filter decomp.). Load balancing.
    *   *Pooling:* Simpler, but needs halos if spatial decomp. and window crosses boundary.
    *   *LRN:* Cross-channel/spatial dependencies make efficient distribution hard.
*   **CUDA:**
    *   *Convolution:* Efficient mapping of loops to threads/blocks/grids. Optimizing memory access (coalescing, shared memory tiling for reuse). Algorithms (im2col, Winograd, direct).
    *   *Pooling:* Simpler kernel, boundary checks, avoid shared memory bank conflicts if used.
    *   *LRN:* Efficient access to neighbors (spatial/channel). Shared memory useful but needs careful indexing.

### 5.3 Critique of `std::vector<float>` for HPC/CUDA

Using `std::vector<float>` (from V1) in V3+ presents performance issues:
*   **Memory Allocation:** Allocates standard pageable host memory.
*   **Host-Device Transfer:** `cudaMemcpy` is slower with pageable memory vs. pinned (page-locked) host memory (allocated via `cudaMallocHost`). Pinned memory enables asynchronous transfers (`cudaMemcpyAsync`) essential for overlapping communication/computation. `std::vector::data()` provides the pointer, but the memory type limits transfer speed and overlap capability. [9]
*   **Alignment:** Default alignment may not be optimal for SIMD/GPU coalescing.
*   **Recommendation:** Transition to pinned memory strategies by V3 for performance-critical data involved in transfers.
    *   *Alternatives:* Raw `float*` with `cudaMallocHost`/`cudaFreeHost`; `std::unique_ptr` with custom deleter for `cudaFreeHost`; `std::vector` with custom pinned allocator; Thrust library (`thrust::host_vector`, `thrust::device_vector`).

---

## 6. CUDA Implementation and MPI+CUDA Integration (V3-V5)

### 6.1 V3 (CUDA Kernels): Best Practices and Pitfalls

*   **Best Practices:** Map computation to CUDA hierarchy. Minimize H<->D transfers. Use pinned host memory. Optimize global memory access (coalescing). Use shared memory for data reuse (beware bank conflicts). Maximize arithmetic intensity. Avoid warp divergence. Use CUDA streams for overlap. Profile with Nsight Compute/Systems. Rigorous error checking (`cudaGetLastError`).
*   **Pitfalls:** Uncoalesced access. Global memory bottleneck. Shared memory bank conflicts. Low occupancy. Thread divergence. Forgetting error checks. Underestimating transfer overhead. Kernel indexing errors.

### 6.2 V4 (MPI+CUDA): Integration Challenges

*   **Host/Device Data Management:** Explicit staging via host memory is required for MPI calls with non-CUDA-aware MPI. `GPU -> cudaMemcpy D2H -> Host Buffer -> MPI_Send -> Network -> MPI_Recv -> Host Buffer -> cudaMemcpy H2D -> GPU`. This adds latency and PCIe load. Optimization involves minimizing/overlapping these steps.
*   **Synchronization:** Coordinate MPI calls (blocking/non-blocking) with asynchronous CUDA operations (kernels, async copies) using `cudaDeviceSynchronize`, `cudaStreamSynchronize`, `cudaEventRecord/Synchronize`. Incorrect sync leads to race conditions or deadlocks.
*   **GPU Affinity:** Ensure each MPI rank binds to a specific GPU on multi-GPU nodes (`cudaSetDevice` or `CUDA_VISIBLE_DEVICES`). Failure leads to resource contention and incorrect results.

### 6.3 V5 (CUDA-aware MPI): Requirements, Benefits, Pitfalls

*   **Requirements:** MPI library built with CUDA support (OpenMPI `--with-cuda`, MVAPICH2-GDR, etc.). Compatible CUDA Toolkit/drivers. HW support for GPU Direct RDMA (specific GPUs/NICs/fabric) for optimal performance. Correct system/MPI environment configuration (e.g., UCX settings [4, 5]).
*   **Benefits:** Pass GPU device pointers directly to MPI calls. Library handles transfer, potentially via RDMA bypassing host memory. Reduces latency, PCIe traffic, CPU overhead. Simplifies application code (removes manual `cudaMemcpy` staging).
*   **Pitfalls:** Complex configuration/compatibility issues. Performance gain not guaranteed (may fallback to internal staging if RDMA unsupported). Harder debugging. Synchronization nuances still apply. Strong dependency on specific cluster environment. [5]
*   **Table 2: MPI+CUDA Data Movement Strategies**
    | Strategy              | Description                                                              | Key Steps (Send)                               | Pros                                   | Cons                                          | Relevant Stage |
    | :-------------------- | :----------------------------------------------------------------------- | :--------------------------------------------- | :------------------------------------- | :-------------------------------------------- | :------------- |
    | Manual Staging (V4)   | Explicit H<->D copies around MPI calls                                   | Kernel -> `cudaMemcpy D2H` -> `MPI_Send`       | Works anywhere, Explicit control       | High latency, PCIe load, More app code        | V4             |
    | CUDA-aware (V5 Ideal) | Pass GPU ptr to MPI; Library uses optimized path (RDMA)                  | Kernel -> `MPI_Send(gpu_ptr)`                  | Low latency, Less PCIe load, Simpler code | Requires HW/SW support, Complex config/debug | V5 (if works)  |
    | CUDA-aware (V5 Fback) | Pass GPU ptr; Library falls back to internal staging (mimics V4)         | Kernel -> `MPI_Send(gpu_ptr)` -> Lib does copies | Simpler app code                       | Perf similar/worse than V4, Hides data path | V5 (fallback)  |

---

## 7. HPC Software Engineering and Development Practices

*   **Code Organization:** Crucial for managing complexity. Use separation of concerns (layers vs MPI vs CUDA logic), clear directory structure (`src`, `include`, `kernels`), encapsulation (namespaces, classes, functions).
*   **Error Handling:** Essential for parallel debugging. Check return codes for *all* MPI and CUDA calls. Use `MPI_Error_string`, `cudaGetErrorString`. Use rank ID in MPI error messages. Consider helper macros for CUDA error checking.
*   **Debugging Techniques:**
    *   *V1:* GDB, Valgrind.
    *   *V2:* Rank-based printf, parallel debuggers (multi-process GDB, TotalView, DDT), MPI analysis tools (if available). Focus on communication issues (deadlocks, tags, sizes).
    *   *V3:* Kernel printf (use sparingly), `cuda-gdb`, `cuda-memcheck`/`compute-sanitizer`, strategic `cudaDeviceSynchronize`.
    *   *V4/V5:* Combine techniques. Parallel debuggers with CUDA awareness (TotalView, DDT) are ideal. Isolate issues (1 process vs multi-process vs multi-node).
*   **Data Precision:** Use `float` (single-precision) for deep learning inference. GPUs are optimized for it [3]. `double` uses 2x memory/bandwidth and is much slower computationally. Ensure consistent use of `float` and `MPI_FLOAT`.

---

## 8. Performance Analysis, Bottlenecks, and Profiling

*   **Likely Bottlenecks per Stage:**
    *   *V1:* CPU compute, Host memory.
    *   *V2:* `MPI_Bcast` / `MPI_Gather` time, Network B/W & Latency, Memory per node.
    *   *V3:* GPU Kernel (compute/memory bound), PCIe transfer time.
    *   *V4:* Inter-node MPI (host staging), Sync overhead, PCIe contention, Load imbalance.
    *   *V5:* Inter-node MPI (direct path), Network B/W & Latency, Library efficiency, Load imbalance, Amdahl's Law.
*   **Standard Performance Metrics:** Wall Clock Time, Time Breakdowns (Compute vs Comm vs H2D/D2H vs Sync), Speedup (S(N) = T1/TN), Efficiency (E(N) = S(N)/N), Scalability (Strong/Weak), Communication Volume/Bandwidth, Kernel Execution Time, GPU Utilization/Occupancy/Memory Throughput.
*   **Suitable Profiling Tools/Techniques:**
    *   *Manual Timers:* `MPI_Wtime()`, CUDA Events (`cudaEventElapsedTime`).
    *   *CPU Profilers:* `gprof`, `perf`.
    *   *MPI Profilers:* `mpiP`, Score-P, Vampir, TAU (provide timelines, message stats, wait state analysis).
    *   *CUDA Profilers:* Nsight Systems (`nsys` - system view, CPU/GPU/API timeline), Nsight Compute (`ncu` - deep kernel analysis, perf counters, bottlenecks).
    *   *Combined:* Score-P, Vampir, TAU often handle both MPI+CUDA for unified view.
*   **Analysis Focus:** Decompose total time to identify true bottlenecks. Understand how bottlenecks shift between stages. Compare against theoretical limits (Amdahl's Law).
*   **Table 3: Performance Metrics and Profiling Tools per Project Stage**
    | Stage            | Likely Bottlenecks                                   | Key Metrics                                                    | Recommended Tools                                            |
    | :--------------- | :--------------------------------------------------- | :------------------------------------------------------------- | :----------------------------------------------------------- |
    | V1 (Serial)      | CPU compute, Host memory                             | Wall time, CPU cycles, Cache misses                            | `gprof`, `perf`, Manual timers                               |
    | V2 (MPI)         | `MPI_Bcast`/`Gather`, Network, Mem/Node, CPU compute | Wall time, Speedup/Efficiency, Time breakdown, MPI stats       | Manual `MPI_Wtime`, MPI Profilers (Score-P, Vampir, TAU)     |
    | V3 (CUDA)        | GPU kernel, PCIe transfer, Launch overhead           | Wall time, Speedup(vsV1), Kernel/H2D/D2H times, GPU util/mem BW | Manual CUDA Events, Nsight Systems (`nsys`), Nsight Compute (`ncu`) |
    | V4 (MPI+CUDA)    | MPI Comm (Host staging), Sync, Load imbalance        | Wall time, Speedup/Eff(vsV1/V3), Breakdown, Net BW, Wait times | Combined Profilers (Score-P, Vampir, TAU), `nsys`, Manual Timers |
    | V5 (CUDA-aware)  | MPI Comm (Direct), Network, Lib efficiency, Amdahl   | Wall time, Speedup/Eff(vsV4), Breakdown, Net BW (direct)      | Combined Profilers, `nsys`, Manual Timers, V4 Comparison      |

---

## 9. Advanced Topics, Libraries Context, and Potential Gaps

*   **Gaps in Current Plan:**
    *   *Async Operations/Overlap:* Critical technique (non-blocking MPI + CUDA streams) not explicitly planned. Should be discussed/introduced conceptually.
    *   *Load Balancing:* Assumed balanced work; real scenarios often require explicit balancing strategies.
    *   *Advanced MPI:* Derived Datatypes, One-Sided Comm (RMA), advanced communicators not covered.
    *   *Topology Awareness:* Performance sensitive to network topology & process mapping, not addressed.
    *   *Alternative Paradigms:* OpenMP, Task-based (Legion), PGAS languages provide context.
*   **Awareness of Relevant Libraries:** Essential context for the manual implementation effort.
    *   *cuDNN:* NVIDIA's optimized library for DNN primitives (Conv, Pool, etc.). Performance benchmark. [8]
    *   *NCCL:* NVIDIA's optimized library for multi-GPU/multi-node collectives (Allreduce, Bcast). Standard for DL training. [7]
    *   *Thrust:* High-level C++ template library for CUDA (parallel algorithms, `device_vector`).
    *   *Vendor MPIs:* May offer better performance/CUDA integration than standard OpenMPI/MPICH on specific clusters.
*   **Validation and Numerical Correctness:** Need strategy to compare outputs between versions within a tolerance (epsilon) due to floating-point non-associativity.

---

## 10. Research-Based Recommendations for the Project

1.  **Standardize Environment:** Use target cluster or exact-replica containers (Docker/Singularity) for all testing/development.
2.  **Refine V2 Strategy:** Either replace "Broadcast All" with Spatial Decomposition (teaching halo exchange) OR clearly document limitations of Broadcast All and frame as introductory step.
3.  **Mandate Pinned Memory by V3:** Require use of `cudaMallocHost` or equivalents for H<->D transfer buffers to enable `cudaMemcpyAsync`.
4.  **Focus V3 Pedagogy:** Emphasize CUDA fundamentals (coalescing, shared mem, sync) and profiling (Nsight), not matching cuDNN performance.
5.  **Support V4/V5 Integration:** Provide guidance/templates for H<->D data flow, MPI/CUDA sync. Verify V5 feasibility/benefit on target cluster before assignment.
6.  **Enforce SE Practices:** Mandate modularity, comprehensive error checking from V1. Provide debugger/profiler access & training.
7.  **Require Performance Analysis:** Mandate timing breakdowns, speedup/efficiency calculation, bottleneck analysis, and scalability discussion for V2-V5.
8.  **Provide Context:** Discuss async overlap conceptually. Introduce cuDNN/NCCL as industry standards and performance references.

---

## 11. References

1.  Lawrence Mitchell, *“MPI: Domain decomposition and halo exchanges”*, Durham Univ. HPC Course ([Link](https://teaching.wence.uk/phys52015/exercises/mpi-stencil/))
2.  Wikipedia – *“Data parallelism vs. Model parallelism”* ([Link](https://en.wikipedia.org/wiki/Data_parallelism))
3.  NVIDIA Developer Blog – *“Defining Floating Point Precision (FP64, FP32, FP16)”*, Exxact Corp. (2024) ([Link](https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16))
4.  OpenUCX Documentation / CISL Tutorial Slides – Recommended environment settings for CUDA-aware MPI (UCX transport). ([Example Slide Link](https://www.cisl.ucar.edu/sites/default/files/2022-09/11_MultiGPU_Part2.slides%20%282%29.pdf))
5.  NVIDIA Forums – discussion *“Windows 11 + WSL + CUDA-aware MPI”* ([Link](https://forums.developer.nvidia.com/t/windows-11-wsl-cuda-aware-mpi-geforce-40-series-seg-fault-but-with-geforce-30-series-ok/292425))
6.  Alex Krizhevsky et al., *“ImageNet Classification with Deep CNNs (AlexNet)”*, NIPS 2012 ([Paper Link - Often Found Online](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf))
7.  NVIDIA Developer – *“NCCL (Nvidia Collective Communication Library)”* ([Link](https://developer.nvidia.com/nccl))
8.  NVIDIA Documentation – *“NVIDIA cuDNN”* ([Link](https://docs.nvidia.com/deeplearning/cudnn/latest/))
9.  Stack Overflow – *“std::vector<T> contiguous memory”* ([Example Link](https://stackoverflow.com/questions/2009531/c-stdpair-stdvector-memcopy))
10. SCM Documentation – *“Known Issues: Intel MPI on WSL”* ([Link](https://www.scm.com/doc/Installation/Additional_Information_and_Known_Issues.html))

---

## 12. Rolling Research Notes & Findings

*(**Instructions:** Manually add your own specific findings, benchmark results, interesting articles, or unexpected behaviors encountered during development here.)*

*   **YYYY-MM-DD:** Note about specific MPI behavior observed on cluster vs WSL2...
*   **YYYY-MM-DD:** Benchmark results for V2.1 Broadcast All show communication time dominates beyond P=4...
*   **YYYY-MM-DD:** Found issue with LRN layer indexing in serial code, fixed in V1...
*   ... *(Add more entries as you progress)* ...

---

---

## Content from: `final_project/DISCUSSION.md`

# CS485 Final Project: Discussion Log & Professor Meeting Prep

**Project:** AlexNet Inference (MPI+CUDA Staged Implementation) - Blocks 1 & 2 Focus
**Student:** [Your Name]
**Date Started:** [Date file created]
**Last Updated:** [Current Date]

**Purpose:** This document serves as a rolling log to prepare for discussions with Professor Sohn regarding the CS485 final project. It outlines current status, challenges, questions, and proposed next steps. Meeting summaries will be manually added at the bottom after each discussion.

---

## 1. Project Overview & Goal Reminder

*   **Objective:** Implement and benchmark inference for **AlexNet Blocks 1 & 2** using a 5-stage approach: V1 (Serial), V2 (MPI CPU), V3 (CUDA GPU), V4 (MPI+CUDA), V5 (CUDA-Aware MPI). The primary goal is learning parallelization techniques and performance analysis on this subset.
*   **Target Environment:** Fedora 37, GCC 12, CUDA 12.x, Open MPI.
*   **Deliverable:** Working code for completed versions (up to V5 ideally), focusing on a comparative performance analysis across the stages. Presentation will highlight the parallelization journey.

---

## 2. Current Status & Immediate Next Step

*   **Last Completed Stages:**
    *   V1 (Serial CPU) - **Completed & Validated.**
    *   V2 (MPI Only - Approach 2.1: Broadcast All) - **Completed.** Demonstrated poor scaling.
    *   V2 (MPI Only - Approach 2.2: Scatter+Halo) - **Completed.** Demonstrated expected speedup.
    *   V3 (CUDA Only - Single GPU) - **Completed.** Functional, needs profiling/optimization.
*   **Current Focus:** **Version 4 (MPI + CUDA Hybrid)**
*   **Working Directory:** `final_project/v4_mpi_cuda/`
*   **Immediate Goal:** Integrate the V2.2 (Scatter+Halo) MPI logic with the V3 CUDA kernels. This involves managing data distribution via MPI on host buffers, implementing explicit Host<->Device copies (`cudaMemcpy`) around MPI communication and kernel launches, ensuring GPU affinity, and synchronizing MPI/CUDA operations. Build using `nvcc -ccbin=mpicxx`.

---

## 3. Recent Accomplishments / Milestones Achieved

*   Completed V1 (Serial) implementation and established baseline performance (~617ms).
*   Implemented V2 MPI using two distinct strategies:
    *   V2.1 (Broadcast All): Validated basic MPI, highlighted scalability issues (~660ms -> ~802ms for np=1->4).
    *   V2.2 (Scatter+Halo): Successfully implemented more complex halo exchange logic using non-blocking MPI, demonstrating expected speedup (~491ms -> ~177ms for np=1->4).
*   Completed V3 (CUDA Only) implementation: Ported layer logic to basic CUDA kernels, functional on single GPU (~750ms, needs optimization/profiling).
*   Established robust project structure with versioned directories and comprehensive documentation (`README.md`, `RESEARCH.md`, `ai_context.txt`, `discussion.md`).

---

## 4. Current Challenges / Roadblocks / Issues

*   **Anticipated for V4:**
    *   **MPI+CUDA Integration Complexity:** Correctly orchestrating the sequence: `MPI_Recv (Host)` -> `cudaMemcpy H2D` -> `Launch Kernel` -> `cudaDeviceSync/EventSync` -> `cudaMemcpy D2H` -> `MPI_Send (Host)`. This applies particularly to halo exchanges.
    *   **Synchronization:** Ensuring CUDA operations complete before data is used by MPI (and vice-versa) without introducing unnecessary serialization or deadlocks. Correct use of `MPI_Wait/Test` and CUDA Events/Streams.
    *   **Host Staging Bottleneck:** The explicit H<->D copies required before/after MPI calls are expected to be a significant performance bottleneck compared to pure CUDA or potential V5. Quantifying this overhead will be key.
    *   **GPU Affinity:** Correctly setting `cudaSetDevice` based on local rank to ensure each MPI process uses its designated GPU on multi-GPU nodes.
    *   **Debugging Hybrid Code:** Debugging issues that might stem from the interaction between MPI state and CUDA state will be challenging. Requires combined debugging approaches.
    *   **Makefile Complexity:** Ensuring the `nvcc -ccbin=mpicxx` build process correctly compiles CUDA (`.cu`) and C++ (`.cpp`) files and links against both CUDA and MPI libraries.

---

## 5. Specific Questions for Professor Sohn

*(**Instructions:** Review/update before meeting)*

1.  **V4 Integration Strategy:** Given V2.2 (Scatter+Halo) and V3 (CUDA kernels) are complete, what are common pitfalls or recommended patterns for structuring the V4 hybrid code, specifically regarding the placement and synchronization of `cudaMemcpy` calls around the MPI halo exchange logic (`MPI_Isend`/`Irecv`/`Wait`)?
2.  **V4 Performance Expectation:** Considering the host staging overhead, is it expected that V4 (MPI+CUDA) might initially perform *worse* than V3 (CUDA only) or even V2.2 (MPI only) for a small number of processes/nodes? How should we interpret such results?
3.  **Asynchronous Operations (Overlap):** Should we attempt to implement asynchronous overlap (CUDA streams with `cudaMemcpyAsync` and non-blocking MPI) in V4/V5, or is mastering the synchronous host-staging approach sufficient for V4?
4.  **V5 Feasibility Check:** What steps should we take to verify if the target cluster's Open MPI installation is truly CUDA-aware and supports efficient GPU Direct RDMA? Are there specific environment variables or test programs recommended?
5.  **Profiling Hybrid Code:** What's the recommended approach for profiling V4/V5 on the cluster? Can Nsight Systems effectively capture both MPI and CUDA timelines, or should we primarily rely on manual `MPI_Wtime` and CUDA Event timers?
6.  **Presentation Scope Confirmation:** Reconfirming that the primary presentation deliverable is the comparative analysis of V1-V5 performance for Blocks 1&2, rather than a fully implemented AlexNet.

---

## 6. Proposed Next Steps / Plan

*(**Instructions:** Outline plan until next meeting)*

1.  **Implement V4 (`v4_mpi_cuda/`):**
    *   Start with restored baseline code.
    *   Integrate V2.2 `main.cpp` MPI logic (Scatterv, Isend/Irecv/Wait for halo, Gatherv).
    *   Integrate V3 `alexnet_cuda.cu` kernel launch logic.
    *   Implement **explicit H<->D copies** (`cudaMemcpy`) around MPI calls (halo exchange, initial scatter, final gather).
    *   Copy V3 CUDA kernels (`layers_cuda.cu`) and headers.
    *   Implement GPU affinity (`cudaSetDevice`).
    *   Implement necessary synchronization (`MPI_Wait`, `cudaDeviceSynchronize` or Events).
    *   Update `Makefile` for `nvcc -ccbin=mpicxx`.
2.  **Compile & Test V4:** Use `make` and `mpirun -np [2, 4, 8] ./template` (on appropriate nodes/GPUs).
3.  **Debug V4:** Resolve integration issues, focusing on data consistency between host/device and correct synchronization.
4.  **Initial V4 Performance Measurement:** Record wall time using `MPI_Wtime` and CUDA Events. Compare rough timing with V2.2 and V3.
5.  **Plan V5:** Based on V4 experience and cluster verification, outline specific code changes needed for CUDA-aware MPI calls.

---

## 7. Design Decisions & Considerations

*(**Instructions:** Note significant choices made and alternatives considered)*

*   **V2 Strategy Choice:** Implemented both V2.1 (Broadcast) and V2.2 (Scatter+Halo) to directly compare simple vs. scalable MPI approaches. V2.2 provides the better foundation for V4.
*   **V3 Kernels:** Implemented basic, functionally correct CUDA kernels. Performance optimization deferred to later or as part of V4/V5 profiling.
*   **V4 Initial Plan:** Focus on correct synchronous host-staging implementation first before attempting asynchronous overlap.
*   **Data Structures:** Using `std::vector` on host, raw `float*` (`cudaMalloc`) on device for V3+. Pinned host memory (`cudaMallocHost`) should be considered for V4/V5 staging buffers to enable async copies later.

---

## 8. Performance & Benchmarking (WSL2 Dev Machine - Blocks 1&2)

*   **V1 (Serial):** ~617 ms (np=1)
*   **V2 (MPI - 2.1 Broadcast):** ~660ms (np=1), ~704ms (np=2), ~802ms (np=4) -> **Degrades**
*   **V2 (MPI - 2.2 Scatter+Halo):** ~491ms (np=1), ~334ms (np=2), ~177ms (np=4) -> **Speeds up**
*   **V3 (CUDA):** ~750 ms (np=1) -> **Needs Profiling/Optimization**
*   **V4 (MPI+CUDA):** *(To be added)*
*   **V5 (CUDA-Aware MPI):** *(To be added)*

*(Note: Absolute times will differ on target cluster. Relative scaling and bottlenecks are key points of analysis.)*

---
---

## Meeting Summaries & Action Items

*(**Instructions:** Manually add notes after each meeting with the professor)*

**YYYY-MM-DD - Discussion with Prof. Sohn**
*   **Topics Discussed:** ...
*   **Key Feedback / Decisions:** ...
*   **Action Items:** ...

---


---

*End of combined document.*
