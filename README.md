Update @ 12/2/2025: Hi! If you're reading this, some of the pictures here are broken BUT the diagrams I used are probably the top search result if you just google, for example, "AlexNet Diagram". Furthermore, this project only focuses on the *first half* of AlexNet with a DIY approach on my 3090. If you have an AMD GPU this project may not be compatible. Next version will have a wider compatibility for GPUs

Finally, I wasn't able to update this repo with my final data as I had to present and then the semester ended, but nonetheless the code works and you can run the experiment yourself and see if your predictions were correct! There is a quickstart in the README below

# Combined Project Documentation (final_project)

*This file combines key project README, context, research, and discussion logs.*
*It is automatically generated by the `collect_p_docs.sh` script.*


---

## Content from: `final_project/README.md`

# CS485: GPU Cluster Programming (MPI+CUDA) – Final Project: AlexNet Inference

This directory contains the code, documentation, and resources for the final project of the CS485 GPU Cluster Programming course: an evolving MPI+CUDA implementation of AlexNet inference. The project follows a staged development approach, progressing from serial execution to advanced hybrid parallelism.

**Project Scope Clarification:** The primary focus of the V1-V5 implementation plan is on the **initial two blocks** of the AlexNet architecture (Conv1->ReLU->Pool1 and Conv2->ReLU->Pool2->LRN2). This provides a representative and computationally significant workload for learning and comparing parallelization techniques (MPI, CUDA, Hybrid). Implementing the *full* AlexNet network (including Conv3-5, FC6-8, Softmax) is considered an extension task to be undertaken only *after* the successful completion and analysis of V1-V5 for the initial subset, if time permits.

![AlexNet Diagram](docs/1_M4jjkOPxvOYNk8f1c7rOcQ.png)

## Table of Contents
1.  [Project Overview](#1-project-overview)
2.  [Target Environment](#2-target-environment)
3.  [Repository Structure (This Directory)](#3-repository-structure-this-directory)
4.  [Project Version Implementation Plan (Blocks 1 & 2 Focus)](#4-project-version-implementation-plan-blocks-1--2-focus)
    *   [Version 1: Serial CPU](#version-1-serial-cpu---completed)
    *   [Version 2: MPI Only (CPU)](#version-2-mpi-only-cpu---completed)
        *   [Approach 2.1: Broadcast All](#approach-21-broadcast-all---implemented)
        *   [Approach 2.2: Scatter + Halo](#approach-22-scatter--halo---implemented)
    *   [Version 3: CUDA Only (Single GPU)](#version-3-cuda-only-single-gpu---completed)
    *   [Version 4: MPI + CUDA (Hybrid)](#version-4-mpi--cuda-hybrid---implemented-debugging)
    *   [Version 5: CUDA-Aware MPI (Optional Optimization)](#version-5-cuda-aware-mpi-optional-optimization---pending)
5.  [Key Technologies](#5-key-technologies)
6.  [Current Implementation Status & Performance Highlights](#6-current-implementation-status--performance-highlights)
7.  [Development Workflow for Versions](#7-development-workflow-for-versions)
8.  [Build & Test Instructions per Version](#8-build--test-instructions-per-version)
9.  [Presentation Strategy](#9-presentation-strategy)
10. [Troubleshooting](#10-troubleshooting)
11. [Future Directions & Extensions](#11-future-directions--extensions)
12. [References & Resources](#12-references--resources)

## 1. Project Overview
This project implements inference for the **initial two blocks** of the AlexNet convolutional neural network (Conv1->ReLU->Pool1 and Conv2->ReLU->Pool2->LRN2). The primary goal is to learn and apply different parallel programming paradigms (MPI, CUDA, MPI+CUDA) to this representative workload and to systematically evaluate their performance impact through a structured, incremental 5-version approach. The focus is on the parallelization techniques and performance analysis, rather than building a complete end-to-end classifier.

**Core Task:** Implement and benchmark inference for AlexNet Blocks 1 & 2 across different parallelization stages (V1-V5).

**Parallelization Strategy (V4/V5):** Primarily data parallelism. Input data rows are distributed across MPI ranks (Scatter+Halo).
*   **V4 Approach:** Each rank performs MPI halo exchange using host buffers, copies its padded data tile to the GPU, executes the *entire* V3 CUDA layer sequence on that tile using a helper function (`alexnetTileForwardCUDA`), copies the result tile back to the host, and trims halo-related rows on the host before the final MPI gather.
*   **V5 Approach (Planned):** Optimize V4 using CUDA-aware MPI calls to potentially eliminate explicit host staging for MPI communication.

## 2. Target Environment
Code must ultimately compile and run correctly under the course's specified environment:
- **OS:** Fedora 37
- **Compilers:** GCC 12 (for host code), `mpicc`/`mpicxx` (MPI wrappers), `nvcc` (CUDA compiler)
- **GPU Toolkit:** CUDA 12.x
- **MPI:** Open MPI (ideally compiled with CUDA-awareness for V5)

Local development is done in WSL2 (Ubuntu) with compatible toolchains, but final testing should target the Fedora environment.

## 3. Repository Structure (This Directory)

```
final_project/
├── v1_serial/ # V1: Serial CPU implementation (COMPLETE)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v2_mpi_only/ # V2: MPI-only (CPU cores) implementation (COMPLETE)
│ ├── 2.1_broadcast_all/ # -> Approach 2.1 (Implemented)
│ │ ├── include/
│ │ ├── src/
│ │ └── Makefile
│ └── 2.2_scatter_halo/ # -> Approach 2.2 (Implemented)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v3_cuda_only/ # V3: CUDA-only (single GPU) implementation (COMPLETE)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v4_mpi_cuda/ # V4: Baseline MPI + CUDA implementation (IMPLEMENTED - DEBUGGING)
│ ├── include/
│ ├── src/
│ └── Makefile
├── v5_cuda_aware_mpi/ # V5: Optional CUDA-aware MPI optimization (PENDING - Baseline Code)
│ ├── include/
│ ├── src/
│ └── Makefile
├── data/ # SHARED: Input data, model weights, etc. (Accessed via ../data/ or ../../data/)
├── docs/ # SHARED: Project documentation, design notes (Accessed via ../docs/ or ../../docs/)
├── logs/ # Basic run logs (gitignored)
├── logs_extended/ # Detailed performance logs (gitignored)
├── ai_context.txt # Technical context summary for AI assistant
├── discussion.md # Rolling log for professor meetings
├── project.txt # Concatenated source code dump (generated by script)
├── RESEARCH.md # Research findings, critical analysis, references
└── README.md # This file
```

**Note:** Shared resources (`data/`, `docs/`) are accessed from within versioned source code using relative paths. Adjust paths depending on whether accessing from `vX/` or `v2_mpi_only/X.Y/`.

## 4. Project Version Implementation Plan (Blocks 1 & 2 Focus)

The project progresses through five versions, focusing on Blocks 1&2 of AlexNet. V2 explored two distinct MPI strategies. The core goal is to demonstrate understanding of each parallelization paradigm using this subset.

---
### Version 1: Serial CPU - COMPLETED
*   **Directory:** `final_project/v1_serial/`
*   **Goal:** Correct, sequential implementation on a single CPU core. Established functional baseline.
*   **Implementation:** Pure C++, `std::vector`, direct loops for layers.

---
### Version 2: MPI Only (CPU) - COMPLETED
*   **Goal:** Parallelize V1 logic across multiple CPU cores using MPI.
*   **Directory Structure:** Contains subdirectories for implemented approaches.

#### Approach 2.1: Broadcast All - IMPLEMENTED
*   **Directory:** `final_project/v2_mpi_only/2.1_broadcast_all/`
*   **Strategy:** Rank 0 `MPI_Bcast`s full input/parameters. All ranks compute the full V1 layer sequence locally. Each rank extracts its assigned slice from the *final* output only. Rank 0 gathers slices via `MPI_Gatherv`.
*   **Outcome:** Simple implementation, validated basic MPI communication, but demonstrated poor scalability due to broadcast overhead and redundant computation. Serves as a contrast to more scalable methods.

#### Approach 2.2: Scatter + Halo - IMPLEMENTED
*   **Directory:** `final_project/v2_mpi_only/2.2_scatter_halo/`
*   **Strategy:** Rank 0 `MPI_Scatterv`s input rows. Ranks exchange halo regions using non-blocking `MPI_Isend`/`MPI_Irecv`/`MPI_Wait` before convolution layers (specifically V2 implementation handled halos before each block needing them). Parameters are broadcast. Each rank computes only on its local data (+halos). Rank 0 gathers final results via `MPI_Gatherv`. Required careful index management and handling of boundary conditions/padding, including asymmetric trimming after pooling.
*   **Outcome:** More complex implementation, but demonstrated expected speedup and better scalability compared to 2.1. Represents a more realistic MPI parallelization pattern for convolutions.

---
### Version 3: CUDA Only (Single GPU) - COMPLETED
*   **Directory:** `final_project/v3_cuda_only/`
*   **Goal:** Port V1 compute logic (layers) to run on a single GPU using CUDA. Establish GPU baseline performance.
*   **Implementation:** Basic CUDA kernels implemented for Conv, ReLU, Pool, LRN (e.g., 1D grid-stride loops). Host code manages `cudaMalloc`/`cudaMemcpy`/`cudaFree`, kernel launches. Basic `CUDA_CHECK` error handling used. Uses `nvcc` compiler.
*   **Outcome:** Functional GPU implementation. Initial performance analysis indicates potential bottlenecks likely related to host-device transfer overhead or unoptimized kernels (requires profiling). Sample output values differ from V1/V2, needing investigation.

---
### Version 4: MPI + CUDA (Hybrid) - IMPLEMENTED (Debugging)
*   **Directory:** `final_project/v4_mpi_cuda/`
*   **Goal:** Combine MPI parallelism (inter-rank, based on V2.2 Scatter+Halo logic) with CUDA parallelism (intra-rank GPU kernel execution from V3).
*   **Implementation Status:** Code implemented but requires debugging.
*   **Current Strategy Implemented:**
    1.  MPI Setup: Ranks initialize, parameters broadcast, input data rows scattered via `MPI_Scatterv` to host buffers.
    2.  Host Halo Exchange: Halo regions for Conv1 are exchanged between ranks using `MPI_Isend`/`Irecv`/`Wait` on host buffers.
    3.  Host->Device Copy: The *padded* local input slice (local rows + received halos) is copied to the GPU (`cudaMemcpyHostToDevice`).
    4.  GPU Computation: A single helper function (`alexnetTileForwardCUDA`) executes the *entire* V3 layer sequence (Conv1->...->LRN2) on the GPU for the padded tile. Intermediate layer results stay on the GPU within this function.
    5.  Device->Host Copy: The final output tile (corresponding to the padded input) is copied back to the host (`cudaMemcpyDeviceToHost`).
    6.  Host Trimming: Rows corresponding to the initial halo padding are trimmed from the host result buffer based on rank (simplified logic currently used).
    7.  Result Aggregation: Trimmed local results are gathered via `MPI_Gatherv` to rank 0.
    8.  GPU Affinity: `cudaSetDevice` is used based on MPI rank.
    9.  Build: Uses `nvcc -ccbin=mpicxx`.
*   **Issues:**
    *   Output format mismatch prevents automatic result parsing by test script.
    *   Runtime error occurs when running with 4 processes (exit code 134).
    *   Correctness of trimming logic and overall numerical output needs verification.
*   **Alternative Path:** An unused function (`alexnetForwardPassMPI_CUDA`) exists with more complex trimming logic, suggesting a potential refinement path.

---
### Version 5: CUDA-Aware MPI (Optional Optimization) - PENDING
*   **Directory:** `final_project/v5_cuda_aware_mpi/`
*   **Goal:** Optimize V4 by using CUDA-aware MPI calls (passing GPU device pointers directly to MPI) to potentially reduce/eliminate host staging overhead *for MPI communication*.
*   **Planned Actions:**
    1.  Modify V4 code's `MPI_Scatterv`, `MPI_Isend`, `MPI_Irecv`, `MPI_Gatherv` calls to use *device* pointers.
    2.  Remove explicit H<->D `cudaMemcpy` calls related solely to MPI staging (copies for padding/trimming might still be needed depending on approach).
    3.  Verify cluster support (CUDA-aware OpenMPI build, HW support like GPUDirect RDMA). Configure environment if needed.
    4.  Compare performance against V4 to quantify benefit/overhead.
    5.  Build with `nvcc -ccbin=mpicxx`.

## 5. Key Technologies
- **MPI (Open MPI):** Distributed memory communication.
- **CUDA (NVIDIA):** GPU programming (`nvcc`, runtime API, kernels).
- **C++11/17:** Host code logic, `std::vector`.
- **Make:** Build system (including `bear` and `clang-tidy` integration in V4).
- **Bash:** Automation scripts (testing, packaging).

## 6. Current Implementation Status & Performance Highlights

*(Based on `run_final_project.sh` output on WSL2 dev machine - Times approximate, relative scaling is key)*

| Version                | Procs | Shape     | First 5 Vals (Sample)          | Time       | Status | Notes                                      |
| :--------------------- | :---- | :-------- | :----------------------------- | :--------- | :----- | :----------------------------------------- |
| V1 Serial              | 1     | 13x13x256 | `0.0653 0.0579 0.0668 ...`     | ~667 ms    | ✔      | Baseline                                   |
| V2 2.1-broadcast-all | 1     | 13x13x256 | `44.4152 42.4612 40.6967 ...` | ~679 ms    | ✔      |                                            |
| V2 2.1-broadcast-all | 2     | 13x13x256 | `44.4152 42.4612 40.6967 ...` | ~809 ms    | ✔      | Scalability issue                          |
| V2 2.1-broadcast-all | 4     | 13x13x256 | `44.4152 42.4612 40.6967 ...` | ~881 ms    | ✔      | Degrades                                   |
| V2 2.2-scatter-halo  | 1     | 13x13x256 | `44.4152 42.4612 40.6967 ...` | ~561 ms    | ✔      |                                            |
| V2 2.2-scatter-halo  | 2     | 13x13x256 | `29.2981 28.7552 28.2351 ...` | ~343 ms    | ✔      | Speedup                                    |
| V2 2.2-scatter-halo  | 4     | 13x13x256 | `29.2981 28.7552 28.2351 ...` | ~281 ms    | ✔      | Good scaling                               |
| V3 CUDA                | 1     | 13x13x256 | `29.2932 25.9153 23.3255 ...` | ~2349 ms   | ✔      | Slower than serial; needs profiling        |
| V4 MPI+CUDA          | 1     | –         | –                              | –          | ⚠      | Runs; Output format mismatch               |
| V4 MPI+CUDA          | 2     | –         | –                              | –          | ⚠      | Runs; Output format mismatch               |
| V4 MPI+CUDA          | 4     | –         | –                              | –          | ⚠      | Fails (Exit 134 - MPI/Resource issue?)     |
| V5 CUDA-Aware        | -     | -         | -                              | -          | (PENDING) |                                            |

**Observations:**
*   V2.2 shows expected MPI speedup. V2.1 shows expected overhead penalty.
*   V3 (CUDA) is currently slower than V1 (Serial), indicating significant overhead or unoptimized kernels. Profiling needed.
*   V4 runs for NP=1,2 but results aren't parsed due to output format differences. NP=4 fails during runtime. **V4 needs debugging.**
*   Sample output values differ significantly between V1, V2.1, V2.2(np>=2), and V3, suggesting potential numerical differences or initialization variations that need investigation for correctness.

## 7. Development Workflow for Versions
1.  **Choose Version/Approach:** Select the target (e.g., Debug V4, Implement V5).
2.  **Navigate:** `cd final_project/vX_suffix[/Y.Z_approach]`
3.  **Implement/Debug:** Modify code in `src/` and `include/` based on the version's goal.
4.  **Update Makefile:** Adjust compiler, flags, libraries, sources if needed.
5.  **Build:** Run `make clean && make` (or `make compile_commands && make build_only` for V4+).
6.  **Test:** Execute the compiled `template` executable using appropriate command (serial/MPI). Use `run_final_project.sh` for automated testing.
7.  **Analyze/Profile:** Use timers (`MPI_Wtime`, CUDA Events), log files, debuggers (GDB, `cuda-gdb`), and profilers (Nsight Systems/Compute) to understand performance and correctness.
8.  **Commit:** Save changes frequently using Git.

## 8. Build & Test Instructions per Version

**(Run commands from within the respective subdirectory)**

*   **V1 (Serial):** `make clean && make && ./template`
*   **V2 (MPI - Broadcast All):** `make clean && make && mpirun -np <N> ./template`
*   **V2 (MPI - Scatter+Halo):** `make clean && make && mpirun -np <N> ./template`
*   **V3 (CUDA Only):** `make clean && make && ./template`
*   **V4 (MPI+CUDA):** `make clean && make && mpirun -np <N> ./template` (Use `make lint` for checks)
*   **V5 (CUDA-Aware MPI):** `make clean && make && mpirun -np <N> ./template` *(Requires correctly configured CUDA-aware MPI environment)*

*(Use `--oversubscribe` for `mpirun` if testing locally with N > physical cores. On cluster, use `-hostfile` and mapping options.)*
*Use `../../scripts/run_final_project.sh` from the repo root to run all versions and generate the summary table.*

## 9. Presentation Strategy

The final project presentation will focus on the **journey of parallelization and performance analysis using the first two blocks of AlexNet** as the consistent workload. Key elements will include:
*   **Demonstration:** Show V1-V4 (or highest working version) running. Explain V4 debugging status.
*   **Methodology:** Explain the parallelization techniques applied in each distinct version (Serial, MPI Broadcast, MPI Scatter+Halo, CUDA, MPI+CUDA Host-Staging). Justify design choices (e.g., V4's full-tile-on-GPU approach).
*   **Performance Analysis:** Present comprehensive results from the target cluster:
    *   Wall-clock times for each version (using corrected V4 results).
    *   Speedup and Efficiency plots relative to V1.
    *   Scalability analysis (strong scaling) for MPI/Hybrid versions (V2.x, V4).
    *   Timing breakdowns (computation vs communication vs H<->D transfer) using profiler data (especially for V3/V4) to identify bottlenecks.
    *   Discuss observed numerical differences if they persist.
*   **Conclusion:** Summarize key learnings, challenges overcome (including V4 debugging), and the effectiveness of each parallelization approach for this specific workload.

## 10. Troubleshooting
- Makefile Errors: Check TABs vs spaces, variable names, paths, dependencies. Run `make -d` for debug info. For V4, ensure `bear` is installed if using `make compile_commands`.
- Include Errors: Verify include paths (`-I`), header guards (`#pragma once` or `#ifndef`).
- Linker Errors: Check linked libraries (`-l`), library paths (`-L`), ensure all object files are included, check for symbol mismatches (`nm object.o`). For V4/V5, ensure MPI/CUDA libs are linked correctly via `nvcc -ccbin=mpicxx`.
- MPI Runtime Errors: Check `mpirun` syntax (`-np`, `--oversubscribe`, hostfile). Check for deadlocks (mismatched send/recv tags, collective calls). Check buffer sizes/counts. Use `MPI_Abort` with error codes. Examine log files from test script.
- CUDA Errors: Use `CUDA_CHECK` macro around *all* runtime API calls and after kernel launches (`cudaGetLastError`). Use `cuda-memcheck` or `compute-sanitizer` to detect memory errors. Check kernel launch configurations (grid/block dims). Ensure correct device is set (`cudaSetDevice`).
- Path Errors: Double-check relative paths (`../`, `./`) used for includes or data access.
- V4 Runtime Error (NP=4): Examine `final_project/logs/final_project_v4_np4.log` first. Re-run with `-g`, potentially under `gdb --args mpirun -np 4 ./template`, or use `cuda-memcheck mpirun -np 4 ./template`. Check resource limits (memory, GPU access per node).

## 11. Future Directions & Extensions
- **Debug & Validate V4:** Fix output format, resolve NP=4 runtime error, verify numerical correctness and trimming logic against V1/V3.
- **Profile V3 & V4:** Use Nsight Systems/Compute to understand V3 slowness and identify V4 bottlenecks (likely H<->D copies, host halo exchange).
- **Implement V5:** If V4 is stable and cluster supports it, implement CUDA-aware MPI optimization and benchmark against V4.
- **Performance Optimization:** Apply techniques based on profiling (e.g., async overlap in V4/V5, kernel tuning).
- **Full AlexNet Implementation:** *If time permits after V1-V5*, extend the most performant version to include remaining layers.
- **Explore Alternative V4 Strategy:** Consider refactoring V4 to use the more complex logic in the unused `alexnetForwardPassMPI_CUDA` function for potentially finer-grained control or different performance characteristics.

## 12. References & Resources
- MPI Forum: [mpi-forum.org](https://mpi-forum.org/)
- NVIDIA CUDA Documentation: [docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)
- Programming Massively Parallel Processors (4th Ed.) Textbook & Companion Site
- Open MPI Documentation: [open-mpi.org](https://www.open-mpi.org/)
- LLNL HPC Tutorials: [hpc-tutorials.llnl.gov](https://hpc-tutorials.llnl.gov/)
- `final_project/RESEARCH.md` for detailed analysis and specific paper references.


---

## Content from: `final_project/AI.md`

# AI Context Document: CS485 Final Project - AlexNet Inference (MPI+CUDA)

**Version:** 1.0 (Updated based on V1-V4 Code Review & Script Output)
**Purpose:** Provide comprehensive context for AI assistants regarding the structure, status, implementation details, challenges, and goals of the CS485 final project. This aims to minimize the need for re-explaining core concepts or re-pasting large code sections in future interactions.

## 1. Project Overview & Scope

*   **Objective:** Implement and benchmark inference for the **first two blocks of the AlexNet CNN** (Conv1->ReLU->Pool1 -> Conv2->ReLU->Pool2->LRN2) across a series of parallelization stages (V1-V5).
*   **Goal:** Learn and compare different parallel programming paradigms (Serial, MPI, CUDA, Hybrid MPI+CUDA) and analyze their performance characteristics (speedup, scalability, bottlenecks) on this representative workload. Full AlexNet implementation is an optional extension.
*   **Core Task:** Produce working code for each stage (V1-V5), perform detailed performance analysis, and present findings.
*   **Target Environment:** Fedora 37, GCC 12, CUDA 12.x, Open MPI (ideally CUDA-aware for V5). Development occurs in WSL2 (Ubuntu) but final validation must be on the target platform.

## 2. Overall Project Structure

*   **Staged Development (V1-V5):** Incremental approach adding complexity:
    *   V1: Serial CPU (Baseline)
    *   V2: MPI Only (CPU cores)
    *   V3: CUDA Only (Single GPU)
    *   V4: MPI + CUDA Hybrid (Host Staging)
    *   V5: MPI + CUDA Hybrid (CUDA-Aware MPI optimization)
*   **Directory Structure:** Organized by version (`final_project/v1_serial/`, `v2_mpi_only/`, etc.), with sub-directories for V2 approaches. Shared data/docs accessed via relative paths. Key files: `README.md`, `RESEARCH.md`, `discussion.md`, `ai_context.txt` (this file), Makefiles, source (`src/`), includes (`include/`).
    ```
    final_project/
    ├── v1_serial/       (Complete)
    ├── v2_mpi_only/
    │   ├── 2.1_broadcast_all/ (Complete)
    │   └── 2.2_scatter_halo/  (Complete)
    ├── v3_cuda_only/    (Complete)
    ├── v4_mpi_cuda/     (Implemented - Debugging)
    ├── v5_cuda_aware_mpi/ (Pending - Baseline Code)
    ├── data/
    ├── docs/
    ├── logs/
    ├── scripts/         (Contains run_final_project.sh)
    └── ... (Documentation files)
    ```
*   **Build System:** Standard Makefiles per version.
    *   V1: `g++`
    *   V2: `mpicxx`
    *   V3: `nvcc`
    *   V4/V5: `nvcc -ccbin=mpicxx` (compiles both .cu and .cpp, links MPI/CUDA). V4 Makefile includes `bear` integration for `compile_commands.json` generation and `clang-tidy` for linting.
    *   Key Flags: `-std=c++11/17`, `-O3`, `-Wall`, MPI include/link flags (via `mpicxx --showme`), CUDA arch flags (`-gencode arch=compute_75,code=sm_75`).

## 3. Data Structures & Parameters

*   **`LayerParams` Struct:** Holds parameters for a logical block (e.g., Conv+Pool+LRN). Defined in `include/alexnet.hpp` (slight variations between versions). Host vectors used for storage.
    ```c++
    // Representative Structure (V1/V3 style)
    struct LayerParams {
        std::vector<float> weights; // Host vectors for weights
        std::vector<float> biases;  // Host vectors for biases
        int K, F, S, P;       // Conv params (Kernels, FilterSize, Stride, Padding)
        int F_pool, S_pool;   // Pooling params
        int N_lrn;            // LRN params
        float alpha, beta, k_lrn;
    };
    ```
*   **Data Representation:** Primarily uses `std::vector<float>` on the host and raw `float*` pointers (allocated via `cudaMalloc`) on the CUDA device.
*   **Key Dimensions/Params (Consistent):**
    *   Input: H=227, W=227, C=3
    *   Conv1: K=96, F=11, S=4, P=0
    *   Pool1: F=3, S=2
    *   Conv2: K=256, F=5, S=1, P=2 (Input Channels = Conv1 K = 96)
    *   Pool2: F=3, S=2
    *   LRN2: N=5, alpha=1e-4, beta=0.75, k=2.0
    *   Final Output Shape (Expected): H=13, W=13, C=256 (Conv2 K)

## 4. Implementation Strategies & Status per Version

### 4.1 Version 1: Serial CPU (Completed)
*   **Strategy:** Straightforward C++ implementation using nested loops for convolution, pooling, etc. Operates on `std::vector<float>`. Serves as functional baseline and correctness reference.
*   **Key Files:** `v1_serial/src/{main.cpp, alexnet_serial.cpp, layers_serial.cpp}`.

### 4.2 Version 2: MPI Only (CPU) (Completed)
*   **Goal:** Parallelize V1 using MPI across CPU cores. Two approaches implemented.
*   **V2.1 (Broadcast All):**
    *   Strategy: Rank 0 broadcasts full input & parameters. All ranks compute the *entire* V1 sequence locally. Rank 0 gathers the final slice needed from each rank.
    *   Outcome: Simple but scales poorly (communication bottleneck, redundant computation).
*   **V2.2 (Scatter + Halo):**
    *   Strategy: Rank 0 scatters input rows (`MPI_Scatterv`). Ranks exchange halo regions (boundary rows needed for convolution) using non-blocking `MPI_Isend`/`Irecv`/`Wait`. Parameters broadcast. Ranks compute V1 sequence on their local data + halos. Asymmetric trimming applied after pooling layers to remove halo influence. Final results gathered (`MPI_Gatherv`).
        ```c++
        // Snippet: V2.2 Halo Exchange Concept (main.cpp)
        // ... scatter input to localIn ...
        int pad1 = conv1.F / 2;
        int slice1 = pad1 * W * C;
        std::vector<float> topHalo(slice1), botHalo(slice1);
        MPI_Request reqs; int q=0;
        if (rank > 0) {
            MPI_Irecv(topHalo.data(), slice1, MPI_FLOAT, rank - 1, 1, MPI_COMM_WORLD, &reqs[q++]);
            MPI_Isend(localIn.data(), slice1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, &reqs[q++]);
        } // else fill topHalo with 0
        if (rank < size - 1) {
            MPI_Irecv(botHalo.data(), slice1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, &reqs[q++]);
            MPI_Isend(localIn.data() + (localH - pad1) * W * C, slice1, MPI_FLOAT, rank + 1, 1, MPI_COMM_WORLD, &reqs[q++]);
        } // else fill botHalo with 0
        MPI_Waitall(q, reqs, MPI_STATUSES_IGNORE);
        // ... construct padded buffer using localIn, topHalo, botHalo ...
        // ... compute layers ...
        // ... trim result ...
        // ... gather ...
        ```
    *   Outcome: More complex but scales well, demonstrating effective MPI communication pattern. Foundation for V4.
*   **Key Files:** `v2_mpi_only/2.2_scatter_halo/src/{main.cpp, alexnet_mpi.cpp, layers_mpi.cpp}` (V2.2 is primary).

### 4.3 Version 3: CUDA Only (Single GPU) (Completed)
*   **Strategy:** Port V1 layer logic to CUDA kernels. Host code manages data lifecycle: allocate device memory (`cudaMalloc`), copy input/weights H->D (`cudaMemcpyHostToDevice`), launch sequence of kernels (Conv1, ReLU1, Pool1, ... LRN2), copy final result D->H (`cudaMemcpyDeviceToHost`), free device memory (`cudaFree`). Basic 1D grid-stride kernels.
    ```c++
    // Snippet: V3 Execution Flow Concept (alexnet_cuda.cu)
    // ... calculate dims ...
    // ... cudaMalloc all device buffers (d_input, d_c1out, d_p1out, etc., d_weights, d_biases) ...
    // ... cudaMemcpy H->D input_host -> d_input, params -> d_weights/d_biases ...

    // Layer Sequence Execution:
    cudaConvLayer(d_c1out, d_input, d_w1, d_b1, ...);
    cudaReluLayer(d_c1out, c1_sz);
    cudaMaxPoolLayer(d_p1out, d_c1out, ...);
    cudaConvLayer(d_c2out, d_p1out, d_w2, d_b2, ...);
    cudaReluLayer(d_c2out, c2_sz);
    cudaMaxPoolLayer(d_p2out, d_c2out, ...);
    cudaLRNLayer(d_l2out, d_p2out, ...);

    // ... cudaMemcpy D->H d_l2out -> output_host ...
    // ... cudaFree all device buffers ...
    ```
*   **Outcome:** Functionally complete. Performance is currently *worse* than V1 serial, indicating high H<->D transfer overhead or inefficient kernels. Needs profiling (`Nsight Systems`/`Compute`). Numerical output differs from V1/V2.
*   **Key Files:** `v3_cuda_only/src/{main_cuda.cpp, alexnet_cuda.cu, layers_cuda.cu}`.

### 4.4 Version 4: MPI + CUDA (Hybrid) (Implemented - Needs Debugging)
*   **Strategy:** Combine V2.2 MPI data distribution (Scatter+Halo) with V3 CUDA computation. **Crucially, uses explicit Host Staging.**
*   **Execution Flow (`main_mpi_cuda.cpp` calling `alexnetTileForwardCUDA`):**
    1.  MPI Setup: Parameters broadcast, input rows scattered to host buffers (`myIn`).
    2.  **Host Halo Exchange:** Conv1 halo regions exchanged via `MPI_Isend/Irecv/Wait` into host buffers (`recvT`, `recvB`). `myIn` is then conceptually padded on host (though implementation inserts into `std::vector`).
    3.  **Host -> Device Transfer:** The *entire padded* host buffer `myIn` is copied to device memory `d_in` (`cudaMemcpyHostToDevice`).
    4.  **GPU Tile Computation:** Call `alexnetTileForwardCUDA(d_in, ..., d_out)`. This helper function (in `alexnet_mpi_cuda.cu`) internally allocates intermediate device buffers (for conv1, pool1, etc.), allocates+copies weights/biases H->D *within the function*, launches the V3 kernel sequence (Conv1..LRN2), and frees internal buffers. The final result is placed in `d_out`.
        ```c++
        // Snippet: V4 GPU Tile Computation Call (main_mpi_cuda.cpp)
        // ... Host halo exchange into myIn ...
        float *d_in=nullptr; CUDA_CHECK(cudaMalloc(&d_in, myIn.size()*sizeof(float)));
        CUDA_CHECK(cudaMemcpy(d_in, myIn.data(), myIn.size()*sizeof(float), cudaMemcpyHostToDevice));
        // ... Calculate final output tile size (Hp2, Wp2, p2.K) ...
        std::vector<float> tileOut((size_t)Hp2 * Wp2 * p2.K); // Host buffer for D->H result
        float* d_out; CUDA_CHECK(cudaMalloc(&d_out, tileOut.size()*sizeof(float))); // Device buffer for final tile output

        // Runs ENTIRE sequence Conv1..LRN2 on the GPU for the padded tile d_in
        alexnetTileForwardCUDA(d_in, p1, p2, (int)myIn.size()/rowSz, W, C, d_out);

        CUDA_CHECK(cudaMemcpy(tileOut.data(), d_out, tileOut.size()*sizeof(float), cudaMemcpyDeviceToHost));
        // ... Host trimming of tileOut into local ...
        // ... Gather local ...
        cudaFree(d_in); cudaFree(d_out);
        ```
    5.  **Device -> Host Transfer:** Final result `d_out` is copied back to host buffer `tileOut` (`cudaMemcpyDeviceToHost`).
    6.  **Host Trimming:** Halo-related rows are removed from `tileOut` on the host to create `local` result (current logic seems simplified).
    7.  MPI Gather: `local` results are gathered (`MPI_Gatherv`).
    8.  GPU Affinity: `cudaSetDevice(rank % nDev)` used.
*   **Current Status:** Implemented but debugging needed.
*   **Key Issues:**
    *   **Output Format:** `cout` statements in `main_mpi_cuda.cpp` don't match `run_final_project.sh` expectations, causing parsing warnings (`⚠`). Needs string changes.
    *   **NP=4 Crash:** Fails with MPI Exit Code 134 when run with 4 processes. Log file `logs/final_project_v4_np4.log` needs inspection. Likely resource issue, memory error, or MPI communication bug triggered at np=4.
    *   **Correctness/Trim:** Numerical output needs verification. Host trimming logic correctness needs review.
*   **Key Files:** `v4_mpi_cuda/src/{main_mpi_cuda.cpp, alexnet_mpi_cuda.cu, layers_mpi_cuda.cu}`. Note the existence of an unused, more complex function `alexnetForwardPassMPI_CUDA` in `alexnet_mpi_cuda.cu`.

### 4.5 Version 5: CUDA-Aware MPI (Pending)
*   **Goal:** Optimize V4 by eliminating explicit host staging *for MPI communication calls* (`Scatterv`, `Isend`, `Irecv`, `Gatherv`).
*   **Strategy:** Modify V4 MPI calls to pass **device pointers** directly. Requires CUDA-aware Open MPI build and potentially GPUDirect RDMA support on the cluster hardware/network for maximum benefit. If RDMA isn't available, the library might internally stage through host memory, reducing benefits.
*   **Status:** Baseline code exists, implementation pending V4 stabilization.

## 5. Performance Summary & Observations (WSL2 Dev Env - Script Output)

| Version                | Procs | Shape     | Time       | Status | Key Observation                             |
| :--------------------- | :---- | :-------- | :--------- | :----- | :------------------------------------------ |
| V1 Serial              | 1     | 13x13x256 | ~667 ms    | ✔      | Baseline                                    |
| V2 2.1 Broadcast       | 4     | 13x13x256 | ~881 ms    | ✔      | Poor scaling (Broadcast/Compute overhead) |
| V2 2.2 Scatter+Halo  | 4     | 13x13x256 | ~281 ms    | ✔      | Good scaling (Efficient MPI comm)         |
| V3 CUDA                | 1     | 13x13x256 | ~2349 ms   | ✔      | Slow (H<->D / Kernel Bottleneck?)         |
| V4 MPI+CUDA          | 1     | –         | –          | ⚠      | Runs, Output format mismatch              |
| V4 MPI+CUDA          | 2     | –         | –          | ⚠      | Runs, Output format mismatch              |
| V4 MPI+CUDA          | 4     | –         | –          | ⚠      | CRASHES (Exit 134)                          |

*   V2.2 shows significant speedup over V1 and V2.1.
*   V3 is unexpectedly slow; needs profiling.
*   V4 is functionally incomplete due to output/crash issues. Performance cannot be assessed yet.
*   Significant numerical differences observed in sample outputs between V1, V2.1, V2.2(np>1), and V3 require investigation.

## 6. Immediate Debugging Tasks (V4)

1.  **Fix Output Formatting:** In `v4_mpi_cuda/src/main_mpi_cuda.cpp`, modify `std::cout` lines in the `rank == 0` block to match the expected format:
    *   `shape HxWxK` -> `Final Output Shape: HxWxK`
    *   `sample v1 v2 ...` -> `Final Output (first 10 values): v1 v2 ...`
    *   Add time output: `std::cout << "AlexNet MPI+CUDA Forward Pass completed in " << duration_ms << " ms" << std::endl;` (and update `run_final_project.sh` to parse it).
2.  **Diagnose NP=4 Crash:**
    *   Analyze `logs/final_project_v4_np4.log` for MPI/CUDA error messages.
    *   Re-run manually: `mpirun -np 4 ./template`.
    *   If needed, use debuggers (`gdb --args mpirun ...`, `cuda-gdb`) or memory checkers (`cuda-memcheck mpirun ...`). Check for out-of-bounds access, incorrect MPI counts/displacements, resource exhaustion.
3.  **Verify Correctness:**
    *   Once V4 output is parseable, compare numerical values against V1/V3 reference outputs (allow for floating-point tolerance).
    *   Review the host trimming logic in `main_mpi_cuda.cpp` (using `start`/`stop` variables) - ensure it correctly removes only the rows corresponding to the effective halo contribution after the full layer sequence. Consider edge cases (small inputs, different NP values).

## 7. Future Directions

*   Complete V4 debugging and validation.
*   Profile V3 and V4 using `Nsight Systems` / `Nsight Compute` to identify and address performance bottlenecks.
*   Implement and benchmark V5 (CUDA-Aware MPI) if cluster environment permits.
*   Investigate numerical differences between versions.
*   Explore implementing the alternative, potentially more complex logic in the unused `alexnetForwardPassMPI_CUDA` function in V4.
*   Consider performance optimizations (async overlap, kernel tuning) based on profiling.

---

---

## Content from: `final_project/RESEARCH.md`

# CS485 Final Project: Research, Analysis, and Findings

**Project:** AlexNet Inference (MPI+CUDA Staged Implementation - Blocks 1&2)
**Purpose:** This document captures research findings, critical analysis of the project plan, High-Performance Computing (HPC) best practices, and relevant context from academic literature and technical documentation pertaining to the CS485 final project. It serves as a rolling reference to inform design decisions and anticipate challenges throughout the V1-V5 implementation stages.

---

## Table of Contents

1.  [Executive Summary of Analysis](#1-executive-summary-of-analysis)
2.  [Pedagogical Evaluation of Staged (V1-V5) Approach](#2-pedagogical-evaluation-of-staged-v1-v5-approach)
    *   [Analysis of the Learning Curve](#21-analysis-of-the-learning-curve)
    *   [Common Student Challenges](#22-common-student-challenges)
    *   [Alignment with Course Objectives](#23-alignment-with-course-objectives)
3.  [Development Environment and Build System](#3-development-environment-and-build-system)
    *   [WSL2 vs. Native Linux (Fedora): Compatibility and Performance](#31-wsl2-vs-native-linux-fedora-compatibility-and-performance)
    *   [Build System: Makefiles and Bash Scripting](#32-build-system-makefiles-and-bash-scripting)
4.  [Analysis of MPI Parallelization Strategies (V2 Focus)](#4-analysis-of-mpi-parallelization-strategies-v2-focus)
    *   [Critique of "Broadcast All, Compute Slices" Approach (V2.1)](#41-critique-of-broadcast-all-compute-slices-approach-v21)
    *   [Analysis of Scatter+Halo Approach (V2.2)](#42-analysis-of-scatterhalo-approach-v22) <!-- Renamed from comparison -->
    *   [Sub-Version Implementation Philosophy](#43-sub-version-implementation-philosophy)
    *   [Correctness Pitfalls in MPI](#44-correctness-pitfalls-in-mpi)
5.  [Layer Parallelization: AlexNet Subset & Challenges (V3 Focus)](#5-layer-parallelization-alexnet-subset--challenges-v3-focus)
    *   [Representativeness of Early AlexNet Blocks](#51-representativeness-of-early-alexnet-blocks)
    *   [Inherent Difficulties in Layer Parallelization (MPI & CUDA)](#52-inherent-difficulties-in-layer-parallelization-mpi--cuda)
    *   [Critique of `std::vector<float>` for HPC/CUDA](#53-critique-of-stdvectorfloat-for-hpccuda)
6.  [CUDA Implementation and MPI+CUDA Integration (V3-V5)](#6-cuda-implementation-and-mpicuda-integration-v3-v5)
    *   [V3 (CUDA Kernels): Best Practices and Pitfalls](#61-v3-cuda-kernels-best-practices-and-pitfalls)
    *   [V4 (MPI+CUDA): Integration Challenges](#62-v4-mpicuda-integration-challenges)
    *   [V5 (CUDA-aware MPI): Requirements, Benefits, Pitfalls](#63-v5-cuda-aware-mpi-requirements-benefits-pitfalls)
7.  [HPC Software Engineering and Development Practices](#7-hpc-software-engineering-and-development-practices)
    *   [Code Organization and Modularity](#71-code-organization-and-modularity)
    *   [Robust Error Handling (MPI & CUDA)](#72-robust-error-handling-mpi--cuda)
    *   [Debugging Techniques per Stage](#73-debugging-techniques-per-stage)
    *   [Data Precision (Single vs. Double)](#74-data-precision-single-vs-double)
8.  [Performance Analysis, Bottlenecks, and Profiling](#8-performance-analysis-bottlenecks-and-profiling)
    *   [Likely Performance Bottlenecks per Stage](#81-likely-performance-bottlenecks-per-stage)
    *   [Standard Performance Metrics](#82-standard-performance-metrics)
    *   [Suitable Profiling Tools/Techniques](#83-suitable-profiling-toolstechniques)
9.  [Advanced Topics, Libraries Context, and Potential Gaps](#9-advanced-topics-libraries-context-and-potential-gaps)
    *   [Gaps in Current Plan (Async Overlap, Load Balancing, etc.)](#91-gaps-in-current-plan-async-overlap-load-balancing-etc)
    *   [Awareness of Relevant Libraries (cuDNN, NCCL, Thrust, etc.)](#92-awareness-of-relevant-libraries-cudnn-nccl-thrust-etc)
    *   [Validation and Numerical Correctness Concerns](#93-validation-and-numerical-correctness-concerns)
10. [Research-Based Recommendations for the Project](#10-research-based-recommendations-for-the-project)
11. [References](#11-references)
12. [Rolling Research Notes & Findings](#12-rolling-research-notes--findings)

---

## 1. Executive Summary of Analysis

The CS485 final project plan, implementing AlexNet inference via a five-stage parallelization (V1-V5), is pedagogically valuable for its incremental introduction to Serial, MPI, CUDA, and Hybrid MPI+CUDA programming. However, it presents significant technical and conceptual challenges. Key concerns identified through research and code review include:
*   **Learning Curve:** Steep difficulty increases between stages (V1->V2, V3->V4). V4 integration is particularly complex.
*   **V2 MPI Strategy:** The "Broadcast All" (V2.1) approach demonstrated poor scaling as expected. The implemented Scatter+Halo (V2.2) is more scalable but involved complex halo/trimming logic.
*   **Environment:** Developing in WSL2 introduces compatibility/performance risks compared to the target Fedora cluster. Observed V4 NP=4 crash could be environment-related.
*   **Data Structures:** `std::vector<float>` remains suboptimal for CUDA H<->D transfers; pinned memory is crucial for optimized V4/V5.
*   **V3 Performance:** The CUDA-only version is significantly slower than serial, pointing to H<->D overhead or kernel inefficiencies needing profiling.
*   **V4 Implementation:** The current host-staging approach (full padded tile H<->D, GPU computes full sequence, D->H, host trim) introduces multiple potential bottlenecks and complexity points. Debugging is currently required (output format, NP=4 crash).
*   **V5 Feasibility:** CUDA-aware MPI benefits depend heavily on specific cluster HW/SW support (GPU Direct RDMA) and may not eliminate all H<->D if host-side logic (like trimming) remains.
*   **Bottlenecks:** Performance limitations shift dramatically (CPU -> MPI Comm -> GPU Kernel/Transfers -> V4 Host Staging/Sync/Comm -> Network).
*   **Software Engineering:** Robust error handling (using `CUDA_CHECK` and MPI checks), modularity, and effective debugging/profiling are critical but challenging, especially in V4.
*   **Context:** Plan lacks emphasis on asynchronous overlap and standard libraries (cuDNN, NCCL). Numerical discrepancies across versions need attention.

Recommendations focus on standardizing the environment, prioritizing V4 debugging/profiling, mandating appropriate data structures by V3, verifying V5 viability, enforcing SE practices, analyzing numerical differences, and contextualizing manual efforts with industry libraries/techniques.

---

## 2. Pedagogical Evaluation of Staged (V1-V5) Approach

### 2.1 Analysis of the Learning Curve
*(Content largely unchanged, still relevant)*
The 5-stage approach offers a structured learning path:
*   **V1 (Serial):** Establishes functional correctness and a performance baseline. Reinforces algorithm understanding.
*   **V2 (MPI):** Introduces distributed memory concepts, explicit communication (`MPI_Bcast`, `MPI_Scatterv`, `MPI_Isend/Irecv`, `MPI_Gather`), halo exchange, and process management. A major conceptual shift.
*   **V3 (CUDA):** Introduces GPU architecture, SIMT execution, kernel programming, and host-device memory management. Another distinct, significant conceptual leap.
*   **V4 (MPI+CUDA):** The core integration challenge, demanding orchestration across nodes and GPUs, managing data movement between host (MPI) and device (CUDA) memory spaces (via host staging in current implementation), and synchronization. Often the steepest curve.
*   **V5 (CUDA-aware MPI):** Optimization layer potentially simplifying V4 communication code and improving performance via direct GPU communication. Requires specific HW/SW support; pedagogical value depends on demonstrable benefits vs V4.

**Overall:** The staging isolates concepts effectively, but instructors/students must anticipate difficulty spikes, especially V1->V2 and V3->V4. V4 debugging is currently a significant hurdle.

### 2.2 Common Student Challenges
*(Content largely unchanged, still relevant)*
*   **Conceptual:** Distinguishing distributed vs. shared memory models. Understanding MPI communicators, CUDA execution/memory hierarchies, synchronization primitives (`MPI_Barrier`, `MPI_Wait`, `cudaDeviceSynchronize`, Events, `__syncthreads`). Correctly mapping algorithm stages to parallel execution models.
*   **Implementation:** Debugging parallel/distributed non-deterministic code. Managing complex C++/MPI/CUDA builds (`nvcc -ccbin`). Correct API usage and error checking (`CUDA_CHECK`). Performance tuning (identifying/addressing bottlenecks like H<->D transfers). Correct halo exchange and data trimming logic.
*   **Integration (V4/V5):** Sequencing MPI calls and CUDA operations correctly. Ensuring host/device data consistency (especially with host staging in V4). Avoiding deadlocks. Managing GPU affinity (`cudaSetDevice`). Configuring/verifying CUDA-aware MPI (V5). Resolving runtime errors (like the V4 NP=4 crash).

### 2.3 Alignment with Course Objectives
*(Content largely unchanged, still relevant)*
This project aligns well with typical HPC course objectives:
*   Hands-on experience with parallelization concepts (data/task parallelism, speedup, efficiency).
*   MPI standard for distributed memory programming.
*   CUDA for GPU accelerator programming.
*   Hybrid MPI+CUDA models (specifically host-staging and potentially direct communication).
*   Performance analysis and profiling.
*   Application to a relevant domain (CNN inference).

---

## 3. Development Environment and Build System

### 3.1 WSL2 vs. Native Linux (Fedora): Compatibility and Performance
*(Content largely unchanged, warning remains highly relevant given V4 crash)*
Using WSL2 (Ubuntu) for development while targeting Fedora 37 presents risks:
*   **WSL2 Overview:** Runs a real Linux kernel via lightweight VM. Good compatibility but not identical to native Linux.
*   **Performance Discrepancies:** Filesystem I/O (Windows<->WSL2) can be slow. Virtualized networking may differ from native MPI performance. GPU compute usually close, but potential overheads exist.
*   **Compatibility Risks:** Differences in kernel versions, system libraries (glibc), CUDA drivers (WSL vs. native), and MPI implementation behavior (e.g., OpenMPI quirks [5], resource limits) can cause "works on my machine" issues. Consumer GPUs in WSL2 may lack features like GPUDirect RDMA [5]. The V4 NP=4 crash *could* be an environment-specific resource issue.
*   **Mitigation:** Minimize environment differences. Use containers (Docker/Singularity) in both dev/target environments. Perform frequent testing on the actual Fedora cluster. Match toolchain versions (GCC 12, CUDA 12.x, Open MPI) precisely.

**Conclusion:** Sole reliance on WSL2 is risky, especially for debugging runtime errors like the V4 crash. Prioritize testing on or containerizing the target environment.

### 3.2 Build System: Makefiles and Bash Scripting
*(Content updated for V4)*
*   **Suitability:** Makefiles + Bash are standard HPC tools. V1-V3 use basic Makefiles. V4 introduces a more complex Makefile integrating `nvcc -ccbin=mpicxx`, MPI flag detection (`--showme`), `bear` for `compile_commands.json`, and `clang-tidy` for linting. This is a good step towards better SE practices but adds complexity.
*   **Complexity Management:** Requires well-organized Makefiles (variables, pattern rules, dependencies). Robust Bash scripts (`run_final_project.sh`) with error checking are used for automation.
*   **Alternative (CMake):** Offers better cross-platform support, dependency finding (FindMPI, FindCUDA), build configurations, and test integration. Scales better for complex projects and is a valuable skill. Represents stronger software engineering practice, though with a slightly steeper initial learning curve than basic Makefiles.

---

## 4. Analysis of MPI Parallelization Strategies (V2 Focus)

### 4.1 Critique of "Broadcast All, Compute Slices" Approach (V2.1)
*(Content largely unchanged, validated by results)*
This strategy (`MPI_Bcast` full input/parameters, each rank computes full V1 sequence, `MPI_Gather`/`MPI_Gatherv` final slices) was implemented and tested.
*   **Scalability Issues:**
    *   *Communication Bottleneck:* `MPI_Bcast` cost scales poorly with process count (P) and data size. Gather also adds overhead. Performance degraded as P increased, as expected.
    *   *Memory Inefficiency:* Each rank stores full input/parameters, negating distributed memory benefits. Limited by single-node memory.
*   **Implementation Simplicity:** Main advantage. Ranks compute independently after broadcast.
*   **Pedagogical Value:** Introduces basic collectives but fails to teach scalable communication patterns. Serves as a useful negative example compared to V2.2.

### 4.2 Analysis of Scatter+Halo Approach (V2.2)
*(Content updated to reflect implementation)*
This strategy (Scatter input rows, exchange halos via `MPI_Isend/Irecv/Wait`, broadcast parameters, compute locally, trim results, Gather output) was implemented and demonstrated good scalability.
*   **Pros:** Reduced activation memory per rank. Good load balancing (with roughly equal row distribution). Neighbor communication more scalable than broadcast. Natural fit for convolution.
*   **Cons:** Higher implementation complexity (halo logic, non-blocking MPI, boundary handling, complex asymmetric trimming after pooling layers). Correctness is harder to achieve.
*   **Key Implementation Points:** Used non-blocking MPI for halo overlap potential. Required careful calculation of which rows constituted halos and which rows needed trimming after pooling layers based on the halo influence propagating through Conv/Pool stages.

### 4.3 Sub-Version Implementation Philosophy
*(Content unchanged)*
Recognize that multiple valid strategies exist for each stage.
1.  **Identify & Document:** Note different approaches in `README.md` or `RESEARCH.md`.
2.  **Implement Primary:** Implement one chosen strategy first within the main version folder (e.g., V2.2 in `v2_mpi_only/2.2_scatter_halo/`).
3.  **Explore Alternatives (Optional):** Implement alternatives in sub-folders (e.g., V2.1) or via Git branches for comparison.

### 4.4 Correctness Pitfalls in MPI
*(Content unchanged, highly relevant to V4 debugging)*
*   **Collectives:** Ensuring all ranks participate, correct counts/displacements/datatypes (`MPI_Scatterv`, `MPI_Gatherv`), correct reduction operations (if used). Off-by-one errors in slicing/distribution.
*   **Halo Exchange (V2.2/V4):** Incorrect neighbor ranks, mismatched tags/counts, buffer errors (size, pointers), deadlocks (blocking send/recv order, insufficient buffer space), race conditions (non-blocking), boundary conditions (rank 0, rank size-1).
*   **Data Types:** `MPI_Datatype` must match C++ memory layout precisely (`MPI_FLOAT` vs `float`).
*   **Buffer Management:** Ensuring send buffers aren't modified before non-blocking sends complete (`MPI_Wait`). Ensuring receive buffers are large enough.

---

## 5. Layer Parallelization: AlexNet Subset & Challenges (V3 Focus)

### 5.1 Representativeness of Early AlexNet Blocks
*(Content unchanged)*
*   **Layers Covered:** Conv1, ReLU1, Pool1, Conv2, ReLU2, Pool2, LRN2.
*   **Characteristics:** Covers compute-intensive convolution, simple element-wise ReLU, local reduction pooling, and more complex cross-channel LRN. Representative of fundamental CNN operations.
*   **Limitations:** Omits deeper Conv layers (smaller spatial size, more channels) and Fully Connected layers (matrix multiplies, different bottlenecks). May give skewed perspective on optimal strategies for the whole network. Early layers have large spatial dimensions suitable for spatial decomposition; later layers might favor filter decomposition.

### 5.2 Inherent Difficulties in Layer Parallelization (MPI & CUDA)
*(Content unchanged)*
*   **MPI:**
    *   *Convolution:* Data distribution needs halo exchange (if spatial decomp.) or input replication (if filter decomp.). Load balancing.
    *   *Pooling:* Simpler, but needs halos if spatial decomp. and window crosses boundary.
    *   *LRN:* Cross-channel/spatial dependencies make efficient distribution hard.
*   **CUDA:**
    *   *Convolution:* Efficient mapping of loops to threads/blocks/grids. Optimizing memory access (coalescing, shared memory tiling for reuse). Algorithms (im2col, Winograd, direct).
    *   *Pooling:* Simpler kernel, boundary checks, avoid shared memory bank conflicts if used.
    *   *LRN:* Efficient access to neighbors (spatial/channel). Shared memory useful but needs careful indexing.

### 5.3 Critique of `std::vector<float>` for HPC/CUDA
*(Content updated - V4 still uses vector on host)*
Using `std::vector<float>` on the host (as done in V1-V4) for data that needs to be transferred to/from the GPU presents performance issues:
*   **Memory Allocation:** Allocates standard pageable host memory.
*   **Host-Device Transfer:** `cudaMemcpy` is significantly slower with pageable memory compared to pinned (page-locked) host memory (allocated via `cudaMallocHost`). Pinned memory enables asynchronous transfers (`cudaMemcpyAsync`) essential for overlapping communication/computation (a potential optimization for V4/V5). The current V4 uses synchronous `cudaMemcpy` with pageable `std::vector` buffers, maximizing transfer latency. [9]
*   **Alignment:** Default alignment may not be optimal for SIMD/GPU coalescing.
*   **Recommendation:** Transition to pinned memory strategies (`cudaMallocHost`) for performance-critical H<->D transfer buffers, especially if attempting asynchronous overlap in V4/V5 optimizations. Raw `float*` with `cudaMallocHost`/`cudaFreeHost` or Thrust vectors are alternatives.

---

## 6. CUDA Implementation and MPI+CUDA Integration (V3-V5)

### 6.1 V3 (CUDA Kernels): Best Practices and Pitfalls
*(Content unchanged - analysis still applies to V3/V4 kernels)*
*   **Best Practices:** Map computation to CUDA hierarchy. Minimize H<->D transfers. Use pinned host memory (not done yet). Optimize global memory access (coalescing). Use shared memory for data reuse (beware bank conflicts). Maximize arithmetic intensity. Avoid warp divergence. Use CUDA streams for overlap (not done yet). Profile with Nsight Compute/Systems. Rigorous error checking (`CUDA_CHECK`).
*   **Pitfalls:** Uncoalesced access. Global memory bottleneck. Shared memory bank conflicts. Low occupancy. Thread divergence. Forgetting error checks. Underestimating transfer overhead (likely cause of V3 slowness). Kernel indexing errors.

### 6.2 V4 (MPI+CUDA): Integration Challenges
*(Content updated based on V4 code)*
*   **Host/Device Data Management (Current V4 Strategy):**
    1.  MPI Scatter to host `std::vector`.
    2.  MPI Halo exchange uses host buffers.
    3.  *Full padded* local slice copied H->D (`cudaMemcpy` from `std::vector` to `cudaMalloc`'d buffer). **Bottleneck 1: Large, synchronous copy.**
    4.  Entire layer sequence (Conv1..LRN2) computed on GPU tile via `alexnetTileForwardCUDA`. This helper likely allocates/copies weights/biases internally on each call. **Potential Bottleneck 2: Redundant internal copies/allocs?**
    5.  Final result tile copied D->H (`cudaMemcpy` to `std::vector`). **Bottleneck 3: Large, synchronous copy.**
    6.  Result trimmed on *host*. **Bottleneck 4: CPU work after GPU, potential serialization.**
    7.  MPI Gather from host `std::vector`.
    This "host staging" pattern is simple conceptually but adds significant latency and PCIe load compared to potentially overlapping or direct communication approaches.
*   **Synchronization:** Current code appears largely synchronous (blocking MPI halo waits, synchronous `cudaMemcpy`, likely implicit sync after kernels before D->H copy). Need to ensure `MPI_Waitall` for halos completes before H->D copy, and GPU work completes (`cudaDeviceSynchronize` likely needed before D->H copy, although `cudaMemcpy` D->H implies sync) before host trimming/gather. Incorrect sync leads to race conditions or deadlocks.
*   **GPU Affinity:** Implemented via `cudaSetDevice(rank % nDev)`. Seems correct but relies on linear rank-to-GPU mapping.
*   **Resource Management:** The NP=4 crash suggests potential issues with memory allocation (host or device limits exceeded with more processes) or MPI resource exhaustion (e.g., message queues, unexpected blocking). Needs debugging.
*   **Complexity:** Managing indices, buffer sizes, padding/trimming logic correctly across MPI ranks and host/device boundaries is error-prone.

### 6.3 V5 (CUDA-aware MPI): Requirements, Benefits, Pitfalls
*(Content updated based on V4)*
*   **Requirements:** MPI library built with CUDA support (OpenMPI `--with-cuda`, MVAPICH2-GDR, etc.). Compatible CUDA Toolkit/drivers. HW support for GPU Direct RDMA (specific GPUs/NICs/fabric) for optimal performance. Correct system/MPI environment configuration (e.g., UCX settings [4, 5]).
*   **Benefits:** Pass GPU device pointers directly to MPI calls (`MPI_Scatterv`, `MPI_Isend`, `Irecv`, `Gatherv`). Library handles transfer, potentially via RDMA bypassing host memory. Reduces latency, PCIe traffic, CPU overhead *for the communication parts*. Simplifies *application* code by removing manual H<->D `cudaMemcpy` calls *for MPI staging*.
*   **Pitfalls:** Complex configuration/compatibility issues. Performance gain not guaranteed (may fallback to internal staging if RDMA unsupported). Harder debugging. Synchronization nuances still apply. Strong dependency on specific cluster environment. [5] **Note:** This would primarily optimize the halo exchange and Scatter/Gather steps in V4. It might not remove the need for D<->D copies or separate kernels if complex on-GPU padding/trimming logic isn't implemented. If trimming remains host-based, a final D->H copy is still needed.
*   **Table 2: MPI+CUDA Data Movement Strategies**
    | Strategy              | Description                                                              | Key Steps (Send Halo Example)                  | Pros                                   | Cons                                                 | Relevant Stage |
    | :-------------------- | :----------------------------------------------------------------------- | :--------------------------------------------- | :------------------------------------- | :--------------------------------------------------- | :------------- |
    | Manual Staging (V4)   | Explicit H<->D copies around MPI calls acting on host buffers            | `cudaMemcpy D2H(halo)` -> `MPI_Send(host_halo)` | Works anywhere, Explicit control       | High latency, PCIe load, More app code, Sync overhead | V4 (Current)   |
    | CUDA-aware (V5 Ideal) | Pass GPU ptr to MPI; Library uses optimized path (RDMA) for comms        | `MPI_Send(gpu_halo_ptr)`                       | Low latency(comms), Less PCIe(comms), Simpler comm code | Requires HW/SW support, Complex config/debug         | V5 (if works)  |
    | CUDA-aware (V5 Fback) | Pass GPU ptr; Library falls back to internal staging (mimics V4 staging) | `MPI_Send(gpu_halo_ptr)` -> Lib does copies    | Simpler app code (comms)               | Perf similar/worse than V4, Hides data path         | V5 (fallback)  |

---

## 7. HPC Software Engineering and Development Practices
*(Content updated for V4)*
*   **Code Organization:** V4 uses separate `.cpp`/`.cu` files but the `alexnetTileForwardCUDA` function encapsulates the entire GPU sequence, reducing modularity at the layer level within the hybrid context. Error handling uses `CUDA_CHECK` macro with `MPI_Abort`.
*   **Error Handling:** Essential for parallel debugging. Check return codes for *all* MPI and CUDA calls. Use `MPI_Error_string`, `cudaGetErrorString`. Use rank ID in MPI error messages. Use robust `CUDA_CHECK` macro that includes rank and aborts cleanly.
*   **Debugging Techniques:**
    *   *V1:* GDB, Valgrind.
    *   *V2:* Rank-based printf, parallel debuggers (multi-process GDB, TotalView, DDT), MPI analysis tools. Focus on communication issues (deadlocks, tags, sizes).
    *   *V3:* Kernel printf (use sparingly), `cuda-gdb`, `cuda-memcheck`/`compute-sanitizer`, strategic `cudaDeviceSynchronize`.
    *   *V4/V5:* Combine techniques. Use rank-based `fprintf(stderr,...)` for diagnostics. Parallel debuggers with CUDA awareness (TotalView, DDT) are ideal. Isolate issues (1 process vs multi-process vs multi-node). Use `cuda-memcheck mpirun ...` for NP=4 crash. Check MPI resource usage.
*   **Data Precision:** Use `float` (single-precision) for deep learning inference. GPUs are optimized for it [3]. `double` uses 2x memory/bandwidth and is much slower computationally. Ensure consistent use of `float` and `MPI_FLOAT`.

---

## 8. Performance Analysis, Bottlenecks, and Profiling

*   **Likely Bottlenecks per Stage:**
    *   *V1:* CPU compute, Host memory bandwidth.
    *   *V2:* MPI Comm (`Bcast`/`Gather`/Halo Exchange), Network B/W & Latency, Memory per node, Load imbalance, CPU compute.
    *   *V3:* GPU Kernel (compute/memory bound), **PCIe transfer time (H<->D)**, Kernel launch overhead.
    *   *V4 (Current):* **PCIe H<->D transfer of full padded tiles**, **Host staging/trimming CPU work**, MPI Halo Comm (Host), Sync overhead, Load imbalance, *Potentially internal inefficiencies in `alexnetTileForwardCUDA`*.
    *   *V5:* Network B/W & Latency (MPI comms potentially optimized), GPU Kernel, PCIe (if internal staging occurs or host logic remains), Sync overhead, Library efficiency, Load imbalance, Amdahl's Law.
*   **Standard Performance Metrics:** Wall Clock Time, Time Breakdowns (Compute vs Comm vs H2D/D2H vs Sync), Speedup (S(N) = T1/TN), Efficiency (E(N) = S(N)/N), Scalability (Strong/Weak), Communication Volume/Bandwidth, Kernel Execution Time, GPU Utilization/Occupancy/Memory Throughput.
*   **Suitable Profiling Tools/Techniques:**
    *   *Manual Timers:* `MPI_Wtime()`, CUDA Events (`cudaEventElapsedTime`). Crucial for V4 breakdown.
    *   *CPU Profilers:* `gprof`, `perf` (for host-side V4 logic).
    *   *MPI Profilers:* `mpiP`, Score-P, Vampir, TAU (analyze V2/V4 MPI phases).
    *   *CUDA Profilers:* Nsight Systems (`nsys` - system view, CPU/GPU/API timeline, **essential for V3/V4**), Nsight Compute (`ncu` - deep kernel analysis).
    *   *Combined:* Score-P, Vampir, TAU often handle both MPI+CUDA for unified view.
*   **Analysis Focus:** Decompose total time to identify true bottlenecks (esp. H<->D vs Compute vs Comm in V4). Understand how bottlenecks shift between stages. Compare against theoretical limits. Analyze numerical discrepancies.
*   **Table 3: Performance Metrics and Profiling Tools per Project Stage**
    | Stage            | Likely Bottlenecks                                                    | Key Metrics                                                    | Recommended Tools                                            |
    | :--------------- | :-------------------------------------------------------------------- | :------------------------------------------------------------- | :----------------------------------------------------------- |
    | V1 (Serial)      | CPU compute, Host memory                                              | Wall time, CPU cycles, Cache misses                            | `gprof`, `perf`, Manual timers                               |
    | V2 (MPI)         | MPI Comm (Halo/Bcast/Gather), Network, Mem/Node, CPU compute          | Wall time, Speedup/Efficiency, Time breakdown, MPI stats       | Manual `MPI_Wtime`, MPI Profilers (Score-P, Vampir, TAU)     |
    | V3 (CUDA)        | **PCIe transfer**, GPU kernel, Launch overhead                        | Wall time, Speedup(vsV1), Kernel/H2D/D2H times, GPU util/mem BW | Manual CUDA Events, Nsight Systems (`nsys`), Nsight Compute (`ncu`) |
    | V4 (MPI+CUDA)    | **PCIe H<->D (Tiles)**, MPI Comm (Host Halo), Host Trim, Sync, GPU Kernel | Wall time, Speedup/Eff(vsV1/V3), Breakdown, Net BW, Wait times | **Nsight Systems (`nsys`)**, Combined Profilers, Manual Timers, Debuggers (for crash) |
    | V5 (CUDA-aware)  | Network (Direct MPI), GPU Kernel, PCIe (if fallback/host logic), Lib eff | Wall time, Speedup/Eff(vsV4), Breakdown, Net BW (direct)      | Combined Profilers, `nsys`, Manual Timers, V4 Comparison      |

---

## 9. Advanced Topics, Libraries Context, and Potential Gaps

*   **Gaps in Current Plan:**
    *   *Async Operations/Overlap:* Critical technique (non-blocking MPI + CUDA streams + pinned memory) not implemented. Could significantly hide V4 latency.
    *   *Load Balancing:* Assumed balanced work; real scenarios often require balancing.
    *   *Advanced MPI:* Derived Datatypes, One-Sided Comm (RMA), advanced communicators not covered.
    *   *Topology Awareness:* Performance sensitive to network topology & process mapping, not addressed.
    *   *Alternative Paradigms:* OpenMP, Task-based (Legion), PGAS languages provide context.
*   **Awareness of Relevant Libraries:** Essential context for the manual implementation effort.
    *   *cuDNN:* NVIDIA's optimized library for DNN primitives (Conv, Pool, etc.). Performance benchmark. [8] Would replace manual kernels.
    *   *NCCL:* NVIDIA's optimized library for multi-GPU/multi-node collectives (Allreduce, Bcast). Standard for DL training. [7] Could replace manual MPI collectives if needed.
    *   *Thrust:* High-level C++ template library for CUDA (parallel algorithms, `device_vector`, pinned memory allocators). Could simplify memory management.
    *   *Vendor MPIs:* May offer better performance/CUDA integration than standard OpenMPI/MPICH on specific clusters.
*   **Validation and Numerical Correctness:** Need strategy to compare outputs between versions within a tolerance (epsilon) due to floating-point non-associativity. **Observed differences need investigation.**

---

## 10. Research-Based Recommendations for the Project

1.  **Standardize Environment:** Use target cluster or exact-replica containers (Docker/Singularity) for all testing/development, especially V4 debugging.
2.  **Prioritize V4 Debugging:** Fix output format, resolve NP=4 crash, **verify numerical correctness and host trimming logic.**
3.  **Profile V3 & V4:** Use `Nsight Systems` to understand V3 slowness and pinpoint V4 bottlenecks (H<->D vs Compute vs Host Comm vs Host Trim). Use `Nsight Compute` for kernel optimization if needed.
4.  **Mandate Pinned Memory:** Require use of `cudaMallocHost` or equivalents for H<->D transfer buffers if performance optimization (e.g., async overlap) is attempted later.
5.  **Support V4/V5 Integration & Analysis:** Provide guidance/templates for H<->D data flow, MPI/CUDA sync. **Verify V5 feasibility/benefit on target cluster before implementation.** Analyze the performance impact of the current V4 host-staging/tile approach vs alternatives.
6.  **Enforce SE Practices:** Mandate modularity, comprehensive error checking (`CUDA_CHECK` w/ `MPI_Abort`). Provide debugger/profiler access & training. Encourage use of version control.
7.  **Require Performance Analysis:** Mandate timing breakdowns, speedup/efficiency calculation, bottleneck analysis, and scalability discussion for V2-V4 (once working).
8.  **Address Numerical Discrepancies:** Investigate and explain (or fix) the observed differences in numerical output between versions.
9.  **Provide Context:** Discuss async overlap conceptually. Introduce cuDNN/NCCL as industry standards and performance references.

---

## 11. References
*(List unchanged)*
1.  Lawrence Mitchell, *“MPI: Domain decomposition and halo exchanges”*, Durham Univ. HPC Course ([Link](https://teaching.wence.uk/phys52015/exercises/mpi-stencil/))
2.  Wikipedia – *“Data parallelism vs. Model parallelism”* ([Link](https://en.wikipedia.org/wiki/Data_parallelism))
3.  NVIDIA Developer Blog – *“Defining Floating Point Precision (FP64, FP32, FP16)”*, Exxact Corp. (2024) ([Link](https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16))
4.  OpenUCX Documentation / CISL Tutorial Slides – Recommended environment settings for CUDA-aware MPI (UCX transport). ([Example Slide Link](https://www.cisl.ucar.edu/sites/default/files/2022-09/11_MultiGPU_Part2.slides%20%282%29.pdf))
5.  NVIDIA Forums – discussion *“Windows 11 + WSL + CUDA-aware MPI”* ([Link](https://forums.developer.nvidia.com/t/windows-11-wsl-cuda-aware-mpi-geforce-40-series-seg-fault-but-with-geforce-30-series-ok/292425))
6.  Alex Krizhevsky et al., *“ImageNet Classification with Deep CNNs (AlexNet)”*, NIPS 2012 ([Paper Link - Often Found Online](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf))
7.  NVIDIA Developer – *“NCCL (Nvidia Collective Communication Library)”* ([Link](https://developer.nvidia.com/nccl))
8.  NVIDIA Documentation – *“NVIDIA cuDNN”* ([Link](https://docs.nvidia.com/deeplearning/cudnn/latest/))
9.  Stack Overflow – *“std::vector<T> contiguous memory”* ([Example Link](https://stackoverflow.com/questions/2009531/c-stdpair-stdvector-memcopy))
10. SCM Documentation – *“Known Issues: Intel MPI on WSL”* ([Link](https://www.scm.com/doc/Installation/Additional_Information_and_Known_Issues.html))

---

## 12. Rolling Research Notes & Findings

*(**Instructions:** Manually add your own specific findings, benchmark results, interesting articles, or unexpected behaviors encountered during development here.)*

*   **[Current Date]:** Confirmed V4 implements host-staging with full padded tile H<->D copy and GPU execution of entire sequence via `alexnetTileForwardCUDA`. Host trimming logic uses simple offsets.
*   **[Current Date]:** V4 output format mismatch identified as cause for parsing errors in `run_final_project.sh`.
*   **[Current Date]:** V4 NP=4 crash occurs (Exit 134), requires debugging via log file inspection, debuggers (`gdb`/`cuda-gdb`), or `cuda-memcheck`. Could be resource limit, memory error, or MPI bug.
*   **[Date]:** Note about specific MPI behavior observed on cluster vs WSL2...
*   **[Date]:** Benchmark results for V2.1 Broadcast All show communication time dominates beyond P=4...
*   **[Date]:** Found issue with LRN layer indexing in serial code, fixed in V1...
*   ... *(Add more entries as you progress)* ...

---

---

## Content from: `final_project/DISCUSSION.md`

# CS485 Final Project: Discussion Log & Professor Meeting Prep

**Project:** AlexNet Inference (MPI+CUDA Staged Implementation) - Blocks 1 & 2 Focus
**Student:** Mykhaylo Kopylov
**Date Started:** [Date file created]
**Last Updated:** [Current Date]

**Purpose:** This document serves as a rolling log to prepare for discussions with Professor Sohn regarding the CS485 final project. The top section provides a narrative script for a meeting update, followed by detailed sections covering status, challenges, questions, and next steps. Meeting summaries will be manually added at the bottom after each discussion.

---

## Professor Meeting Opening Script/Narrative (Approx. 5-10 min verbal walkthrough)

"Hi Professor, thanks for meeting. I wanted to give you an update on the CS485 final project – the staged MPI+CUDA implementation of the first two blocks of AlexNet inference.

Just to recap, the main goal is to implement and benchmark these initial layers (Conv1 through LRN2) across five stages: starting with a basic serial version, then adding MPI parallelism on the CPU, then a CUDA-only version on the GPU, followed by a hybrid MPI+CUDA version, and finally an optional optimization using CUDA-aware MPI. The focus is really on learning the parallelization techniques and analyzing the performance trade-offs at each stage using this consistent workload.

So far, the first three versions are functionally complete.

*   **V1 (Serial)** is working correctly and gives us a baseline performance measurement. On my development machine, the test script clocks it around 600-700 milliseconds.
*   For **V2 (MPI Only)**, I actually implemented two different approaches as planned in the `RESEARCH.md` to see the contrast.
    *   `V2.1` used a simple 'broadcast all' strategy. As expected, it ran correctly but showed negative scaling – it got slower as we added more processes, likely due to the broadcast overhead and redundant computations.
    *   `V2.2` used a more standard 'scatter with halo exchange' approach, using non-blocking MPI calls. This was more complex to implement, especially handling the halo padding and the necessary trimming after the pooling layers, but it showed good strong scaling, getting significantly faster with 2 and 4 processes compared to V1 or V2.1. This confirms the value of distributing the data properly.
*   **V3 (CUDA Only)** ports the layer logic to CUDA kernels running on a single GPU. This version also runs to completion. However, the performance is currently quite slow – significantly slower than the V1 serial version, actually. This strongly suggests there are bottlenecks, likely either in the host-to-device data transfers or perhaps the kernels themselves aren't very optimized yet. This is something that definitely needs profiling with Nsight tools.
*   An important observation across V1, V2, and V3 is that the sample output values reported by the test script seem to differ quite a bit between the versions, and even between different process counts in V2.2. This is something I need to investigate further – it could be minor floating-point differences, or potentially initialization variations or even subtle bugs.

Now, that brings us to the current focus: **Version 4 (MPI + CUDA Hybrid)**.

*   The goal here is to combine the scalable MPI structure from V2.2 (scatter, host halo exchange, gather) with the GPU computation from V3.
*   I've implemented a version of V4 based on a host-staging strategy. The overall flow is: MPI scatters the input rows to host buffers on each rank, then the ranks perform the first halo exchange using MPI on these host buffers. The resulting padded data tile (local rows plus halos) is then copied from the host to the GPU. A helper function then takes over on the GPU and runs the *entire* sequence of layers – Conv1, ReLU1, Pool1, Conv2, ReLU2, Pool2, LRN2 – all on that single tile. The final result tile is copied back from the GPU to the host, and then the rows corresponding to the halo contribution are trimmed off on the host before the final MPI gather. Each rank also sets its GPU using `cudaSetDevice`.
*   **The status is that the V4 code is implemented, but it's currently in the debugging phase.** The automated test script (`run_final_project.sh`) shows it runs successfully for 1 and 2 processes, but it flags warnings because the output format (`shape...` / `sample...`) in my code doesn't exactly match what the script is trying to parse (`Final Output Shape...` / `Final Output (first 10 values)...`). That's a straightforward fix I need to make in the `cout` statements.
*   More importantly, **when run with 4 processes (NP=4), V4 crashes** with an MPI error (exit code 134). I need to dig into the log file for that run and likely use debuggers or `cuda-memcheck` to figure out if it's a resource limit issue, a memory access error, or perhaps a subtle bug in the MPI communication logic (like buffer sizes or offsets) that only manifests at NP=4.
*   Finally, once it's running correctly, I also need to **verify the numerical output** against V1/V3 and double-check that the host-side trimming logic is correctly removing the halo influence for all ranks and process counts.

So, the **immediate next steps** are squarely focused on debugging V4: fixing the output format, resolving the NP=4 crash, and verifying the correctness. Once V4 is stable and working correctly, the plan is to profile it and V3 properly, analyze the performance, and then evaluate the feasibility and potential benefit of implementing V5 (CUDA-aware MPI) based on the cluster's capabilities.

With that overview, I can show you the output from the `run_final_project.sh` script which summarizes the status of V1 through V4, including the warnings and the crash indication for V4..."

**(End of Script - Transition to showing results/demo/addressing specific questions)**

---
---

## 1. Project Overview & Goal Reminder

*   **Objective:** Implement and benchmark inference for **AlexNet Blocks 1 & 2** using a 5-stage approach: V1 (Serial), V2 (MPI CPU), V3 (CUDA GPU), V4 (MPI+CUDA), V5 (CUDA-Aware MPI). The primary goal is learning parallelization techniques and performance analysis on this subset.
*   **Target Environment:** Fedora 37, GCC 12, CUDA 12.x, Open MPI.
*   **Deliverable:** Working code for completed versions (up to V5 ideally), focusing on a comparative performance analysis across the stages. Presentation will highlight the parallelization journey.

---

## 2. Current Status & Immediate Next Step

*   **Last Completed Stages:**
    *   V1 (Serial CPU) - **Completed & Validated.**
    *   V2 (MPI Only - Approach 2.1: Broadcast All) - **Completed.** Demonstrated poor scaling.
    *   V2 (MPI Only - Approach 2.2: Scatter+Halo) - **Completed.** Demonstrated expected speedup.
    *   V3 (CUDA Only - Single GPU) - **Completed.** Functional, needs profiling/optimization; numerical output differs from V1/V2.
*   **Current Focus:** **Version 4 (MPI + CUDA Hybrid) - Implemented, Currently Debugging.**
*   **Working Directory:** `final_project/v4_mpi_cuda/`
*   **Immediate Goal:** **Debug V4 implementation:**
    1.  Fix output format in `main_mpi_cuda.cpp` to match test script expectations.
    2.  Diagnose and resolve the runtime crash occurring with NP=4 (Exit Code 134).
    3.  Verify numerical correctness of V4 output and the host-side trimming logic against V1/V3 results.

---

## 3. Recent Accomplishments / Milestones Achieved

*   Completed V1 (Serial) implementation and established baseline performance (~667ms via script).
*   Implemented V2 MPI using two distinct strategies:
    *   V2.1 (Broadcast All): Validated basic MPI, highlighted scalability issues (~679ms -> ~881ms for np=1->4).
    *   V2.2 (Scatter+Halo): Successfully implemented halo exchange logic using non-blocking MPI, demonstrating expected speedup (~561ms -> ~281ms for np=1->4).
*   Completed V3 (CUDA Only) implementation: Ported layer logic to basic CUDA kernels, functional on single GPU (~2349ms, significantly slower than V1, needs profiling). Noted numerical differences.
*   **Implemented V4 (MPI+CUDA Host-Staging approach)** using V2.2 MPI structure and V3 CUDA kernels via `alexnetTileForwardCUDA` helper (currently requires debugging).
*   Established robust project structure with versioned directories and comprehensive documentation (`README.md`, `RESEARCH.md`, `AI.md`, `discussion.md`). V4 includes `bear`/`clang-tidy` integration.

---

## 4. Current Challenges / Roadblocks / Issues

*   **Specific V4 Debugging Needs:**
    *   **Output Format Mismatch:** `cout` statements in `main_mpi_cuda.cpp` differ from `run_final_project.sh` parser expectations (`shape`/`sample` vs `Final Output Shape`/`Final Output (first 10 values)`), preventing automated results capture. (Requires simple code change).
    *   **Runtime Crash (NP=4):** V4 fails with MPI Exit Code 134 when run with 4 processes. Requires investigation using logs (`logs/final_project_v4_np4.log`), debuggers (`gdb`, `cuda-gdb`), or memory checkers (`cuda-memcheck`). Could be resource limits, memory error (host/device), or MPI bug (e.g., incorrect buffer size/offset calculation in comms at NP=4).
    *   **Correctness Verification:** Need to confirm numerical output of V4 matches V1/V3 (within tolerance). The host-side trimming logic applied after the D->H copy needs careful validation for all ranks and NP values.
*   **V3 Performance:** V3 (CUDA Only) is much slower than V1 (Serial), indicating likely bottlenecks in H<->D transfers or kernel execution that need profiling with Nsight tools.
*   **Numerical Differences:** Sample outputs vary significantly across V1, V2.1, V2.2(NP>1), and V3. Needs investigation (initialization differences? floating-point non-associativity? bugs?).
*   **MPI+CUDA Integration Complexity:** Even with V4 implemented, ensuring correct synchronization, data flow (esp. padding/trimming), and resource management between MPI and CUDA remains inherently complex. The current V4 strategy (full tile H<->D, internal GPU sequence, host trim) may have performance implications to analyze later.

---

## 5. Specific Questions for Professor Sohn

*(**Instructions:** Review/update before meeting)*

1.  **V4 Debugging Guidance:** Regarding the V4 NP=4 crash (Exit 134), are there common MPI resource limits or configuration issues on the target cluster we should check first? Any advice on effectively debugging hybrid MPI+CUDA code, particularly potential interactions leading to such errors? Also, any tips for validating the host-side output trimming logic?
2.  **V4 Performance Expectation:** Once V4 is working, considering the current host-staging overhead (full tile H<->D copies), is it still expected that it might perform *worse* than V3 (CUDA only) or even V2.2 (MPI only) for a small number of processes/nodes? How should we interpret such results if they occur?
3.  **Asynchronous Operations (Overlap):** *After* V4 is debugged and profiled, would attempting asynchronous overlap (CUDA streams with `cudaMemcpyAsync` on pinned memory, non-blocking MPI) be a valuable optimization step for this project, or is mastering the synchronous host-staging approach sufficient?
4.  **V5 Feasibility Check:** What steps should we take to verify if the target cluster's Open MPI installation is truly CUDA-aware and supports efficient GPU Direct RDMA? Are there specific environment variables (e.g., UCX) or test programs recommended?
5.  **Profiling Hybrid Code:** What's the recommended approach for profiling V4/V5 on the cluster? Can Nsight Systems effectively capture both MPI wait times and CUDA kernel/memcpy timelines, or should we primarily rely on manual `MPI_Wtime` and CUDA Event timers inserted into the code?
6.  **Presentation Scope Confirmation:** Reconfirming that the primary presentation deliverable is the comparative analysis of V1-V4/V5 performance for Blocks 1&2, including the debugging journey, rather than a fully implemented AlexNet.
7.  **Numerical Differences:** We've observed different sample output values across V1, V2 (different strategies/NPs), and V3. How should we approach investigating these? Is minor variation expected due to floating-point math, or does it likely indicate bugs or initialization differences?

---

## 6. Proposed Next Steps / Plan

*(**Instructions:** Outline plan until next meeting)*

1.  **Debug V4 (`v4_mpi_cuda/`):**
    *   **Fix Output Format:** Modify `cout` lines in `main_mpi_cuda.cpp` to match `run_final_project.sh` expectations. Re-run script to confirm parsing success for NP=1, 2.
    *   **Investigate NP=4 Crash:** Analyze log file (`logs/final_project_v4_np4.log`). Run manually with debuggers/memcheck as needed to identify root cause (memory, MPI comms, resource limits?). Implement fix.
    *   **Verify Correctness:** Compare V4 numerical output (NP=1, 2, 4 once working) against V1/V3 reference. Review and test the host-side trimming logic for correctness across different ranks/NPs.
2.  **Profile V3:** Use Nsight Systems/Compute to understand why V3 is slow compared to V1. Identify kernel vs. H<->D bottlenecks.
3.  **Initial V4 Performance Measurement:** Once V4 is working correctly for NP=1, 2, 4, record wall times using `MPI_Wtime` and/or CUDA Events. Compare rough timing with V2.2 and V3.
4.  **Profile V4:** Use Nsight Systems to analyze the execution flow and identify key bottlenecks (H<->D copies, host halo exchange, host trimming, GPU compute time).
5.  **Plan V5 / Optimizations:** Based on V4 debugging/profiling experience and cluster verification (Question 4), decide on feasibility/approach for V5 (CUDA-Aware MPI) or other optimizations (e.g., async overlap).

---

## 7. Design Decisions & Considerations

*(**Instructions:** Note significant choices made and alternatives considered)*

*   **V2 Strategy Choice:** Implemented both V2.1 (Broadcast) and V2.2 (Scatter+Halo) to directly compare simple vs. scalable MPI approaches. V2.2 provides the better foundation for V4.
*   **V3 Kernels:** Implemented basic, functionally correct CUDA kernels. Performance optimization deferred. Slowness relative to V1 is noted.
*   **V4 Implementation Choice:** Current V4 uses a host-staging approach where the entire padded tile is copied H<->D, and a helper function (`alexnetTileForwardCUDA`) runs the full GPU sequence internally. Trimming is done on the host after D->H copy. This simplifies the main MPI loop but might introduce H<->D bottlenecks and hides layer-level GPU details within the helper. An alternative function (`alexnetForwardPassMPI_CUDA`) exists but is currently unused.
*   **Data Structures:** Using `std::vector` on host, raw `float*` (`cudaMalloc`) on device for V3/V4. Pinned host memory (`cudaMallocHost`) should be considered for V4/V5 staging buffers if async copies are attempted.

---

## 8. Performance & Benchmarking (WSL2 Dev Machine - Script Output)

| Version                | Procs | Shape     | Time       | Status | Notes                                      |
| :--------------------- | :---- | :-------- | :--------- | :----- | :----------------------------------------- |
| V1 Serial              | 1     | 13x13x256 | ~667 ms    | ✔      | Baseline                                   |
| V2 2.1-broadcast-all | 4     | 13x13x256 | ~881 ms    | ✔      | Degrades (Expected)                        |
| V2 2.2-scatter-halo  | 4     | 13x13x256 | ~281 ms    | ✔      | Scales Well (Expected)                     |
| V3 CUDA                | 1     | 13x13x256 | ~2349 ms   | ✔      | Slow (Needs Profiling)                     |
| V4 MPI+CUDA          | 1     | –         | –          | ⚠      | Runs; Output format issue                  |
| V4 MPI+CUDA          | 2     | –         | –          | ⚠      | Runs; Output format issue                  |
| V4 MPI+CUDA          | 4     | –         | –          | ⚠      | CRASH (Exit 134)                           |

*(Note: Absolute times will differ on target cluster. Relative scaling and bottlenecks are key points of analysis. V4 status reflects known issues.)*

---
---

## Meeting Summaries & Action Items

*(**Instructions:** Manually add notes after each meeting with the professor)*

**YYYY-MM-DD - Discussion with Prof. Sohn**
*   **Topics Discussed:** ...
*   **Key Feedback / Decisions:** ...
*   **Action Items:** ...

---

---

*End of combined document.*
