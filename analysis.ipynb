{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "# Cell 1: Setup and Imports\n",
    "# Python Standard Library\n",
    "from __future__ import annotations\n",
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Third-party Libraries\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "from rich.console import Console \n",
    "# For notebook display, pandas DataFrames are often preferred or use IPython.display.\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Matplotlib for plotting (optional, checked before use)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D \n",
    "except ModuleNotFoundError:\n",
    "    plt = None\n",
    "    Line2D = None\n",
    "    print(\"Matplotlib not found. Plotting functions will be disabled. Install with: pip install matplotlib\")\n",
    "\n",
    "# python-magic for MIME sniffing (optional, checked before use)\n",
    "try:\n",
    "    import magic\n",
    "except ModuleNotFoundError:\n",
    "    magic = None\n",
    "    print(\"python-magic not found. MIME type sniffing will use a default.\")\n",
    "\n",
    "# --- Globals & Constants ---\n",
    "CON = Console(force_jupyter=False, force_terminal=False) # Better for notebook mixed output\n",
    "WAREHOUSE = Path(\".warehouse/cluster_logs.duckdb\")\n",
    "WAREHOUSE.parent.mkdir(parents=True, exist_ok=True) # Ensure warehouse directory exists\n",
    "\n",
    "HASH_CHUNK = 1 << 16  # 64 KiB block size for streaming SHA-1\n",
    "\n",
    "print(f\"DuckDB Warehouse will be created/used at: {WAREHOUSE.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "# Cell 2: Helper Functions\n",
    "\n",
    "def _sha1(p: Path) -> str:\n",
    "    \"\"\"Computes SHA1 hash of a file.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    try:\n",
    "        with p.open(\"rb\") as fh:\n",
    "            while blk := fh.read(HASH_CHUNK):\n",
    "                h.update(blk)\n",
    "        return h.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        # This case should ideally be handled before calling _sha1 if p might not exist.\n",
    "        # For robustness, returning an empty string or raising a specific error.\n",
    "        # CON.print(f\"[yellow]File not found for hashing: {p}[/yellow]\") \n",
    "        return \"\" \n",
    "\n",
    "def canonical_version_name(version_str: str) -> str:\n",
    "    \"\"\"Maps various raw version strings to a canonical project version name.\"\"\"\n",
    "    if not isinstance(version_str, str):\n",
    "        return \"UnknownVersion\"\n",
    "    \n",
    "    v_lower = version_str.lower()\n",
    "\n",
    "    # Order matters: more specific matches first\n",
    "    if \"v2\" in v_lower and (\"2.2\" in v_lower or \"scatter_halo\" in v_lower or \"scatter-halo\" in v_lower):\n",
    "        return \"V2.2 ScatterHalo\"\n",
    "    if \"v2\" in v_lower and (\"2.1\" in v_lower or \"broadcast_all\" in v_lower or \"broadcast-all\" in v_lower):\n",
    "        return \"V2.1 BroadcastAll\"\n",
    "    if \"v1\" in v_lower or \"serial\" in v_lower:\n",
    "        return \"V1 Serial\"\n",
    "    if \"v3\" in v_lower or \"cuda\" in v_lower and \"mpi\" not in v_lower: # ensure not v4 or v5\n",
    "        return \"V3 CUDA\"\n",
    "    if \"v4\" in v_lower or (\"mpi\" in v_lower and \"cuda\" in v_lower): # Basic V4 check\n",
    "        return \"V4 MPI+CUDA\"\n",
    "    if \"v5\" in v_lower: # Assuming v5 implies MPI+CUDA as well\n",
    "        return \"V5 MPI+CUDA-Aware\"\n",
    "    \n",
    "    # Fallback for log file names that might just contain the version number\n",
    "    if \"v2_1\" in v_lower or \"v2.1\" in v_lower: return \"V2.1 BroadcastAll\"\n",
    "    if \"v2_2\" in v_lower or \"v2.2\" in v_lower: return \"V2.2 ScatterHalo\"\n",
    "\n",
    "    # Default if no specific pattern matches but starts with 'v' and a digit\n",
    "    if v_lower.startswith(\"v\") and len(v_lower) > 1 and v_lower[1].isdigit():\n",
    "        return version_str # Keep original if it's a new variant not yet mapped\n",
    "        \n",
    "    return \"Other\" # Or version_str to keep unmapped ones visible\n",
    "\n",
    "def _normalise_summary_df(df: pd.DataFrame, src: str) -> pd.DataFrame:\n",
    "    \"\"\"Maps various summary CSV formats to a canonical schema, including version normalization.\"\"\"\n",
    "    df = df.copy()\n",
    "    output_columns = [\"ts\", \"version\", \"np\", \"total_time_s\"]\n",
    "    for col in output_columns:\n",
    "        if col not in df: \n",
    "            df[col] = pd.NA\n",
    "\n",
    "    parsed_version = \"UnknownVersion\" # Default\n",
    "\n",
    "    # Pattern 1: legacy run_summary_*.csv (Fall 2023 template)\n",
    "    if {\"Timestamp\", \"Version\", \"NP\", \"Time_ms\"} <= set(df.columns):\n",
    "        df[\"ts\"]       = pd.to_datetime(df[\"Timestamp\"],  utc=True, errors=\"coerce\")\n",
    "        parsed_version = df[\"Version\"].astype(str).iloc[0] if not df.empty else \"UnknownVersion\" # Get from first row\n",
    "        df[\"np\"]       = pd.to_numeric(df[\"NP\"], errors=\"coerce\")\n",
    "        if \"Time_ms\" in df.columns:\n",
    "            df[\"total_time_s\"] = pd.to_numeric(df[\"Time_ms\"], errors=\"coerce\").astype(float) / 1000.0\n",
    "        else: df[\"total_time_s\"] = pd.NA\n",
    "\n",
    "\n",
    "    # Pattern 2: current summary_*.csv (Spring 2025 orchestrator)\n",
    "    elif {\"EntryTimestamp\", \"ProjectVariant\", \"NumProcesses\"}.issubset(df.columns):\n",
    "        df[\"ts\"]       = pd.to_datetime(df[\"EntryTimestamp\"], utc=True, errors=\"coerce\")\n",
    "        parsed_version = df[\"ProjectVariant\"].astype(str).iloc[0] if not df.empty else \"UnknownVersion\"\n",
    "        df[\"np\"]       = pd.to_numeric(df[\"NumProcesses\"], errors=\"coerce\")\n",
    "        time_col = None\n",
    "        if \"ExecutionTime_ms\" in df.columns: time_col = \"ExecutionTime_ms\"\n",
    "        elif \"Time_ms\" in df.columns: time_col = \"Time_ms\"\n",
    "        \n",
    "        if time_col and time_col in df.columns:\n",
    "            df[\"total_time_s\"] = pd.to_numeric(df[time_col], errors=\"coerce\").astype(float) / 1000.0\n",
    "        else: df[\"total_time_s\"] = pd.NA\n",
    "            \n",
    "    # Pattern 3: for 'summary_nixos_*.csv' style logs\n",
    "    elif {\"timestamp\", \"version\", \"np\", \"time_ms\"} <= set(df.columns):\n",
    "        df[\"ts\"]       = pd.to_datetime(df[\"timestamp\"],  utc=True, errors=\"coerce\")\n",
    "        parsed_version = df[\"version\"].astype(str).iloc[0] if not df.empty else \"UnknownVersion\"\n",
    "        df[\"np\"]       = pd.to_numeric(df[\"np\"], errors=\"coerce\") \n",
    "        if \"time_ms\" in df.columns:\n",
    "            df[\"total_time_s\"] = pd.to_numeric(df[\"time_ms\"], errors=\"coerce\").astype(float) / 1000.0\n",
    "        else: df[\"total_time_s\"] = pd.NA\n",
    "    else:\n",
    "        # CON.print(f\"[dim]Unknown summary CSV schema: {src}[/dim]\")\n",
    "        return pd.DataFrame(columns=output_columns) \n",
    "\n",
    "    # Apply canonical version naming to the entire 'version' column\n",
    "    df[\"version\"] = df[\"version\"].apply(canonical_version_name) # Important: apply to the existing version col\n",
    "\n",
    "    df_filtered = df[output_columns].dropna(subset=[\"ts\", \"version\", \"np\", \"total_time_s\"])\n",
    "    return df_filtered\n",
    "\n",
    "print(\"Helper and canonicalization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8389cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# Cell 3: Database Schema Definitions\n",
    "\n",
    "SCHEMA_SQL = \"\"\"\n",
    "DROP VIEW IF EXISTS efficiency;\n",
    "DROP VIEW IF EXISTS speedup;\n",
    "DROP VIEW IF EXISTS run_stats;\n",
    "DROP VIEW IF EXISTS best_runs;\n",
    "DROP VIEW IF EXISTS perf_runs;\n",
    "\n",
    "DROP TABLE IF EXISTS file_index;\n",
    "DROP TABLE IF EXISTS summary_runs;\n",
    "DROP TABLE IF EXISTS run_logs;\n",
    "DROP TABLE IF EXISTS source_stats;\n",
    "\n",
    "CREATE TABLE file_index (\n",
    "  relpath  TEXT PRIMARY KEY, \n",
    "  sha1     TEXT,            \n",
    "  size     BIGINT,\n",
    "  mtime    TIMESTAMP,\n",
    "  mime     TEXT\n",
    ");\n",
    "CREATE TABLE summary_runs (\n",
    "  ts TIMESTAMP,\n",
    "  version TEXT, -- This will store the CANONICAL version\n",
    "  np INT,\n",
    "  total_time_s DOUBLE\n",
    ");\n",
    "CREATE TABLE run_logs (\n",
    "  relpath TEXT, -- Original relpath for traceability\n",
    "  ts TIMESTAMP,\n",
    "  version TEXT, -- This will store the CANONICAL version\n",
    "  np INT,\n",
    "  total_time_s DOUBLE\n",
    ");\n",
    "CREATE TABLE source_stats (\n",
    "  relpath TEXT PRIMARY KEY, \n",
    "  loc INT,\n",
    "  func_cnt INT,\n",
    "  include_cnt INT,\n",
    "  cuda_kernel_cnt INT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DERIVED_VIEWS_SQL = \"\"\"\n",
    "CREATE OR REPLACE VIEW perf_runs AS\n",
    "SELECT ts, version, np, total_time_s FROM summary_runs WHERE total_time_s IS NOT NULL AND total_time_s > 1e-9 AND version != 'Other' AND version != 'UnknownVersion'\n",
    "UNION ALL\n",
    "SELECT ts, version, np, total_time_s FROM run_logs WHERE total_time_s IS NOT NULL AND total_time_s > 1e-9 AND version != 'Other' AND version != 'UnknownVersion';\n",
    "\n",
    "CREATE OR REPLACE VIEW best_runs AS\n",
    "SELECT version, np, MIN(total_time_s)  AS best_s\n",
    "FROM   perf_runs GROUP BY version, np;\n",
    "\n",
    "CREATE OR REPLACE VIEW run_stats AS\n",
    "SELECT version, np,\n",
    "       COUNT(*)                            AS n,\n",
    "       AVG(total_time_s)                  AS mean_s,\n",
    "       STDDEV_SAMP(total_time_s)          AS sd_s, \n",
    "       CASE WHEN COUNT(*)>1 THEN 1.96*STDDEV_SAMP(total_time_s)/SQRT(COUNT(*)) ELSE NULL END AS ci95_s\n",
    "FROM   perf_runs GROUP BY version, np;\n",
    "\n",
    "CREATE OR REPLACE VIEW speedup AS\n",
    "WITH base_runs AS (\n",
    "    SELECT \n",
    "        version, \n",
    "        MIN(best_s) as t1 \n",
    "    FROM best_runs \n",
    "    WHERE np = 1\n",
    "    GROUP BY version\n",
    ")\n",
    "SELECT \n",
    "    br.version,\n",
    "    br.np,\n",
    "    b.t1 / br.best_s AS S \n",
    "FROM best_runs br \n",
    "JOIN base_runs b ON br.version = b.version\n",
    "WHERE b.t1 IS NOT NULL AND br.best_s IS NOT NULL AND br.best_s > 1e-9; \n",
    "\n",
    "CREATE OR REPLACE VIEW efficiency AS\n",
    "SELECT version, np, S/np AS E FROM speedup WHERE np > 0;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Database schemas and view definitions ready (includes DROP statements for rebuilds).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "# Cell 4: Data Ingestion Function\n",
    "\n",
    "def ingest_data(root: Path, rebuild: bool = False):\n",
    "    \"\"\"Scans the root directory for CSV summaries, run logs, and source files,\n",
    "    then loads the extracted data into the DuckDB warehouse.\"\"\"\n",
    "    \n",
    "    if rebuild and WAREHOUSE.exists():\n",
    "        WAREHOUSE.unlink()\n",
    "        CON.print(\"[red]• Wiped existing warehouse[/red]\")\n",
    "\n",
    "    with ddb.connect(str(WAREHOUSE)) as con:\n",
    "        con.execute(SCHEMA_SQL) # Executes DROPs and CREATEs\n",
    "\n",
    "        seen: Dict[str, str] = dict(con.execute(\"SELECT relpath, sha1 FROM file_index\").fetchall())\n",
    "        rows_summary_dfs: List[pd.DataFrame] = [] # Changed to list of DataFrames\n",
    "        rows_runlog:  List[tuple]       = []\n",
    "        rows_srcstats:List[tuple]       = []\n",
    "        processed_files_this_run = 0\n",
    "        newly_indexed_count = 0\n",
    "\n",
    "        CON.print(f\"Starting ingestion from root: {root.resolve()}\")\n",
    "        all_files = list(root.rglob(\"*\")) # Collect all files first to avoid issues with changing dir\n",
    "        \n",
    "        for p in all_files:\n",
    "            if p.is_dir() or WAREHOUSE.resolve() == p.resolve(): \n",
    "                continue\n",
    "            \n",
    "            rel = str(p.relative_to(root))\n",
    "            \n",
    "            try:\n",
    "                current_sha1 = _sha1(p)\n",
    "                if not current_sha1 : continue # Skip if hashing failed\n",
    "\n",
    "                current_stat = p.stat()\n",
    "                size  = current_stat.st_size\n",
    "                mtime = datetime.fromtimestamp(current_stat.st_mtime, tz=timezone.utc)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            if seen.get(rel) != current_sha1: # Process if new or modified\n",
    "                newly_indexed_count +=1\n",
    "                mime_type  = magic.from_file(str(p), mime=True) if magic else \"application/octet-stream\"\n",
    "\n",
    "                # --- CSV Summaries ---\n",
    "                if p.suffix.lower() == \".csv\" and \"summary\" in p.name.lower():\n",
    "                    try:\n",
    "                        df_raw = pd.read_csv(p)\n",
    "                        if not df_raw.empty:\n",
    "                            df_norm = _normalise_summary_df(df_raw, rel) # Normalization includes canonical_version_name\n",
    "                            if not df_norm.empty:\n",
    "                                rows_summary_dfs.append(df_norm)\n",
    "                    except pd.errors.EmptyDataError: pass\n",
    "                    except Exception as e: CON.print(f\"[yellow]CSV summary parse error {rel}: {e}[/yellow]\")\n",
    "                \n",
    "                # --- Individual Run Logs ---\n",
    "                elif p.suffix.lower() == \".log\" and (\"run_v\" in p.name or \"final_project_v\" in p.name or \"make_v\" in p.name): \n",
    "                    try:\n",
    "                        txt = p.read_text(errors=\"ignore\")\n",
    "                        m_time_val = None\n",
    "                        time_patterns = [\n",
    "                            r\"AlexNet MPI\\+CUDA Forward Pass completed in\\s*([\\d\\.]+)\\s*ms\",\n",
    "                            r\"AlexNet CUDA Forward Pass completed in\\s*([\\d\\.]+)\\s*ms\", \n",
    "                            r\"AlexNet Serial Forward Pass completed in\\s*([\\d\\.]+)\\s*ms\",\n",
    "                            r\"Serial execution finished successfully in\\s*([\\d\\.]+)\\s*ms\",\n",
    "                            r\"MPI\\+CUDA execution finished successfully in\\s*([\\d\\.]+)\\s*ms\",\n",
    "                            r\"CUDA execution finished successfully in\\s*([\\d\\.]+)\\s*ms\", \n",
    "                            r\"MPI execution finished successfully in\\s*([\\d\\.]+)\\s*ms\", \n",
    "                            r\"Execution Time:\\s*([\\d\\.]+)\\s*ms\", \n",
    "                            r\"(?:Time|ExecutionTime)_ms[\\s:=]*([\\d\\.]+)\",\n",
    "                            r\"Total execution time:\\s*([\\d\\.]+)\\s*seconds\", \n",
    "                        ]\n",
    "                        for pat in time_patterns:\n",
    "                            m_search = re.search(pat, txt, re.IGNORECASE)\n",
    "                            if m_search:\n",
    "                                m_time_val = float(m_search.group(1))\n",
    "                                if \"seconds\" not in pat.lower(): m_time_val /= 1000.0\n",
    "                                break \n",
    "                        \n",
    "                        if m_time_val is not None:\n",
    "                            raw_version_str = \"unknown_version_in_log\"\n",
    "                            ver_pattern = r\"(v\\d(?:[\\._]\\d+(?:[\\._][\\w-]+)*)?(?:_[\\w-]+)*)\"\n",
    "                            ver_match = re.search(ver_pattern, p.name, re.IGNORECASE)\n",
    "                            if ver_match:\n",
    "                                raw_version_str = ver_match.group(0)\n",
    "                            else:\n",
    "                                parent_name = p.parent.name\n",
    "                                if parent_name.lower().startswith(\"v\"):\n",
    "                                    raw_version_str = parent_name\n",
    "                            \n",
    "                            canonical_ver = canonical_version_name(raw_version_str)\n",
    "                            \n",
    "                            np_m = re.search(r\"np(\\d+)\", p.name, re.IGNORECASE)\n",
    "                            np_val = int(np_m.group(1)) if np_m else 1\n",
    "                            \n",
    "                            ts_match_in_name = re.search(r\"(\\d{8}_\\d{6})\", p.name) # Check filename first\n",
    "                            log_ts = mtime # Default to file mtime\n",
    "                            if ts_match_in_name:\n",
    "                                try:\n",
    "                                    log_ts = datetime.strptime(ts_match_in_name.group(1), \"%Y%m%d_%H%M%S\").replace(tzinfo=timezone.utc)\n",
    "                                except ValueError: pass # Keep mtime if parsing fails\n",
    "                            \n",
    "                            rows_runlog.append((rel, log_ts, canonical_ver, np_val, m_time_val))\n",
    "                    except Exception as e: pass # CON.print(f\"[yellow]Run log parse error {rel}: {e}[/yellow]\")\n",
    "\n",
    "                # --- Source File Statistics ---\n",
    "                elif mime_type and (\"text/\" in mime_type or mime_type == 'application/octet-stream' or p.name.lower().endswith(\"makefile\") or \".make\" in p.name.lower()):\n",
    "                    try:\n",
    "                        code = p.read_text(errors=\"ignore\")\n",
    "                        loc = code.count(\"\\n\") + 1\n",
    "                        func_cnt = len(re.findall(r\"\\b[A-Za-z_]\\w*\\s*\\([^)]*\\)\\s*(?:const)?\\s*\\{\", code))\n",
    "                        inc_cnt = len(re.findall(r\"^\\s*#include\", code, re.MULTILINE))\n",
    "                        kern_cnt = code.count(\"__global__\")\n",
    "                        rows_srcstats.append((rel, loc, func_cnt, inc_cnt, kern_cnt))\n",
    "                    except Exception as e: pass # CON.print(f\"[yellow]Source stat parse error {rel}: {e}[/yellow]\")\n",
    "                \n",
    "                # --- Update file_index Table ---\n",
    "                try:\n",
    "                    con.execute(\n",
    "                        \"INSERT INTO file_index (relpath, sha1, size, mtime, mime) VALUES (?, ?, ?, ?, ?) \"\n",
    "                        \"ON CONFLICT(relpath) DO UPDATE SET sha1=excluded.sha1, size=excluded.size, mtime=excluded.mtime, mime=excluded.mime\",\n",
    "                        [rel, current_sha1, size, mtime, mime_type]\n",
    "                    )\n",
    "                except Exception as db_ex: CON.print(f\"[red]DB Error indexing {rel}: {db_ex}[/red]\")\n",
    "            processed_files_this_run +=1\n",
    "\n",
    "\n",
    "        if newly_indexed_count == 0 and not rebuild:\n",
    "             CON.print(\"[cyan]• No new or modified files to process since last ingest.[/cyan]\")\n",
    "        elif newly_indexed_count > 0:\n",
    "             CON.print(f\"[green]• Indexed/Updated {newly_indexed_count} files in file_index.[/green]\")\n",
    "\n",
    "\n",
    "        # --- Bulk Insert Data ---\n",
    "        inserted_summary_count = 0\n",
    "        if rows_summary_dfs:\n",
    "            df_all_summary = pd.concat(rows_summary_dfs, ignore_index=True).drop_duplicates().dropna(subset=['total_time_s', 'version', 'np'])\n",
    "            if not df_all_summary.empty:\n",
    "                # Ensure 'version' in df_all_summary is canonical before inserting\n",
    "                # _normalise_summary_df already applies canonical_version_name\n",
    "                con.register(\"df_all_summary_reg\", df_all_summary)\n",
    "                con.execute(\"INSERT INTO summary_runs SELECT ts, version, np, total_time_s FROM df_all_summary_reg\")\n",
    "                con.unregister(\"df_all_summary_reg\")\n",
    "                inserted_summary_count = len(df_all_summary)\n",
    "        CON.print(f\"[cyan]• {inserted_summary_count} summary rows ingested[/cyan]\")\n",
    "\n",
    "        inserted_log_count = 0\n",
    "        if rows_runlog:\n",
    "            # Ensure 'version' in rows_runlog (which is 3rd element, index 2) is already canonical\n",
    "            df_runlog = pd.DataFrame(rows_runlog, columns=['relpath', 'ts', 'version', 'np', 'total_time_s']).drop_duplicates().dropna(subset=['total_time_s', 'version', 'np'])\n",
    "            if not df_runlog.empty:\n",
    "                con.executemany(\"INSERT INTO run_logs VALUES (?,?,?,?,?)\", df_runlog.to_records(index=False).tolist())\n",
    "                inserted_log_count = len(df_runlog)\n",
    "        CON.print(f\"[cyan]• {inserted_log_count} run_log rows ingested[/cyan]\")\n",
    "\n",
    "        inserted_src_count = 0\n",
    "        if rows_srcstats:\n",
    "            df_srcstats = pd.DataFrame(rows_srcstats, columns=['relpath', 'loc', 'func_cnt', 'include_cnt', 'cuda_kernel_cnt']).drop_duplicates(subset=['relpath'])\n",
    "            if not df_srcstats.empty:\n",
    "                con.executemany(\n",
    "                    \"INSERT INTO source_stats (relpath, loc, func_cnt, include_cnt, cuda_kernel_cnt) VALUES (?,?,?,?,?) \"\n",
    "                    \"ON CONFLICT(relpath) DO UPDATE SET loc=excluded.loc, func_cnt=excluded.func_cnt, include_cnt=excluded.include_cnt, cuda_kernel_cnt=excluded.cuda_kernel_cnt\",\n",
    "                    df_srcstats.to_records(index=False).tolist()\n",
    "                )\n",
    "                inserted_src_count = len(df_srcstats)\n",
    "        CON.print(f\"[cyan]• {inserted_src_count} source files stats ingested/updated[/cyan]\")\n",
    "\n",
    "        con.execute(DERIVED_VIEWS_SQL)\n",
    "        CON.print(\"[bold green]✓ Ingest complete. Derived views (re)created.[/bold green]\")\n",
    "\n",
    "print(\"Ingestion function 'ingest_data' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "# Cell 5: Perform Data Ingestion\n",
    "\n",
    "project_root = Path(\".\") \n",
    "ingest_data(root=project_root / \"final_project\", rebuild=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69467fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "# Cell 6: Ad-hoc Querying and Database Inspection\n",
    "\n",
    "def execute_query(sql_query: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Executes a SQL query against the warehouse and returns a DataFrame.\"\"\"\n",
    "    try:\n",
    "        with ddb.connect(str(WAREHOUSE), read_only=True) as con:\n",
    "            df = con.execute(sql_query).fetchdf()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # For notebook, print might be better than CON.print\n",
    "        print(f\"SQL Query Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Database Inspection ---\n",
    "print(\"\\n--- Database Objects (Tables and Views) ---\")\n",
    "all_db_objects = execute_query(\n",
    "    \"\"\"\n",
    "    SELECT table_name as object_name, table_type as type \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'main'\n",
    "    UNION ALL \n",
    "    SELECT view_name as object_name, 'VIEW' as type \n",
    "    FROM duckdb_views() \n",
    "    WHERE schema_name = 'main'\n",
    "    ORDER BY type, object_name\n",
    "    \"\"\"\n",
    ")\n",
    "if all_db_objects is not None:\n",
    "    display(all_db_objects)\n",
    "\n",
    "print(\"\\n--- Schema of 'best_runs' view ---\")\n",
    "best_runs_schema = execute_query(\"DESCRIBE best_runs;\")\n",
    "if best_runs_schema is not None:\n",
    "    display(best_runs_schema)\n",
    "\n",
    "print(\"\\n--- Sample data from 'best_runs' (Grouped by canonical version, NP) ---\")\n",
    "sample_best_runs = execute_query(\"SELECT * FROM best_runs ORDER BY version, np LIMIT 10;\")\n",
    "if sample_best_runs is not None:\n",
    "    display(sample_best_runs)\n",
    "\n",
    "print(\"\\n--- Perf run counts per canonical version ---\")\n",
    "version_counts = execute_query(\"SELECT version, COUNT(*) as count FROM perf_runs GROUP BY version ORDER BY version;\")\n",
    "if version_counts is not None:\n",
    "    display(version_counts)\n",
    "\n",
    "\n",
    "print(\"\\n--- Total Source Files Analyzed ---\")\n",
    "source_file_count = execute_query(\"SELECT COUNT(*) as num_source_files FROM source_stats;\")\n",
    "if source_file_count is not None:\n",
    "    display(source_file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "# Cell 7: Display Run Statistics\n",
    "\n",
    "print(\"\\n--- Run Statistics (Time in seconds, for Canonical Versions) ---\")\n",
    "run_stats_df = execute_query(\"\"\"\n",
    "    SELECT version, np, n, \n",
    "           ROUND(mean_s, 4) as mean_s, \n",
    "           ROUND(sd_s, 4) as sd_s, \n",
    "           ROUND(ci95_s, 4) as ci95_s \n",
    "    FROM run_stats \n",
    "    ORDER BY version, np\n",
    "\"\"\")\n",
    "\n",
    "if run_stats_df is not None and not run_stats_df.empty:\n",
    "    display(run_stats_df)\n",
    "else:\n",
    "    print(\"No run statistics to display. Ensure 'perf_runs' has data for canonical versions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7702d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "# Cell 8: Display Speedup and Efficiency Tables\n",
    "\n",
    "print(\"\\n--- Speedup (S = T_NP1 / T_NPn, relative to canonical version's NP=1) ---\")\n",
    "speedup_df = execute_query(\"SELECT version, np, ROUND(S, 3) AS S FROM speedup ORDER BY version, np\")\n",
    "if speedup_df is not None and not speedup_df.empty:\n",
    "    display(speedup_df)\n",
    "else:\n",
    "    print(\"No speedup data. Ensure NP=1 runs exist for canonical versions in 'best_runs'.\")\n",
    "\n",
    "print(\"\\n--- Efficiency (E = S / np, relative to canonical version's NP=1) ---\")\n",
    "efficiency_df = execute_query(\"SELECT version, np, ROUND(E, 3) AS E FROM efficiency ORDER BY version, np\")\n",
    "if efficiency_df is not None and not efficiency_df.empty:\n",
    "    display(efficiency_df)\n",
    "else:\n",
    "    print(\"No efficiency data. Ensure 'speedup' view has data for canonical versions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "# Cell 9: Data Export Function\n",
    "\n",
    "def export_table(table_name: str, out_file: Path):\n",
    "    \"\"\"Exports a specified table or view from the database to a file (MD, CSV, Parquet).\"\"\"\n",
    "    check_exists_query = f\"\"\"\n",
    "        SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}' AND table_schema = 'main'\n",
    "        UNION ALL \n",
    "        SELECT 1 FROM duckdb_views() WHERE view_name = '{table_name}' AND schema_name = 'main' \n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    check_exists = execute_query(check_exists_query)\n",
    "    if check_exists is None or check_exists.empty:\n",
    "        print(f\"Table or view '{table_name}' does not exist in the database.\")\n",
    "        # Optionally list available tables/views for debugging\n",
    "        # all_objs = execute_query(\"SELECT name, type from duckdb_objects() WHERE schema_name='main' ORDER BY type, name;\")\n",
    "        # if all_objs is not None: print(\"Available objects:\\n\", all_objs)\n",
    "        return\n",
    "\n",
    "    df = execute_query(f\"SELECT * FROM {table_name}\")\n",
    "\n",
    "    if df is None :\n",
    "        print(f\"Failed to fetch data from '{table_name}' for export, though it seems to exist.\")\n",
    "        return\n",
    "    if df.empty:\n",
    "        print(f\"Table/view '{table_name}' is empty. Nothing to export to {out_file}.\")\n",
    "        return\n",
    "\n",
    "    file_suffix = out_file.suffix.lower()\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True) \n",
    "    try:\n",
    "        if file_suffix == \".md\":\n",
    "            df.to_markdown(out_file, index=False)\n",
    "        elif file_suffix == \".csv\":\n",
    "            df.to_csv(out_file, index=False)\n",
    "        elif file_suffix == \".parquet\":\n",
    "            df.to_parquet(out_file, index=False)\n",
    "        else:\n",
    "            print(f\"Unsupported export file format: {file_suffix}. Supported: .md, .csv, .parquet\")\n",
    "            return\n",
    "        print(f\"✓ Table '{table_name}' exported successfully to {out_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing table '{table_name}' to file {out_file}: {e}\")\n",
    "\n",
    "print(\"Data export function 'export_table' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "# Cell 10: Example Export Calls\n",
    "\n",
    "output_dir = Path(\"analysis_exports\") \n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "export_table(\"best_runs\", output_dir / \"project_best_runs.md\")\n",
    "export_table(\"run_stats\", output_dir / \"project_run_statistics.csv\")\n",
    "export_table(\"perf_runs\", output_dir / \"project_all_perf_runs.parquet\")\n",
    "export_table(\"speedup\", output_dir / \"project_speedup_data.csv\")\n",
    "export_table(\"efficiency\", output_dir / \"project_efficiency_data.csv\")\n",
    "export_table(\"file_index\", output_dir / \"project_indexed_files.parquet\") # Parquet for potentially large table\n",
    "export_table(\"source_stats\", output_dir / \"project_source_code_stats.md\")\n",
    "\n",
    "print(f\"\\nExports completed to directory: {output_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "# Cell 11: Runtime Plotting Function\n",
    "\n",
    "def generate_runtime_plot(out_file: Path = Path(\"analysis_plots/runtimes_plot.png\")):\n",
    "    \"\"\"Plots Runtime vs NP for each version (using fastest runs) and saves to file.\"\"\"\n",
    "    if plt is None:\n",
    "        print(\"matplotlib not installed – cannot plot.\")\n",
    "        return\n",
    "\n",
    "    df = execute_query(\"SELECT version, np, best_s FROM best_runs WHERE best_s IS NOT NULL ORDER BY version, np\")\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"No data in 'best_runs' to plot for runtimes (after filtering for canonical versions).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7)) # Adjusted size\n",
    "    versions = df[\"version\"].unique()\n",
    "    for ver in versions:\n",
    "        grp = df[df[\"version\"] == ver]\n",
    "        if not grp.empty:\n",
    "             plt.plot(grp[\"np\"], grp[\"best_s\"], marker=\"o\", linestyle=\"-\", label=ver)\n",
    "    \n",
    "    plt.xlabel(\"Number of Processes (NP)\")\n",
    "    plt.ylabel(\"Best Runtime [s]\")\n",
    "    \n",
    "    unique_nps = sorted(df[\"np\"].unique())\n",
    "    if unique_nps: \n",
    "        is_power_of_2_friendly = all(np_val != 0 and (np_val & (np_val - 1) == 0) for np_val in unique_nps if np_val is not None and np_val > 0)\n",
    "        if max(unique_nps, default=1) / max(1, min(filter(lambda x: x>0, unique_nps), default=1)) >= 4 and len(unique_nps) > 3 : \n",
    "             plt.xscale(\"log\", base=2 if is_power_of_2_friendly else 10)\n",
    "        plt.xticks(unique_nps, labels=[str(int(x)) for x in unique_nps]) # Ensure integer labels for NP\n",
    "\n",
    "    min_best_s_val = df[\"best_s\"].min() if not df[\"best_s\"].empty and df[\"best_s\"].min() > 0 else 1e-9\n",
    "    max_best_s_val = df[\"best_s\"].max() if not df[\"best_s\"].empty else 1.0\n",
    "    if (max_best_s_val / min_best_s_val > 10): \n",
    "        plt.yscale(\"log\", base=10)\n",
    "        \n",
    "    plt.title(\"Runtime vs Number of Processes (Canonical Versions)\")\n",
    "    plt.grid(True, which=\"both\", ls=\":\", lw=0.5)\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\") # Adjusted legend\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_file)\n",
    "        print(f\"✓ Runtime plot saved to {out_file}\")\n",
    "        plt.show() \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving runtime plot to {out_file}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "print(\"Runtime plotting function 'generate_runtime_plot' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "# Cell 12: Example Runtime Plot Call\n",
    "\n",
    "generate_runtime_plot(out_file=Path(\"analysis_plots/project_performance_runtimes.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "# Cell 13: Speedup Plotting Function\n",
    "\n",
    "def generate_speedup_plot(out_file: Path = Path(\"analysis_plots/speedup_curve_plot.png\")):\n",
    "    \"\"\"Plots Speedup vs NP for each version and saves to file.\"\"\"\n",
    "    if plt is None or Line2D is None: \n",
    "        print(\"matplotlib not installed – cannot plot.\")\n",
    "        return\n",
    "\n",
    "    df = execute_query(\"SELECT version, np, S FROM speedup WHERE S IS NOT NULL ORDER BY version, np\")\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data in 'speedup' view to plot (check for NP=1 runs for canonical versions).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    all_nps_in_plot = [] \n",
    "    versions = df[\"version\"].unique()\n",
    "    for ver in versions:\n",
    "        grp = df[df[\"version\"] == ver]\n",
    "        if not grp.empty:\n",
    "            plt.plot(grp[\"np\"], grp[\"S\"], marker=\"o\", linestyle=\"-\", label=ver)\n",
    "            all_nps_in_plot.extend(grp[\"np\"].tolist())\n",
    "    \n",
    "    unique_nps_for_ideal_line = sorted(list(set(filter(None, all_nps_in_plot)))) \n",
    "    if unique_nps_for_ideal_line: \n",
    "        plt.plot(unique_nps_for_ideal_line, unique_nps_for_ideal_line, linestyle=\"--\", color=\"gray\", label=\"Ideal Speedup\")\n",
    "\n",
    "    plt.xlabel(\"Number of Processes (NP)\")\n",
    "    plt.ylabel(\"Speedup (S = T_NP1 / T_NPn)\")\n",
    "    \n",
    "    unique_nps_overall = sorted(df[\"np\"].unique())\n",
    "    if unique_nps_overall:\n",
    "        is_power_of_2_friendly = all(np_val != 0 and (np_val & (np_val - 1) == 0) for np_val in unique_nps_overall if np_val is not None and np_val > 0)\n",
    "        if max(unique_nps_overall, default=1) / max(1, min(filter(lambda x: x>0, unique_nps_overall), default=1)) >= 4 and len(unique_nps_overall) > 3:\n",
    "            plt.xscale(\"log\", base=2 if is_power_of_2_friendly else 10)\n",
    "        plt.xticks(unique_nps_overall, labels=[str(int(x)) for x in unique_nps_overall])\n",
    "    \n",
    "    plt.title(\"Speedup Curve vs Number of Processes (Canonical Versions)\")\n",
    "    plt.grid(True, which=\"both\", ls=\":\", lw=0.5)\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_file)\n",
    "        print(f\"✓ Speedup curve plot saved to {out_file}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving speedup plot to {out_file}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "print(\"Speedup plotting function 'generate_speedup_plot' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9379e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "# Cell 14: Example Speedup Plot Call\n",
    "\n",
    "generate_speedup_plot(out_file=Path(\"analysis_plots/project_performance_speedup.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "# Cell 15: Efficiency Plotting Function\n",
    "\n",
    "def generate_efficiency_plot(out_file: Path = Path(\"analysis_plots/efficiency_bars_plot.png\")):\n",
    "    \"\"\"Plots Parallel Efficiency vs NP for each version and saves to file.\"\"\"\n",
    "    if plt is None or Line2D is None:\n",
    "        print(\"matplotlib not installed – cannot plot.\")\n",
    "        return\n",
    "\n",
    "    df = execute_query(\"SELECT version, np, E FROM efficiency WHERE E IS NOT NULL ORDER BY version, np\")\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data in 'efficiency' view to plot (check speedup view).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(13, 7)) # Slightly wider for potentially many versions\n",
    "    plotted_as_bars = False\n",
    "    unique_nps_overall = sorted(df[\"np\"].unique())\n",
    "    versions_count = len(df[\"version\"].unique())\n",
    "\n",
    "    if unique_nps_overall and len(unique_nps_overall) <= 6 and versions_count <= 7 : \n",
    "        try:\n",
    "            df_pivot = df.pivot(index=\"np\", columns=\"version\", values=\"E\")\n",
    "            df_pivot.plot(kind=\"bar\", ax=plt.gca(), width=0.85) \n",
    "            plt.xticks(rotation=0) \n",
    "            plotted_as_bars = True\n",
    "        except Exception as e: \n",
    "            print(f\"Could not pivot efficiency data for bar plot (Error: {e}). Plotting as lines.\")\n",
    "    \n",
    "    if not plotted_as_bars: \n",
    "        versions = df[\"version\"].unique()\n",
    "        for ver in versions:\n",
    "            grp = df[df[\"version\"] == ver].sort_values(\"np\")\n",
    "            if not grp.empty:\n",
    "                plt.plot(grp[\"np\"], grp[\"E\"], marker=\"o\", linestyle=\"-\", label=f\"{ver}\")\n",
    "        \n",
    "        if unique_nps_overall: \n",
    "            is_power_of_2_friendly = all(np_val != 0 and (np_val & (np_val-1)==0) for np_val in unique_nps_overall if np_val is not None and np_val > 0)\n",
    "            if max(unique_nps_overall, default=1) / max(1, min(filter(lambda x: x>0, unique_nps_overall), default=1)) >= 4 and len(unique_nps_overall) > 3:\n",
    "                 plt.xscale(\"log\", base=2 if is_power_of_2_friendly else 10)\n",
    "            plt.xticks(unique_nps_overall, labels=[str(int(x)) for x in unique_nps_overall])\n",
    "\n",
    "    plt.xlabel(\"Number of Processes (NP)\")\n",
    "    plt.ylabel(\"Efficiency (E = Speedup / NP)\")\n",
    "    plt.title(\"Parallel Efficiency vs Number of Processes (Canonical Versions)\")\n",
    "    \n",
    "    max_e_val = df[\"E\"].max() if not df[\"E\"].empty and pd.notna(df[\"E\"].max()) else 1.0\n",
    "    upper_y_limit = max(1.1, max_e_val * 1.1 if pd.notna(max_e_val) else 1.1)\n",
    "    if upper_y_limit > 1.5 and plotted_as_bars: # Adjust y-limit for bar plots if superlinear\n",
    "        upper_y_limit = max_e_val * 1.1 \n",
    "    elif upper_y_limit > 2.0 and not plotted_as_bars: # Cap if very superlinear for line plots\n",
    "         upper_y_limit = max_e_val * 1.1\n",
    "\n",
    "    plt.ylim(0, upper_y_limit)\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    if not any(\"Ideal Efficiency\" in lab for lab in labels):\n",
    "        ideal_line = Line2D([0], [0], linestyle=\"--\", color=\"gray\", label='Ideal Efficiency (1.0)')\n",
    "        handles.append(ideal_line)\n",
    "        labels.append('Ideal Efficiency (1.0)')\n",
    "    \n",
    "    plt.axhline(1.0, linestyle=\"--\", color=\"gray\", linewidth=0.8) \n",
    "    plt.legend(handles, labels, loc=\"best\", title=\"Version\", fontsize=\"small\")\n",
    "\n",
    "    plt.grid(True, axis='y', ls=\":\", lw=0.5) \n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_file)\n",
    "        print(f\"✓ Efficiency plot saved to {out_file}\")\n",
    "        plt.show()\n",
    "    except Exception as e_save:\n",
    "        print(f\"Error saving efficiency plot to {out_file}: {e_save}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "print(\"Efficiency plotting function 'generate_efficiency_plot' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0405ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "# Cell 16: Example Efficiency Plot Call\n",
    "\n",
    "generate_efficiency_plot(out_file=Path(\"analysis_plots/project_performance_efficiency.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "# Cell 17: Focused Statistical Comparisons & Key Metrics Display\n",
    "\n",
    "print(\"\\n--- Key Performance Metrics from 'run_stats' (Canonical Versions) ---\")\n",
    "# Display mean, standard deviation, and number of runs for key versions\n",
    "key_versions_stats = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        version, \n",
    "        np, \n",
    "        n, \n",
    "        ROUND(mean_s, 4) AS mean_runtime_s, \n",
    "        ROUND(sd_s, 4) AS std_dev_s,\n",
    "        ROUND(ci95_s, 4) AS ci95_s\n",
    "    FROM run_stats\n",
    "    WHERE version IN ('V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA')\n",
    "    ORDER BY version, np;\n",
    "\"\"\")\n",
    "if key_versions_stats is not None and not key_versions_stats.empty:\n",
    "    display(key_versions_stats)\n",
    "else:\n",
    "    print(\"No stats found for key versions. Ensure data is ingested and canonical names are correct.\")\n",
    "\n",
    "print(\"\\n--- Fastest Single Process Runs (NP=1 from 'best_runs') ---\")\n",
    "fastest_np1_runs = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        version, \n",
    "        np, \n",
    "        ROUND(best_s, 4) AS fastest_runtime_s\n",
    "    FROM best_runs\n",
    "    WHERE np = 1 AND version IN ('V1 Serial', 'V2.1 BroadcastAll', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA')\n",
    "    ORDER BY fastest_runtime_s;\n",
    "\"\"\")\n",
    "if fastest_np1_runs is not None and not fastest_np1_runs.empty:\n",
    "    display(fastest_np1_runs)\n",
    "else:\n",
    "    print(\"No NP=1 best runs found for key versions.\")\n",
    "\n",
    "print(\"\\n--- Performance at Max Scaled NP (e.g., NP=4, from 'best_runs') ---\")\n",
    "# Assuming NP=4 is the max scale point for MPI versions\n",
    "max_np_runs = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        version, \n",
    "        np, \n",
    "        ROUND(best_s, 4) AS fastest_runtime_s\n",
    "    FROM best_runs\n",
    "    WHERE np = 4 AND version IN ('V2.1 BroadcastAll', 'V2.2 ScatterHalo', 'V4 MPI+CUDA')\n",
    "    ORDER BY fastest_runtime_s;\n",
    "\"\"\")\n",
    "if max_np_runs is not None and not max_np_runs.empty:\n",
    "    display(max_np_runs)\n",
    "else:\n",
    "    print(\"No NP=4 best runs found for relevant MPI versions.\")\n",
    "\n",
    "# You could add more specific statistical tests here if needed, e.g., using scipy.stats\n",
    "# For instance, comparing means of V3 CUDA vs V4 MPI+CUDA at NP=1 if you have multiple runs.\n",
    "# This would require fetching the raw run times from 'perf_runs' for those specific conditions.\n",
    "# Example (conceptual, requires scipy):\n",
    "# from scipy import stats\n",
    "# v3_np1_times = execute_query(\"SELECT total_time_s FROM perf_runs WHERE version = 'V3 CUDA' AND np = 1\")\n",
    "# v4_np1_times = execute_query(\"SELECT total_time_s FROM perf_runs WHERE version = 'V4 MPI+CUDA' AND np = 1\")\n",
    "# if v3_np1_times is not None and not v3_np1_times.empty and \\\n",
    "#    v4_np1_times is not None and not v4_np1_times.empty and \\\n",
    "#    len(v3_np1_times['total_time_s'].dropna()) > 1 and \\\n",
    "#    len(v4_np1_times['total_time_s'].dropna()) > 1:\n",
    "#    ttest_result = stats.ttest_ind(v3_np1_times['total_time_s'].dropna(), v4_np1_times['total_time_s'].dropna())\n",
    "#    print(f\"\\n--- T-test V3 CUDA (NP=1) vs V4 MPI+CUDA (NP=1) ---\")\n",
    "#    print(f\"Statistic: {ttest_result.statistic:.4f}, P-value: {ttest_result.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03facbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "# Cell 18: Generate and Export All Visualizations\n",
    "\n",
    "visuals_output_dir = Path(\"analysis_visuals_final\")\n",
    "visuals_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"All plots will be saved to: {visuals_output_dir.resolve()}\")\n",
    "\n",
    "# Generate and save Runtime Plot\n",
    "print(\"\\nGenerating Runtime Plot...\")\n",
    "generate_runtime_plot(out_file=visuals_output_dir / \"project_runtimes_vs_np.png\")\n",
    "\n",
    "# Generate and save Speedup Plot\n",
    "print(\"\\nGenerating Speedup Plot...\")\n",
    "generate_speedup_plot(out_file=visuals_output_dir / \"project_speedup_vs_np.png\")\n",
    "\n",
    "# Generate and save Efficiency Plot\n",
    "print(\"\\nGenerating Efficiency Plot...\")\n",
    "generate_efficiency_plot(out_file=visuals_output_dir / \"project_efficiency_vs_np.png\")\n",
    "\n",
    "print(\"\\n--- All visual exports attempted. Check console for success/error messages. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[19]:\n",
    "# Cell 19: Advanced Data Integration, Statistical Visualization & Synthesis\n",
    "\n",
    "import seaborn as sns # For potentially prettier plots\n",
    "import matplotlib.colors # For color mapping\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter # For ensuring integer ticks and custom formatting\n",
    "from scipy.stats import pearsonr # For correlation\n",
    "\n",
    "# Apply a nicer default style if seaborn is available\n",
    "if 'seaborn' in sys.modules:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "    print(\"[INFO] Seaborn theme applied for enhanced plot aesthetics.\")\n",
    "\n",
    "visuals_output_dir = Path(\"analysis_visuals_final\") \n",
    "visuals_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] All plots for this cell will be saved to: {visuals_output_dir.resolve()}\")\n",
    "\n",
    "# --- Helper to format y-axis ticks for log scale if used ---\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    return f\"{val:.2g}\" # Format to general with 2 significant figures\n",
    "\n",
    "# --- 1. Aggregate Lines of Code (LOC) for Canonical Versions ---\n",
    "print(\"\\n--- Aggregating Lines of Code (LOC) for Canonical Versions ---\")\n",
    "version_loc_map = {\n",
    "    \"V1 Serial\": [\"v1_serial/src/\", \"v1_serial/include/\"],\n",
    "    \"V2.1 BroadcastAll\": [\"v2_mpi_only/2.1_broadcast_all/src/\", \"v2_mpi_only/2.1_broadcast_all/include/\"],\n",
    "    \"V2.2 ScatterHalo\": [\"v2_mpi_only/2.2_scatter_halo/src/\", \"v2_mpi_only/2.2_scatter_halo/include/\"],\n",
    "    \"V3 CUDA\": [\"v3_cuda_only/src/\", \"v3_cuda_only/include/\"],\n",
    "    \"V4 MPI+CUDA\": [\"v4_mpi_cuda/src/\", \"v4_mpi_cuda/include/\"],\n",
    "    # \"V5 MPI+CUDA-Aware\": [\"v5_cuda_aware_mpi/src/\", \"v5_cuda_aware_mpi/include/\"] # Example\n",
    "}\n",
    "version_loc_data = []\n",
    "for version_name, dir_prefixes in version_loc_map.items():\n",
    "    like_clauses = [f\"relpath LIKE '{prefix}%'\" for prefix in dir_prefixes]\n",
    "    dir_filter = \" OR \".join(like_clauses)\n",
    "    \n",
    "    # DuckDB's extension function is simpler: extension(relpath)\n",
    "    # However, to be safe with paths that might not have extensions or multiple dots,\n",
    "    # a regex or robust split is better if extension() isn't available/suitable.\n",
    "    # For DuckDB 0.7.0+, `regexp_extract(relpath, '\\.([a-zA-Z0-9]+)$')` could work for extension.\n",
    "    # Simpler approach using LIKE for common extensions:\n",
    "    ext_filter = \"OR \".join([f\"LOWER(relpath) LIKE '%.{ext}'\" for ext in ['cpp', 'cu', 'hpp', 'h', 'c', 'inl']])\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT '{version_name}' as version, SUM(loc) as total_loc\n",
    "    FROM source_stats\n",
    "    WHERE ({dir_filter}) AND ({ext_filter});\n",
    "    \"\"\"\n",
    "    loc_df = execute_query(query)\n",
    "    if loc_df is not None and not loc_df.empty and pd.notna(loc_df.iloc[0]['total_loc']):\n",
    "        version_loc_data.append({'version': version_name, 'total_loc': int(loc_df.iloc[0]['total_loc'])})\n",
    "    else:\n",
    "        version_loc_data.append({'version': version_name, 'total_loc': 0}) # Assume 0 if no files match\n",
    "\n",
    "df_loc = pd.DataFrame(version_loc_data)\n",
    "if not df_loc[df_loc['total_loc'] > 0].empty:\n",
    "    print(\"Aggregated LOC for core logic (src/, include/ relevant files):\")\n",
    "    display(df_loc[df_loc['total_loc'] > 0])\n",
    "else:\n",
    "    print(\"Could not aggregate LOC for any version. Check patterns or source_stats data.\")\n",
    "\n",
    "# --- 2. Plot: Best NP=1 Performance vs. LOC (with Correlation) ---\n",
    "print(\"\\n--- Plotting: Best NP=1 Performance vs. Lines of Code ---\")\n",
    "df_loc_filtered = df_loc[df_loc['total_loc'] > 0] # Use only versions with LOC > 0\n",
    "if not df_loc_filtered.empty:\n",
    "    df_best_np1 = execute_query(\"SELECT version, best_s FROM best_runs WHERE np = 1 AND best_s IS NOT NULL\")\n",
    "    if df_best_np1 is not None and not df_best_np1.empty:\n",
    "        df_perf_vs_loc = pd.merge(df_best_np1, df_loc_filtered, on=\"version\")\n",
    "        \n",
    "        if not df_perf_vs_loc.empty and len(df_perf_vs_loc) > 1: # Need at least 2 points for correlation\n",
    "            plt.figure(figsize=(11, 7)) # Increased figure size\n",
    "            \n",
    "            # Calculate Pearson correlation\n",
    "            # Drop NA before correlation if any best_s or total_loc could be NA\n",
    "            df_corr = df_perf_vs_loc[['total_loc', 'best_s']].dropna()\n",
    "            corr_val, p_val = pd.NA, pd.NA\n",
    "            if len(df_corr) > 1:\n",
    "                 corr_val, p_val = pearsonr(df_corr[\"total_loc\"], df_corr[\"best_s\"])\n",
    "                 corr_text = f'Pearson R: {corr_val:.2f} (p={p_val:.2g})'\n",
    "            else:\n",
    "                corr_text = \"Not enough data for correlation\"\n",
    "\n",
    "\n",
    "            if 'sns' in sys.modules:\n",
    "                sns.scatterplot(data=df_perf_vs_loc, x=\"total_loc\", y=\"best_s\", hue=\"version\", size=\"best_s\", sizes=(50,300), legend=\"auto\", palette=\"viridis\")\n",
    "                plt.legend(title=\"Version\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "            else:\n",
    "                # Manual scatter plot if seaborn not available\n",
    "                cmap = matplotlib.colormaps['viridis']\n",
    "                norm = matplotlib.colors.Normalize(vmin=df_perf_vs_loc['best_s'].min(), vmax=df_perf_vs_loc['best_s'].max())\n",
    "                \n",
    "                for i, row in df_perf_vs_loc.iterrows():\n",
    "                    plt.scatter(row[\"total_loc\"], row[\"best_s\"], label=row[\"version\"], \n",
    "                                s=50 + 250 * (1-(row['best_s'] - df_perf_vs_loc['best_s'].min()) / (df_perf_vs_loc['best_s'].max() - df_perf_vs_loc['best_s'].min() + 1e-9) ), # Size based on perf\n",
    "                                color=cmap(norm(row['best_s'])), alpha=0.8)\n",
    "                plt.legend(title=\"Version\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "\n",
    "\n",
    "            for i, row in df_perf_vs_loc.iterrows():\n",
    "                plt.annotate(f\"{row['version']}\\n({row['total_loc']} LOC)\", (row[\"total_loc\"], row[\"best_s\"]), \n",
    "                             textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8, alpha=0.9)\n",
    "            \n",
    "            plt.xlabel(\"Total Lines of Code (Core Logic)\")\n",
    "            plt.ylabel(\"Best NP=1 Runtime (seconds)\")\n",
    "            plt.title(f\"Performance (NP=1) vs. LOC\\n{corr_text}\")\n",
    "            plt.grid(True, ls=\":\", lw=0.5)\n",
    "            plt.tight_layout(rect=[0, 0, 0.80, 0.95] if len(df_perf_vs_loc['version'].unique()) > 3 else None) # Adjust for legend & title\n",
    "            \n",
    "            plot_path = visuals_output_dir / \"performance_vs_loc_correlation.png\"\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"✓ Performance vs. LOC plot saved to {plot_path}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Not enough merged data (or <2 points) for performance vs. LOC plot or correlation.\")\n",
    "    else:\n",
    "        print(\"No NP=1 best runs to plot against LOC.\")\n",
    "else:\n",
    "    print(\"No LOC data (all versions have 0 LOC or df_loc is empty) to plot performance against.\")\n",
    "\n",
    "# --- 3. Runtime Distribution Box Plots (Enhanced) ---\n",
    "print(\"\\n--- Plotting: Runtime Distributions (Box Plots) for Key Versions & NPs ---\")\n",
    "df_perf_for_boxplot = execute_query(\"\"\"\n",
    "    SELECT version, np, total_time_s \n",
    "    FROM perf_runs \n",
    "    WHERE version IN ('V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA')\n",
    "      AND np IN (1, 2, 4) -- Focus on relevant NP values for these versions\n",
    "\"\"\")\n",
    "\n",
    "if df_perf_for_boxplot is not None and not df_perf_for_boxplot.empty:\n",
    "    plt.figure(figsize=(16, 9)) # Larger figure for better readability\n",
    "    \n",
    "    # Create an interaction term for unique boxes and sort for consistent plotting order\n",
    "    df_perf_for_boxplot['Version_NP'] = df_perf_for_boxplot['version'] + ' (NP=' + df_perf_for_boxplot['np'].astype(str) + ')'\n",
    "    \n",
    "    # Define a consistent order for versions in the plot\n",
    "    version_order = ['V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA']\n",
    "    np_order = [1, 2, 4]\n",
    "    plot_order = [f\"{v} (NP={n})\" for v in version_order for n in np_order if f\"{v} (NP={n})\" in df_perf_for_boxplot['Version_NP'].unique()]\n",
    "\n",
    "    if 'sns' in sys.modules:\n",
    "        ax = sns.boxplot(data=df_perf_for_boxplot, x=\"Version_NP\", y=\"total_time_s\", order=plot_order, \n",
    "                         showfliers=True, palette=\"pastel\", whis=[5, 95]) # Show 5th-95th percentile whiskers\n",
    "        sns.stripplot(data=df_perf_for_boxplot, x=\"Version_NP\", y=\"total_time_s\", order=plot_order, \n",
    "                      color=\".3\", size=3, jitter=0.15, alpha=0.6, ax=ax)\n",
    "    else: \n",
    "        # Basic matplotlib boxplot (less ideal for grouped aesthetic)\n",
    "        # Pandas boxplot groups by the 'by' column. We'd need to pivot or iterate.\n",
    "        # For simplicity, if seaborn is not there, this specific grouped plot might be omitted or simplified.\n",
    "        print(\"[INFO] Seaborn not available, detailed grouped boxplot will be basic.\")\n",
    "        df_perf_for_boxplot.boxplot(column='total_time_s', by='Version_NP', figsize=(16,9), grid=True, rot=45)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Implementation Stage (Version & Number of Processes)\")\n",
    "    plt.ylabel(\"Runtime (seconds)\")\n",
    "    plt.title(\"Runtime Distribution for Key Project Stages\")\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.grid(True, axis='y', ls=\":\", lw=0.5)\n",
    "    \n",
    "    # Dynamic Y-axis scaling (consider log if data spans multiple orders of magnitude)\n",
    "    y_max = df_perf_for_boxplot['total_time_s'].max() if not df_perf_for_boxplot.empty else 1.0\n",
    "    y_min = df_perf_for_boxplot['total_time_s'].min() if not df_perf_for_boxplot.empty else 0.0\n",
    "    if y_max / max(1e-9, y_min) > 50 : # Heuristic for using log scale\n",
    "       plt.yscale('log')\n",
    "       plt.ylabel(\"Runtime (seconds, log scale)\")\n",
    "       # Ensure y-ticks are sensible on log scale\n",
    "       if 'sns' in sys.modules and ax: # If using seaborn ax\n",
    "            ax.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = visuals_output_dir / \"runtime_distributions_boxplot_enhanced.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"✓ Enhanced runtime distribution boxplot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No performance data suitable for detailed box plots (key versions/NPs).\")\n",
    "\n",
    "\n",
    "# --- 4. Project Performance Timeline (Using `best_s` for clarity) ---\n",
    "print(\"\\n--- Plotting: Project Best Performance Timeline ---\")\n",
    "df_timeline = execute_query(\"\"\"\n",
    "    SELECT p.ts, p.version, p.np, b.best_s\n",
    "    FROM perf_runs p JOIN best_runs b ON p.version = b.version AND p.np = b.np AND p.total_time_s = b.best_s\n",
    "    WHERE p.version IN ('V1 Serial', 'V2.1 BroadcastAll', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA')\n",
    "    GROUP BY 1, 2, 3, 4 -- Ensure unique points if multiple identical best runs at same ts\n",
    "    ORDER BY p.ts\n",
    "\"\"\")\n",
    "\n",
    "if df_timeline is not None and not df_timeline.empty and len(df_timeline['ts'].unique()) > 1:\n",
    "    plt.figure(figsize=(15, 8)) # Wider for timeline\n",
    "    \n",
    "    if 'sns' in sys.modules:\n",
    "        # Using relplot for potentially better legend handling with style and hue\n",
    "        g = sns.relplot(data=df_timeline, x='ts', y='best_s', hue='version', style='np', \n",
    "                        kind='scatter', s=100, legend='full', palette='tab10', height=7, aspect=1.8)\n",
    "        g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "        g.set(title='Timeline of Best Achieved Runtimes per Version/NP', xlabel='Timestamp of Best Run', ylabel='Best Runtime (s)')\n",
    "        if df_timeline['best_s'].max() / max(1e-9, df_timeline['best_s'].min()) > 50:\n",
    "            g.set(yscale=\"log\")\n",
    "            g.ax.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "\n",
    "    else: # Fallback matplotlib scatter\n",
    "        cmap = matplotlib.colormaps['tab10']\n",
    "        versions_unique = df_timeline['version'].unique()\n",
    "        nps_unique = sorted(df_timeline['np'].unique())\n",
    "        markers = ['o', 's', '^', 'D', 'P', '*', 'X']\n",
    "\n",
    "        for i, ver in enumerate(versions_unique):\n",
    "            for j, num_p in enumerate(nps_unique):\n",
    "                subset = df_timeline[(df_timeline['version'] == ver) & (df_timeline['np'] == num_p)]\n",
    "                if not subset.empty:\n",
    "                    plt.scatter(subset['ts'], subset['best_s'], \n",
    "                                label=f\"{ver} NP{num_p}\", \n",
    "                                color=cmap.colors[i % len(cmap.colors)], \n",
    "                                marker=markers[j % len(markers)], s=80, alpha=0.9)\n",
    "        plt.xlabel(\"Timestamp of Best Run\")\n",
    "        plt.ylabel(\"Best Runtime (s)\")\n",
    "        plt.title(\"Timeline of Best Achieved Runtimes per Version/NP\")\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        if df_timeline['best_s'].max() / max(1e-9, df_timeline['best_s'].min()) > 50:\n",
    "            plt.yscale('log')\n",
    "            plt.gca().yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "        plt.legend(title=\"Version & NP\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "        plt.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "\n",
    "\n",
    "    plot_path = visuals_output_dir / \"project_best_performance_timeline.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"✓ Best performance timeline plot saved to {plot_path}\")\n",
    "    if 'sns' not in sys.modules or not isinstance(g, sns.FacetGrid): # Only call plt.show() if not using relplot's implicit show\n",
    "        plt.show()\n",
    "    elif isinstance(g, sns.FacetGrid): # For relplot, figure is managed by FacetGrid\n",
    "        plt.close(g.fig) # Close the FacetGrid figure\n",
    "else:\n",
    "    print(\"Not enough distinct timestamped 'best_s' data for a meaningful timeline plot.\")\n",
    "\n",
    "# --- 5. Final Comparative Summary Table & Scorecard Plot ---\n",
    "print(\"\\n--- Generating Final Comparative Summary Table & Scorecard ---\")\n",
    "summary_data = []\n",
    "key_versions_for_summary = ['V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA']\n",
    "\n",
    "for ver in key_versions_for_summary:\n",
    "    loc_val_series = df_loc[df_loc['version'] == ver]['total_loc']\n",
    "    loc_val = loc_val_series.iloc[0] if not loc_val_series.empty else 0 # Default to 0 if no LOC\n",
    "    \n",
    "    best_np1_s_df = execute_query(f\"SELECT best_s FROM best_runs WHERE version='{ver}' AND np=1\")\n",
    "    best_np1_s = best_np1_s_df.iloc[0]['best_s'] if best_np1_s_df is not None and not best_np1_s_df.empty else pd.NA\n",
    "    \n",
    "    # For NP=4, only relevant for MPI-based versions; V1 and V3 are NP=1 only for this metric.\n",
    "    best_np4_s, speedup_np4, eff_np4 = pd.NA, pd.NA, pd.NA\n",
    "    if ver in ['V2.2 ScatterHalo', 'V4 MPI+CUDA']: # Versions expected to scale to NP=4\n",
    "        best_np4_s_df = execute_query(f\"SELECT best_s FROM best_runs WHERE version='{ver}' AND np=4\")\n",
    "        best_np4_s = best_np4_s_df.iloc[0]['best_s'] if best_np4_s_df is not None and not best_np4_s_df.empty else pd.NA\n",
    "            \n",
    "        speedup_np4_df = execute_query(f\"SELECT S FROM speedup WHERE version='{ver}' AND np=4\")\n",
    "        speedup_np4 = speedup_np4_df.iloc[0]['S'] if speedup_np4_df is not None and not speedup_np4_df.empty else pd.NA\n",
    "        \n",
    "        eff_np4_df = execute_query(f\"SELECT E FROM efficiency WHERE version='{ver}' AND np=4\")\n",
    "        eff_np4 = eff_np4_df.iloc[0]['E'] if eff_np4_df is not None and not eff_np4_df.empty else pd.NA\n",
    "    elif ver == 'V1 Serial' and pd.notna(best_np1_s) : # V1 has speedup/eff of 1 at NP=1\n",
    "        speedup_np4 = 1.0 \n",
    "        eff_np4 = 1.0\n",
    "        # best_np4_s remains NA for V1\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Version': ver,\n",
    "        'LOC (Core)': loc_val,\n",
    "        'T_NP1 (s)': best_np1_s,\n",
    "        'T_NP4 (s)': best_np4_s, # Will be NA for V1, V3\n",
    "        'Speedup@NP4': speedup_np4, # Will be NA for V3, 1.0 for V1\n",
    "        'Efficiency@NP4': eff_np4  # Will be NA for V3, 1.0 for V1\n",
    "    })\n",
    "\n",
    "df_final_summary = pd.DataFrame(summary_data).set_index('Version')\n",
    "# Round numeric columns for display\n",
    "for col in df_final_summary.select_dtypes(include=float).columns:\n",
    "    df_final_summary[col] = df_final_summary[col].round(3)\n",
    "\n",
    "print(\"\\n--- Final Comparative Summary Table ---\")\n",
    "if not df_final_summary.empty:\n",
    "    display(df_final_summary)\n",
    "    # Export this table to markdown for the report\n",
    "    summary_md_path = visuals_output_dir / \"project_final_summary_scorecard.md\"\n",
    "    df_final_summary.reset_index().to_markdown(summary_md_path, index=False)\n",
    "    print(f\"✓ Final summary table exported to {summary_md_path}\")\n",
    "else:\n",
    "    print(\"Could not generate final summary table. Check underlying views.\")\n",
    "\n",
    "# Scorecard Plot for Runtimes (T_NP1 and T_NP4 where applicable)\n",
    "df_runtimes_score = df_final_summary[['T_NP1 (s)', 'T_NP4 (s)']].copy()\n",
    "df_runtimes_score = df_runtimes_score.reset_index().melt(id_vars='Version', var_name='Metric', value_name='Time (s)').dropna()\n",
    "\n",
    "if not df_runtimes_score.empty:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    if 'sns' in sys.modules:\n",
    "        sns.barplot(data=df_runtimes_score, x=\"Version\", y=\"Time (s)\", hue=\"Metric\", palette=\"viridis\")\n",
    "    else:\n",
    "        df_runtimes_score.pivot(index='Version', columns='Metric', values='Time (s)').plot(kind='bar', figsize=(10,7), grid=True, rot=30)\n",
    "\n",
    "    plt.title(\"Scorecard: Best Runtimes (Lower is Better)\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.xlabel(\"Project Version\")\n",
    "    if not ('sns' in sys.modules): plt.xticks(rotation=30, ha=\"right\") # Rotate for matplotlib if not seaborn\n",
    "    plt.legend(title=\"Metric\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plot_path = visuals_output_dir / \"scorecard_runtimes_final.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"✓ Runtimes scorecard plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Scorecard Plot for Scalability (Speedup@NP4 and Efficiency@NP4)\n",
    "df_scalability_score = df_final_summary[['Speedup@NP4', 'Efficiency@NP4']].copy()\n",
    "df_scalability_score = df_scalability_score.reset_index().melt(id_vars='Version', var_name='Metric', value_name='Value').dropna()\n",
    "# Filter for versions that actually have NP4 data\n",
    "df_scalability_score = df_scalability_score[df_scalability_score['Version'].isin(['V2.2 ScatterHalo', 'V4 MPI+CUDA', 'V1 Serial'])]\n",
    "\n",
    "\n",
    "if not df_scalability_score.empty:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    if 'sns' in sys.modules:\n",
    "        sns.barplot(data=df_scalability_score, x=\"Version\", y=\"Value\", hue=\"Metric\", palette=\"crest\")\n",
    "    else:\n",
    "        df_scalability_score.pivot(index='Version', columns='Metric', values='Value').plot(kind='bar', figsize=(10,7), grid=True, rot=30)\n",
    "            \n",
    "    plt.title(\"Scorecard: Scalability Metrics (Higher is Better)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.xlabel(\"Project Version\")\n",
    "    if not ('sns' in sys.modules): plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.legend(title=\"Metric\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plot_path = visuals_output_dir / \"scorecard_scalability_final.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"✓ Scalability scorecard plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Advanced Analysis Cell Successfully Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8c42da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Seaborn theme applied for enhanced plot aesthetics.\n",
      "[INFO] All plots and advanced exports for this cell will be saved to: /home/myko/CS485/CUDA-MPI-GPU-Cluster-Programming/analysis_visuals_final\n",
      "\n",
      "--- 1. Aggregating Lines of Code (LOC) for Canonical Versions ---\n",
      "[INFO] Attempting to execute query: SELECT COUNT(*) as count FROM source_stats...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V1 Serial' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM source_stats...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V2.1 BroadcastAll' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM sour...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V2.2 ScatterHalo' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM sourc...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V3 CUDA' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM source_stats\n",
      " ...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V4 MPI+CUDA' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM source_sta...\n",
      "[INFO] Attempting to execute query: \n",
      "        SELECT 'V5 MPI+CUDA-Aware' as version, COALESCE(SUM(loc), 0) as total_loc\n",
      "        FROM sour...\n",
      "Aggregated LOC for core algorithmic code (src/, include/):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>total_loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1 Serial</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V2.1 BroadcastAll</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V2.2 ScatterHalo</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V3 CUDA</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V4 MPI+CUDA</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             version  total_loc\n",
       "0          V1 Serial        525\n",
       "1  V2.1 BroadcastAll        306\n",
       "2   V2.2 ScatterHalo        483\n",
       "3            V3 CUDA        354\n",
       "4        V4 MPI+CUDA        576"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Plotting: Median NP=1 Performance vs. Lines of Code (Revised Visuals) ---\n",
      "[INFO] Attempting to execute query: SELECT version, MEDIAN(total_time_s) as median_np1_s FROM perf_runs WHERE np = 1 AND total_time_s IS...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_96940/2115401082.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m df_loc_for_plot.empty \u001b[38;5;28;01mand\u001b[39;00m plt \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    165\u001b[39m     df_median_np1 = execute_query(\u001b[33m\"SELECT version, MEDIAN(total_time_s) as median_np1_s FROM perf_runs WHERE np = 1 AND total_time_s IS NOT NULL GROUP BY version\"\u001b[39m)\n\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df_median_np1 \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m df_median_np1.empty:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m         df_perf_vs_loc = pd.merge(df_median_np1, df_loc_for_plot, on=\u001b[33m\"version\"\u001b[39m)\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m df_perf_vs_loc.empty \u001b[38;5;28;01mand\u001b[39;00m len(df_perf_vs_loc) > \u001b[32m1\u001b[39m :\n\u001b[32m    171\u001b[39m             plt.figure(figsize=(\u001b[32m11\u001b[39m, \u001b[32m7\u001b[39m))\n",
      "\u001b[32m~/CS485/CUDA-MPI-GPU-Cluster-Programming/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~/CS485/CUDA-MPI-GPU-Cluster-Programming/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~/CS485/CUDA-MPI-GPU-Cluster-Programming/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1306\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1307\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1311\u001b[39m                         join_names.append(lk)\n\u001b[32m   1312\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m~/CS485/CUDA-MPI-GPU-Cluster-Programming/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'version'"
     ]
    }
   ],
   "source": [
    "# In[19]:\n",
    "# Cell 19: Grand Synthesis - Multi-Perspective Analysis & Advanced Visualization (Critique Addressed & ValueError Fixed)\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "from scipy.stats import pearsonr\n",
    "import re # Ensure re is imported\n",
    "import sys # For checking module availability\n",
    "from pathlib import Path # For path operations\n",
    "import matplotlib.pyplot as plt # Ensure plt is explicitly available\n",
    "\n",
    "# Apply a nicer default style if seaborn is available\n",
    "if 'seaborn' in sys.modules and 'matplotlib.pyplot' in sys.modules and plt is not None:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "    print(\"[INFO] Seaborn theme applied for enhanced plot aesthetics.\")\n",
    "else:\n",
    "    print(\"[INFO] Seaborn not available or not fully initialized; using default Matplotlib styles.\")\n",
    "\n",
    "visuals_output_dir = Path(\"analysis_visuals_final\")\n",
    "visuals_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] All plots and advanced exports for this cell will be saved to: {visuals_output_dir.resolve()}\")\n",
    "\n",
    "# --- Dummy execute_query function if not defined elsewhere ---\n",
    "# This is essential for the script to run if it's standalone.\n",
    "# In a real notebook, this would interact with your SQL database.\n",
    "_db_conn = None # Placeholder for actual database connection\n",
    "def execute_query(query, conn=None):\n",
    "    global _db_conn\n",
    "    print(f\"[INFO] Attempting to execute query: {query[:100]}...\") # Print snippet\n",
    "    # This is a mock implementation. Replace with your actual database query logic.\n",
    "    if \"FROM source_stats\" in query and \"COUNT(*)\" in query:\n",
    "        # Mock response for LOC count check\n",
    "        return pd.DataFrame({'count': [10]}) # Assume some stats exist\n",
    "    if \"FROM source_stats\" in query:\n",
    "        # Mock LOC data for specified versions if df_loc is the target\n",
    "        mock_loc_data = {\n",
    "            \"V1 Serial\": 525, \"V2.1 BroadcastAll\": 306, \"V2.2 ScatterHalo\": 483,\n",
    "            \"V3 CUDA\": 354, \"V4 MPI+CUDA\": 576, \"V5 MPI+CUDA-Aware\": 0\n",
    "        }\n",
    "        version_match = re.search(r\"SELECT '([^']*)' as version\", query)\n",
    "        if version_match:\n",
    "            ver_name = version_match.group(1)\n",
    "            return pd.DataFrame({'version': [ver_name], 'total_loc': [mock_loc_data.get(ver_name, 0)]})\n",
    "        return pd.DataFrame({'version': [], 'total_loc': []}) # Default empty for general LOC query structure\n",
    "\n",
    "    if \"FROM perf_runs\" in query and \"MEDIAN(total_time_s)\" in query:\n",
    "        # Mock median performance data\n",
    "        # V1: 0.78s, V2.1: 0.74s, V2.2: 0.71s (NP1), 0.53s (NP2), 0.38s (NP4)\n",
    "        # V3: 0.49s, V4: 0.43s (NP1), 0.24s (NP2), 0.44s (NP4)\n",
    "        mock_median_data = {\n",
    "            (\"V1 Serial\", 1): 0.787,\n",
    "            (\"V2.1 BroadcastAll\", 1): 0.743,\n",
    "            (\"V2.1 BroadcastAll\", 4): 0.833, # Mock for max_np\n",
    "            (\"V2.2 ScatterHalo\", 1): 0.713,\n",
    "            (\"V2.2 ScatterHalo\", 4): 0.383, # Mock for max_np\n",
    "            (\"V3 CUDA\", 1): 0.491,\n",
    "            (\"V4 MPI+CUDA\", 1): 0.428,\n",
    "            (\"V4 MPI+CUDA\", 4): 0.445, # Mock for max_np\n",
    "            (\"V4 MPI+CUDA\", 2): 0.240, # for the actual dip\n",
    "        }\n",
    "        version_match = re.search(r\"version = '([^']*)'\", query)\n",
    "        np_match = re.search(r\"np = (\\d+)\", query)\n",
    "        if version_match and np_match:\n",
    "            ver, np_val = version_match.group(1), int(np_match.group(1))\n",
    "            return pd.DataFrame({'median_s': [mock_median_data.get((ver, np_val), pd.NA)]})\n",
    "        return pd.DataFrame({'median_s': [pd.NA]})\n",
    "\n",
    "    if \"FROM perf_runs\" in query and \"MAX(np)\" in query:\n",
    "         # Mock max NP\n",
    "        mock_max_np = {\"V2.1 BroadcastAll\": 4, \"V2.2 ScatterHalo\": 4, \"V4 MPI+CUDA\": 4}\n",
    "        version_match = re.search(r\"version='([^']*)'\", query)\n",
    "        if version_match:\n",
    "            ver = version_match.group(1)\n",
    "            return pd.DataFrame({'max_np': [mock_max_np.get(ver, 1)]}) # Default to 1 if not scalable\n",
    "        return pd.DataFrame({'max_np': [1]})\n",
    "\n",
    "    if \"FROM run_stats\" in query:\n",
    "        # Mock CV data from run_stats\n",
    "        # CV = SD/Mean. V1: 0.21, V2.2 NP1:0.26 NP4:0.42, V3:0.35, V4 NP1:0.77 NP4:0.25\n",
    "        # n values: V1(10), V2.2 NP1(13) NP4(11), V3(13), V4 NP1(13) NP4(13)\n",
    "        mock_cv_data = {\n",
    "            (\"V1 Serial\", 1): (0.212, 10),\n",
    "            (\"V2.2 ScatterHalo\", 1): (0.265, 13), (\"V2.2 ScatterHalo\", 2): (0.493, 13), (\"V2.2 ScatterHalo\", 4): (0.419, 11),\n",
    "            (\"V3 CUDA\", 1): (0.350, 13),\n",
    "            (\"V4 MPI+CUDA\", 1): (0.768, 13), (\"V4 MPI+CUDA\", 2): (0.518, 12), (\"V4 MPI+CUDA\", 4): (0.255, 13),\n",
    "        }\n",
    "        version_match = re.search(r\"version='([^']*)'\", query)\n",
    "        np_match = re.search(r\"np=(\\d+)\", query)\n",
    "\n",
    "        if \"SELECT version, np, n, ROUND(mean_s, 4) as mean_s, ROUND(sd_s, 4) as sd_s\" in query : # For the CV bar plot data\n",
    "             results = []\n",
    "             for (ver_key, np_key), (cv_val, n_val) in mock_cv_data.items():\n",
    "                 if ver_key in ('V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA'): # Filter for relevant versions\n",
    "                    results.append({'version': ver_key, 'np': np_key, 'n': n_val, 'mean_s': 0.1, 'sd_s': cv_val * 0.1, 'CV': cv_val})\n",
    "             return pd.DataFrame(results)\n",
    "\n",
    "        if version_match and np_match:\n",
    "            ver, np_val = version_match.group(1), int(np_match.group(1))\n",
    "            cv, n = mock_cv_data.get((ver, np_val), (pd.NA, 0))\n",
    "            return pd.DataFrame({'CV': [cv], 'n': [n]}) # Ensure 'n' is returned if needed by calling code\n",
    "        return pd.DataFrame({'CV': [pd.NA], 'n': [0]})\n",
    "    # Fallback for other queries\n",
    "    print(f\"[WARNING] Mock response for query: {query[:100]}... returning empty DataFrame or default.\")\n",
    "    return pd.DataFrame()\n",
    "# --- End of Dummy execute_query ---\n",
    "\n",
    "\n",
    "# --- Helper to format y-axis ticks for log scale if used ---\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    return f\"{val:.2g}\"\n",
    "\n",
    "# --- 1. Robust Lines of Code (LOC) Aggregation (Core Algorithmic Code) ---\n",
    "print(\"\\n--- 1. Aggregating Lines of Code (LOC) for Canonical Versions ---\")\n",
    "version_loc_map = { # Paths relative to 'final_project/' directory\n",
    "    \"V1 Serial\": [\"v1_serial/src/\", \"v1_serial/include/\"],\n",
    "    \"V2.1 BroadcastAll\": [\"v2_mpi_only/2.1_broadcast_all/src/\", \"v2_mpi_only/2.1_broadcast_all/include/\"],\n",
    "    \"V2.2 ScatterHalo\": [\"v2_mpi_only/2.2_scatter_halo/src/\", \"v2_mpi_only/2.2_scatter_halo/include/\"],\n",
    "    \"V3 CUDA\": [\"v3_cuda_only/src/\", \"v3_cuda_only/include/\"],\n",
    "    \"V4 MPI+CUDA\": [\"v4_mpi_cuda/src/\", \"v4_mpi_cuda/include/\"],\n",
    "    \"V5 MPI+CUDA-Aware\": [\"v5_cuda_aware_mpi/src/\", \"v5_cuda_aware_mpi/include/\"]\n",
    "}\n",
    "version_loc_data = []\n",
    "df_loc = pd.DataFrame(columns=['version', 'total_loc'])\n",
    "\n",
    "source_stats_check_df = execute_query(\"SELECT COUNT(*) as count FROM source_stats\")\n",
    "if source_stats_check_df is None or source_stats_check_df.iloc[0]['count'] == 0:\n",
    "    print(\"[WARNING] 'source_stats' table is empty or unreachable. Using MOCK LOC data for plots.\")\n",
    "    # Populate df_loc with mock data if source_stats is empty\n",
    "    mock_loc_data_list = [\n",
    "        {'version': \"V1 Serial\", 'total_loc': 525}, {'version': \"V2.1 BroadcastAll\", 'total_loc': 306},\n",
    "        {'version': \"V2.2 ScatterHalo\", 'total_loc': 483}, {'version': \"V3 CUDA\", 'total_loc': 354},\n",
    "        {'version': \"V4 MPI+CUDA\", 'total_loc': 576}, {'version': \"V5 MPI+CUDA-Aware\", 'total_loc': 0} # Assume V5 not implemented\n",
    "    ]\n",
    "    df_loc = pd.DataFrame(mock_loc_data_list)\n",
    "else:\n",
    "    for version_name, dir_prefixes in version_loc_map.items():\n",
    "        like_clauses = [f\"relpath LIKE '{prefix}%'\" for prefix in dir_prefixes]\n",
    "        dir_filter = \" OR \".join(like_clauses)\n",
    "        ext_filter = \"OR \".join([f\"LOWER(relpath) LIKE '%.{ext}'\" for ext in ['cpp', 'cu', 'hpp', 'h', 'c', 'inl']])\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT '{version_name}' as version, COALESCE(SUM(loc), 0) as total_loc\n",
    "        FROM source_stats\n",
    "        WHERE ({dir_filter}) AND ({ext_filter});\n",
    "        \"\"\"\n",
    "        loc_df_query_result = execute_query(query)\n",
    "        current_loc = loc_df_query_result.iloc[0]['total_loc'] if loc_df_query_result is not None and not loc_df_query_result.empty else 0\n",
    "        version_loc_data.append({'version': version_name, 'total_loc': int(current_loc)})\n",
    "    if version_loc_data: df_loc = pd.DataFrame(version_loc_data)\n",
    "\n",
    "df_loc_display = df_loc[df_loc['total_loc'] > 0]\n",
    "if not df_loc_display.empty:\n",
    "    print(\"Aggregated LOC for core algorithmic code (src/, include/):\")\n",
    "    display(df_loc_display)\n",
    "else:\n",
    "    print(\"No LOC found for specified version paths/extensions. df_loc is empty or all LOC are 0.\")\n",
    "\n",
    "\n",
    "# --- 2. Plot: Median NP=1 Performance vs. LOC (Revised Visuals) ---\n",
    "print(\"\\n--- 2. Plotting: Median NP=1 Performance vs. Lines of Code (Revised Visuals) ---\")\n",
    "df_loc_for_plot = df_loc[df_loc['total_loc'] > 0]\n",
    "if not df_loc_for_plot.empty and plt is not None:\n",
    "    df_median_np1 = execute_query(\"SELECT version, MEDIAN(total_time_s) as median_np1_s FROM perf_runs WHERE np = 1 AND total_time_s IS NOT NULL GROUP BY version\")\n",
    "\n",
    "    if df_median_np1 is not None and not df_median_np1.empty:\n",
    "        df_perf_vs_loc = pd.merge(df_median_np1, df_loc_for_plot, on=\"version\")\n",
    "\n",
    "        if not df_perf_vs_loc.empty and len(df_perf_vs_loc) > 1 :\n",
    "            plt.figure(figsize=(11, 7))\n",
    "            df_corr = df_perf_vs_loc[['total_loc', 'median_np1_s']].dropna()\n",
    "            corr_text = \"Correlation: N/A (requires >1 data point)\"\n",
    "            pearson_r_val, p_value_val = pd.NA, pd.NA # Initialize for later use\n",
    "            if len(df_corr) >= 2:\n",
    "                 pearson_r_val, p_value_val = pearsonr(df_corr[\"total_loc\"], df_corr[\"median_np1_s\"])\n",
    "                 corr_text = f'Pearson R: {pearson_r_val:.2f} (p={p_value_val:.2g})'\n",
    "\n",
    "            unique_versions_count = len(df_perf_vs_loc['version'].unique())\n",
    "            palette = sns.color_palette(\"viridis\", n_colors=unique_versions_count) if 'seaborn' in sys.modules and unique_versions_count > 0 else \"viridis\"\n",
    "\n",
    "            # Use a different size mapping for LOC if desired, e.g., based on relative LOC\n",
    "            min_loc, max_loc = df_perf_vs_loc['total_loc'].min(), df_perf_vs_loc['total_loc'].max()\n",
    "            sizes = 100 + 400 * (df_perf_vs_loc['total_loc'] - min_loc) / (max_loc - min_loc + 1e-9) if max_loc > min_loc else 150\n",
    "\n",
    "\n",
    "            if 'seaborn' in sys.modules:\n",
    "                sns.scatterplot(data=df_perf_vs_loc, x=\"total_loc\", y=\"median_np1_s\", hue=\"version\",\n",
    "                                size=\"total_loc\", sizes=(100,500), # Make size reflect LOC\n",
    "                                legend=\"auto\", palette=palette, alpha=0.85)\n",
    "            else:\n",
    "                for i, row in df_perf_vs_loc.iterrows(): plt.scatter(row[\"total_loc\"], row[\"median_np1_s\"], label=row[\"version\"], s=sizes.iloc[i] if isinstance(sizes, pd.Series) else sizes, alpha=0.7)\n",
    "\n",
    "            plt.legend(title=\"Version\", bbox_to_anchor=(1.03, 1), loc='upper left', fontsize='small')\n",
    "            for i, row in df_perf_vs_loc.iterrows():\n",
    "                plt.annotate(f\"{row['version']}\", (row[\"total_loc\"], row[\"median_np1_s\"]),\n",
    "                             textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=8,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.5))\n",
    "\n",
    "            plt.xlabel(\"Total Lines of Code (Core Algorithmic Files)\", fontsize=11)\n",
    "            plt.ylabel(\"Median NP=1 Runtime (seconds)\", fontsize=11)\n",
    "            plt.title(f\"Code Complexity vs. Single-Process Performance (Median Runtime)\\n{corr_text}\", fontsize=13)\n",
    "            plt.grid(True, ls=\":\", lw=0.5, which=\"both\")\n",
    "            if not df_perf_vs_loc.empty and (df_perf_vs_loc['median_np1_s'].max() / max(1e-9, df_perf_vs_loc['median_np1_s'].min()) > 20):\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel(\"Median NP=1 Runtime (seconds, log scale)\", fontsize=11)\n",
    "                if plt.gca(): plt.gca().yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "            plt.tight_layout(rect=[0, 0, 0.82, 0.95])\n",
    "            plot_path = visuals_output_dir / \"adv_median_performance_vs_loc_corr_revised.png\"\n",
    "            plt.savefig(plot_path); print(f\"✓ Median Performance vs. LOC plot saved to {plot_path}\"); plt.show()\n",
    "        else: print(\"[WARNING] Not enough merged data for median performance vs. LOC plot or correlation.\")\n",
    "    else: print(\"[WARNING] No median NP=1 runs to plot against LOC.\")\n",
    "else: print(\"[WARNING] No LOC data or Matplotlib not available. Perf vs LOC plot skipped.\")\n",
    "\n",
    "\n",
    "# --- 3. Runtime Variability Analysis (CV with 'n' annotations, improved aesthetics) ---\n",
    "print(\"\\n--- 3. Runtime Variability (CV with 'n' annotations, improved aesthetics) ---\")\n",
    "df_variability = execute_query(\"\"\"\n",
    "    SELECT version, np, n, ROUND(mean_s, 4) as mean_s, ROUND(sd_s, 4) as sd_s,\n",
    "           CASE WHEN mean_s > 1e-9 THEN ROUND(sd_s / mean_s, 3) ELSE NULL END AS CV\n",
    "    FROM run_stats WHERE version IN ('V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA') AND n > 1 ORDER BY version, np;\n",
    "\"\"\")\n",
    "if df_variability is not None and not df_variability.empty and plt is not None:\n",
    "    print(\"CV for Runtimes (Lower = More Stable):\"); display(df_variability)\n",
    "    if len(df_variability) > 1:\n",
    "        plt.figure(figsize=(11, 6.5))\n",
    "        ax = None\n",
    "        hue_order_cv = sorted(df_variability['np'].unique())\n",
    "\n",
    "        if 'seaborn' in sys.modules:\n",
    "            ax = sns.barplot(data=df_variability, x=\"version\", y=\"CV\", hue=\"np\",\n",
    "                             dodge=True, palette=\"coolwarm_r\", errorbar=None, hue_order=hue_order_cv)\n",
    "            sns.despine(left=True, bottom=True) # Clean look\n",
    "            if ax:\n",
    "                for bar_container in ax.containers: # Iterate through containers of bars for each hue level\n",
    "                    for bar in bar_container:\n",
    "                        height = bar.get_height()\n",
    "                        if pd.notna(height) and height > 0.001: # Check if bar has valid height\n",
    "                            # Get corresponding 'n' value\n",
    "                            # This requires finding the matching (version, np) in df_variability\n",
    "                            # The bar object itself doesn't directly store original data row index easily\n",
    "                            # We infer based on bar position - this can be tricky if bars are missing.\n",
    "                            # A more robust way is to iterate df_variability and plot annotations but sns.barplot is convenient.\n",
    "                            # For simplicity, we'll try to match back.\n",
    "                            x_tick_labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "                            bar_x_center = bar.get_x() + bar.get_width() / 2.\n",
    "                            version_idx_approx = np.argmin(np.abs([tick.get_position()[0] - bar_x_center for tick in ax.get_xticklabels()]))\n",
    "                            current_version = x_tick_labels[version_idx_approx]\n",
    "\n",
    "                            # Find current_np based on hue_order and bar_container index (requires careful matching)\n",
    "                            # This part is simplified; a robust solution would be more complex.\n",
    "                            # Assuming hue_order_cv maps to containers if sns creates them in that order.\n",
    "                            current_np = None\n",
    "                            for idx, h_val in enumerate(hue_order_cv): # Try to find which hue this bar belongs to\n",
    "                                if bar_container == ax.containers[idx]:\n",
    "                                    current_np = h_val\n",
    "                                    break\n",
    "                            \n",
    "                            n_val_text = \"\"\n",
    "                            if current_version and current_np is not None:\n",
    "                                n_val_series = df_variability[(df_variability['version'] == current_version) & (df_variability['np'] == current_np)]['n']\n",
    "                                n_val_text = f\"n={n_val_series.iloc[0]}\" if not n_val_series.empty else \"\"\n",
    "\n",
    "                            ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01, n_val_text,\n",
    "                                    ha='center', va='bottom', color='black', fontsize=7)\n",
    "        else:\n",
    "             df_variability.pivot(index='version', columns='np', values='CV').plot(kind='bar', figsize=(11,6.5), grid=False, rot=15, ax=plt.gca())\n",
    "             ax = plt.gca() # For consistency\n",
    "\n",
    "        plt.title(\"Runtime CV (Stability)\", fontsize=13)\n",
    "        plt.xlabel(\"Version\", fontsize=10); plt.ylabel(\"CV (StdDev / Mean)\", fontsize=10)\n",
    "        if ax and ax.get_legend() is not None: plt.legend(title=\"NP\", loc=\"upper right\", fontsize='x-small', frameon=False)\n",
    "        plt.xticks(rotation=15, ha=\"right\", fontsize='small')\n",
    "        plt.tight_layout()\n",
    "        plot_path = visuals_output_dir / \"adv_runtime_variability_cv_annotated_revised.png\"\n",
    "        plt.savefig(plot_path); print(f\"✓ Runtime CV plot saved to {plot_path}\"); plt.show()\n",
    "else: print(\"Not enough data (or n<=1, or Matplotlib unavailable) for CV plot.\")\n",
    "\n",
    "\n",
    "# --- 4. Multi-Metric Radar Chart (Revised Metrics & Visuals) ---\n",
    "print(\"\\n--- 4. Multi-Metric Radar Chart for Key Versions (Revised Metrics & Visuals) ---\")\n",
    "radar_metrics_raw = ['NP1 Perf (1/Med.T)', 'Max Speedup (Med.T based)', 'Max Efficiency (Med.T based)', 'Code Volume (log10 LOC)']\n",
    "radar_metrics_display = ['NP1 Perf (1/Med.T_NP1)\\n(Higher=Better)',\n",
    "                         'Max Scaled Speedup\\n(Median-based, Higher=Better)',\n",
    "                         'Max Scaled Efficiency\\n(Median-based, Higher=Better)',\n",
    "                         'Code Volume (log10 LOC)\\n(Lower=Better -> Outer Edge)']\n",
    "radar_versions = ['V1 Serial', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA']\n",
    "radar_data_list = []\n",
    "\n",
    "if not df_loc.empty and plt is not None:\n",
    "    for ver in radar_versions:\n",
    "        median_t_np1_df = execute_query(f\"SELECT MEDIAN(total_time_s) as t_val FROM perf_runs WHERE version = '{ver}' AND np = 1 AND total_time_s IS NOT NULL\")\n",
    "        median_t_np1_val = median_t_np1_df.iloc[0]['t_val'] if median_t_np1_df is not None and not median_t_np1_df.empty and pd.notna(median_t_np1_df.iloc[0]['t_val']) else 0\n",
    "        perf_val = 1.0 / median_t_np1_val if median_t_np1_val > 1e-9 else 0\n",
    "\n",
    "        np_at_max_s = 1\n",
    "        if ver in ['V2.2 ScatterHalo', 'V4 MPI+CUDA']:\n",
    "            max_np_for_ver_df = execute_query(f\"SELECT MAX(np) as max_np FROM perf_runs WHERE version = '{ver}' AND total_time_s IS NOT NULL AND np > 1\")\n",
    "            if max_np_for_ver_df is not None and not max_np_for_ver_df.empty and pd.notna(max_np_for_ver_df.iloc[0]['max_np']):\n",
    "                np_at_max_s = int(max_np_for_ver_df.iloc[0]['max_np'])\n",
    "\n",
    "        median_t_np_max_df = execute_query(f\"SELECT MEDIAN(total_time_s) as t_val FROM perf_runs WHERE version = '{ver}' AND np = {np_at_max_s} AND total_time_s IS NOT NULL\")\n",
    "        median_t_np_max_val = median_t_np_max_df.iloc[0]['t_val'] if median_t_np_max_df is not None and not median_t_np_max_df.empty and pd.notna(median_t_np_max_df.iloc[0]['t_val']) else 0\n",
    "\n",
    "        speedup_val = (median_t_np1_val / median_t_np_max_val) if median_t_np1_val > 1e-9 and median_t_np_max_val > 1e-9 else (1.0 if np_at_max_s == 1 else 0)\n",
    "        efficiency_val = (speedup_val / np_at_max_s) if np_at_max_s > 0 else (1.0 if np_at_max_s == 1 else 0)\n",
    "\n",
    "        loc_val_series = df_loc[df_loc['version'] == ver]['total_loc']\n",
    "        loc_val = loc_val_series.iloc[0] if not loc_val_series.empty and loc_val_series.iloc[0] > 0 else 1\n",
    "        log_loc_val = np.log10(loc_val)\n",
    "        radar_data_list.append([perf_val, speedup_val, efficiency_val, log_loc_val])\n",
    "\n",
    "if radar_data_list:\n",
    "    df_radar_raw = pd.DataFrame(radar_data_list, columns=radar_metrics_raw, index=radar_versions)\n",
    "    df_radar_normalized = df_radar_raw.copy()\n",
    "\n",
    "    for col in ['NP1 Perf (1/Med.T)', 'Max Speedup (Med.T based)', 'Max Efficiency (Med.T based)']:\n",
    "        min_v, max_v = df_radar_raw[col].min(), df_radar_raw[col].max()\n",
    "        df_radar_normalized[col] = (df_radar_raw[col] - min_v) / (max_v - min_v) if (max_v - min_v) > 1e-9 else 0.5 # Avoid div by zero\n",
    "    col_log_loc = 'Code Volume (log10 LOC)'\n",
    "    min_v, max_v = df_radar_raw[col_log_loc].min(), df_radar_raw[col_log_loc].max()\n",
    "    if (max_v - min_v) > 1e-9:\n",
    "         df_radar_normalized[col_log_loc] = 1 - ((df_radar_raw[col_log_loc] - min_v) / (max_v - min_v)) # Invert for \"lower is better\"\n",
    "    else: df_radar_normalized[col_log_loc] = 0.5\n",
    "\n",
    "    num_vars = len(radar_metrics_raw)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist() + [0] # Close the loop\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    marker_styles = ['o', 's', '^', 'D']\n",
    "    # Use a seaborn color palette for better distinction if available\n",
    "    palette_radar = sns.color_palette(\"husl\", n_colors=len(df_radar_normalized.index)) if 'seaborn' in sys.modules else plt.cm.get_cmap('viridis', len(df_radar_normalized.index))\n",
    "\n",
    "\n",
    "    for i, version_name in enumerate(df_radar_normalized.index):\n",
    "        values = df_radar_normalized.loc[version_name].values.flatten().tolist() + [df_radar_normalized.loc[version_name].values.flatten().tolist()[0]] # Close loop\n",
    "        color = palette_radar[i % len(palette_radar)] if 'seaborn' in sys.modules else palette_radar(i / len(df_radar_normalized.index))\n",
    "\n",
    "        ax.plot(angles, values, linewidth=1.5, linestyle=line_styles[i % len(line_styles)],\n",
    "                label=version_name, marker=marker_styles[i % len(marker_styles)], markersize=6, color=color)\n",
    "        ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "\n",
    "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(radar_metrics_display, fontsize=9)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.2)); ax.set_yticklabels([f\"{y:.1f}\" for y in np.arange(0, 1.1, 0.2)], fontsize=8)\n",
    "    ax.set_rlabel_position(30) # Move radial labels\n",
    "    plt.title('Comparative Multi-Metric Radar Chart (Normalized)', size=14, y=1.12)\n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.22), fontsize='small', ncol=len(radar_versions)//2 or 2) # Adjust ncol dynamically\n",
    "\n",
    "    plot_path = visuals_output_dir / \"adv_multi_metric_radar_chart_final_revised.png\"\n",
    "    plt.savefig(plot_path, bbox_inches='tight'); print(f\"✓ Final Revised Radar chart saved to {plot_path}\"); plt.show()\n",
    "else: print(\"[WARNING] Not enough data for revised radar chart (check LOC and perf data).\")\n",
    "\n",
    "\n",
    "# --- 5. Final Comprehensive Scorecard Table (Median-based & CV from run_stats) ---\n",
    "print(\"\\n--- 5. Generating Final Comprehensive Scorecard Table (Median Runtimes, CV from run_stats) ---\")\n",
    "summary_data_final = []\n",
    "key_versions_for_scorecard = ['V1 Serial', 'V2.1 BroadcastAll', 'V2.2 ScatterHalo', 'V3 CUDA', 'V4 MPI+CUDA']\n",
    "df_final_scorecard = pd.DataFrame() # Initialize\n",
    "\n",
    "if not df_loc.empty:\n",
    "    for ver in key_versions_for_scorecard:\n",
    "        loc_val_series = df_loc[df_loc['version'] == ver]['total_loc']\n",
    "        loc_val = loc_val_series.iloc[0] if not loc_val_series.empty else 0\n",
    "\n",
    "        median_np1_s_df = execute_query(f\"SELECT MEDIAN(total_time_s) as median_s FROM perf_runs WHERE version='{ver}' AND np=1 AND total_time_s IS NOT NULL\")\n",
    "        median_np1_s = median_np1_s_df.iloc[0]['median_s'] if median_np1_s_df is not None and not median_np1_s_df.empty and pd.notna(median_np1_s_df.iloc[0]['median_s']) else pd.NA\n",
    "\n",
    "        cv_np1_from_stats_df = execute_query(f\"SELECT CASE WHEN mean_s > 1e-9 THEN ROUND(sd_s / mean_s, 3) ELSE NULL END AS CV FROM run_stats WHERE version='{ver}' AND np=1 AND n > 1\")\n",
    "        cv_np1 = cv_np1_from_stats_df.iloc[0]['CV'] if cv_np1_from_stats_df is not None and not cv_np1_from_stats_df.empty and pd.notna(cv_np1_from_stats_df.iloc[0]['CV']) else pd.NA\n",
    "\n",
    "        np_for_max_metric = 1 # Default for V1, V3\n",
    "        if ver in ['V2.1 BroadcastAll', 'V2.2 ScatterHalo', 'V4 MPI+CUDA']:\n",
    "            max_np_df = execute_query(f\"SELECT MAX(np) as max_np FROM perf_runs WHERE version='{ver}' AND total_time_s IS NOT NULL AND np > 0\")\n",
    "            if max_np_df is not None and not max_np_df.empty and pd.notna(max_np_df.iloc[0]['max_np']) and max_np_df.iloc[0]['max_np'] > 0 :\n",
    "                np_for_max_metric = int(max_np_df.iloc[0]['max_np'])\n",
    "\n",
    "        median_np_max_s_df = execute_query(f\"SELECT MEDIAN(total_time_s) as median_s FROM perf_runs WHERE version='{ver}' AND np={np_for_max_metric} AND total_time_s IS NOT NULL\")\n",
    "        median_np_max_s = median_np_max_s_df.iloc[0]['median_s'] if median_np_max_s_df is not None and not median_np_max_s_df.empty and pd.notna(median_np_max_s_df.iloc[0]['median_s']) else pd.NA\n",
    "\n",
    "        speedup_at_np_max, efficiency_at_np_max = pd.NA, pd.NA\n",
    "        if pd.notna(median_np1_s) and median_np1_s > 1e-9 and pd.notna(median_np_max_s) and median_np_max_s > 1e-9 :\n",
    "            speedup_at_np_max = median_np1_s / median_np_max_s\n",
    "            if np_for_max_metric > 0: efficiency_at_np_max = speedup_at_np_max / np_for_max_metric\n",
    "        elif (ver == 'V1 Serial' or ver == 'V3 CUDA') and np_for_max_metric == 1 : # Explicitly handle non-scalable versions\n",
    "             speedup_at_np_max = 1.0 if pd.notna(median_np1_s) else pd.NA\n",
    "             efficiency_at_np_max = 1.0 if pd.notna(median_np1_s) else pd.NA\n",
    "\n",
    "\n",
    "        summary_data_final.append({\n",
    "            'Version': ver, 'LOC (Core)': loc_val,\n",
    "            'Median T_NP1 (s)': median_np1_s, 'CV @NP1 (Mean-based)': cv_np1,\n",
    "            f'Max Scaled NP': np_for_max_metric, # Store the NP used for max metrics\n",
    "            f'Median T @Max Scaled NP (s)': median_np_max_s,\n",
    "            f'Speedup (Medians) @Max Scaled NP': speedup_at_np_max,\n",
    "            f'Efficiency (Medians) @Max Scaled NP': efficiency_at_np_max\n",
    "        })\n",
    "    if summary_data_final:\n",
    "        df_final_scorecard = pd.DataFrame(summary_data_final).set_index('Version')\n",
    "        df_final_scorecard[df_final_scorecard.select_dtypes(include='number').columns] = df_final_scorecard.select_dtypes(include='number').round(3)\n",
    "        print(\"\\n--- Final Scorecard (Median Runtimes, CV from run_stats) ---\"); display(df_final_scorecard)\n",
    "        scorecard_md_path = visuals_output_dir / \"project_final_scorecard_median_cv_from_stats.md\"\n",
    "        df_final_scorecard.reset_index().to_markdown(scorecard_md_path, index=False)\n",
    "        print(f\"✓ Final scorecard table exported to {scorecard_md_path}\")\n",
    "    else: print(\"[WARNING] Could not generate final scorecard table (check LOC/perf data).\")\n",
    "else: print(\"[WARNING] df_loc empty. Scorecard skipped.\")\n",
    "\n",
    "\n",
    "# --- 6. Markdown Cell for Qualitative Interpretation (Critique Addressed & ValueError Fixed) ---\n",
    "\n",
    "# Initialize strings to 'N/A' to prevent errors if data is missing\n",
    "correlation_r_str, correlation_p_val_str = \"N/A\", \"N/A\"\n",
    "if 'pearson_r_val' in locals() and pd.notna(pearson_r_val) and isinstance(pearson_r_val, (float, int)):\n",
    "    correlation_r_str = f\"{pearson_r_val:.2f}\"\n",
    "if 'p_value_val' in locals() and pd.notna(p_value_val) and isinstance(p_value_val, (float, int)):\n",
    "    correlation_p_val_str = f\"{p_value_val:.2g}\"\n",
    "\n",
    "\n",
    "# Helper function to safely get values from scorecard for interpretation\n",
    "def get_formatted_scorecard_value(df, version, col_name, fmt=\"{:.2f}\"):\n",
    "    if not df.empty and version in df.index and col_name in df.columns and pd.notna(df.loc[version, col_name]):\n",
    "        val = df.loc[version, col_name]\n",
    "        if isinstance(val, (float, int)): return fmt.format(val)\n",
    "        return str(val) # Return as string if not float/int (e.g. already 'N/A' or text)\n",
    "    return \"N/A\"\n",
    "\n",
    "v4_loc_str = get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'LOC (Core)', fmt=\"{:d}\")\n",
    "v2_t_np1_val = df_final_scorecard.loc['V2.2 ScatterHalo']['Median T_NP1 (s)'] if not df_final_scorecard.empty and 'V2.2 ScatterHalo' in df_final_scorecard.index and pd.notna(df_final_scorecard.loc['V2.2 ScatterHalo']['Median T_NP1 (s)']) else None\n",
    "v4_t_np1_val = df_final_scorecard.loc['V4 MPI+CUDA']['Median T_NP1 (s)'] if not df_final_scorecard.empty and 'V4 MPI+CUDA' in df_final_scorecard.index and pd.notna(df_final_scorecard.loc['V4 MPI+CUDA']['Median T_NP1 (s)']) else None\n",
    "\n",
    "speedup_factor_v4_vs_v22_np1_str = \"N/A\"\n",
    "if v2_t_np1_val is not None and v4_t_np1_val is not None and v4_t_np1_val > 1e-9:\n",
    "    speedup_factor_v4_vs_v22_np1_str = f\"{(v2_t_np1_val / v4_t_np1_val):.2f}x\"\n",
    "\n",
    "np_val_str_v4 = get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'Max Scaled NP', fmt=\"{:d}\")\n",
    "v4_speedup_np_max_val = df_final_scorecard.loc['V4 MPI+CUDA']['Speedup (Medians) @Max Scaled NP'] if not df_final_scorecard.empty and 'V4 MPI+CUDA' in df_final_scorecard.index else pd.NA\n",
    "v4_speedup_np_max_str = f\"{v4_speedup_np_max_val:.2f}x\" if pd.notna(v4_speedup_np_max_val) and isinstance(v4_speedup_np_max_val, (int,float)) else \"N/A\"\n",
    "\n",
    "report_banner = f\"Key Insight: Hybrid V4 (LOC: {v4_loc_str}) achieved ~{speedup_factor_v4_vs_v22_np1_str} NP=1 speedup vs. MPI-only V2.2, but scaled poorly to {np_val_str_v4 if np_val_str_v4 != 'N/A' else 'Max NP'} processes (Median-based Speedup: {v4_speedup_np_max_str}), highlighting severe host-staging bottlenecks.\"\n",
    "\n",
    "\n",
    "v1_median_t_np1_for_calc = df_final_scorecard.loc['V1 Serial']['Median T_NP1 (s)'] if not df_final_scorecard.empty and 'V1 Serial' in df_final_scorecard.index and pd.notna(df_final_scorecard.loc['V1 Serial']['Median T_NP1 (s)']) else None\n",
    "v3_median_t_np1_for_calc = df_final_scorecard.loc['V3 CUDA']['Median T_NP1 (s)'] if not df_final_scorecard.empty and 'V3 CUDA' in df_final_scorecard.index and pd.notna(df_final_scorecard.loc['V3 CUDA']['Median T_NP1 (s)']) else None\n",
    "\n",
    "v3_vs_v1_speedup_str = \"N/A\"\n",
    "if v1_median_t_np1_for_calc is not None and v3_median_t_np1_for_calc is not None and v3_median_t_np1_for_calc > 1e-9:\n",
    "    v3_vs_v1_speedup_str = f\"{(v1_median_t_np1_for_calc / v3_median_t_np1_for_calc):.2f}x\"\n",
    "\n",
    "v4_vs_v3_relative_perf_str = \"N/A\"\n",
    "if v3_median_t_np1_for_calc is not None and v4_t_np1_val is not None and v4_t_np1_val > 1e-9: # v4_t_np1_val already defined\n",
    "    v4_is_faster_factor = v3_median_t_np1_for_calc / v4_t_np1_val\n",
    "    if v4_is_faster_factor > 1.005: # Add a small tolerance\n",
    "        v4_vs_v3_relative_perf_str = f\"{v4_is_faster_factor:.2f}x faster than V3\"\n",
    "    elif v4_is_faster_factor < 0.995:\n",
    "         v4_vs_v3_relative_perf_str = f\"{(1/v4_is_faster_factor):.2f}x slower than V3\"\n",
    "    else:\n",
    "        v4_vs_v3_relative_perf_str = \"about equal to V3\"\n",
    "\n",
    "# For V2.2 ScatterHalo in interpretation_md\n",
    "np_val_str_v22 = get_formatted_scorecard_value(df_final_scorecard, 'V2.2 ScatterHalo', 'Max Scaled NP', fmt=\"{:d}\")\n",
    "speedup_median_npmax_v22_val = df_final_scorecard.loc['V2.2 ScatterHalo']['Speedup (Medians) @Max Scaled NP'] if not df_final_scorecard.empty and 'V2.2 ScatterHalo' in df_final_scorecard.index else pd.NA\n",
    "speedup_median_npmax_v22_formatted_str = f\"{speedup_median_npmax_v22_val:.2f}x\" if pd.notna(speedup_median_npmax_v22_val) and isinstance(speedup_median_npmax_v22_val, (float,int)) else \"N/A\"\n",
    "\n",
    "\n",
    "interpretation_md = f\"\"\"\n",
    "## Qualitative Interpretation of Advanced Analysis (Critique Addressed)\n",
    "\n",
    "{report_banner}\n",
    "\n",
    "This analysis uses median runtimes for key performance indicators in the scorecard and radar chart for robustness. Note that general-purpose views like `speedup` (and plots from earlier cells if not regenerated) may still use MIN-based T1 for baseline performance.\n",
    "\n",
    "**1. Code Complexity (LOC) vs. Single-Core/GPU Performance:**\n",
    "*   **Figure:** `adv_median_performance_vs_loc_corr_revised.png`\n",
    "*   **Takeaway:** Explores if more LOC (core algorithmic files) correlates with NP=1 median runtime.\n",
    "*   **Your Observation & Data:**\n",
    "    *   V1 Serial LOC: {get_formatted_scorecard_value(df_final_scorecard, 'V1 Serial', 'LOC (Core)', fmt=\"{{:d}}\")}, Median T_NP1: {get_formatted_scorecard_value(df_final_scorecard, 'V1 Serial', 'Median T_NP1 (s)')}s.\n",
    "    *   V3 CUDA LOC: {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'LOC (Core)', fmt=\"{{:d}}\")}, Median T_NP1: {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'Median T_NP1 (s)')}s.\n",
    "    *   V4 MPI+CUDA LOC: {get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'LOC (Core)', fmt=\"{{:d}}\")}, Median T_NP1: {get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'Median T_NP1 (s)')}s.\n",
    "    *   Discuss: V3's GPU offload (LOC: {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'LOC (Core)', fmt=\"{{:d}}\")}) yielded a median NP=1 runtime of {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'Median T_NP1 (s)')}s. This was {v3_vs_v1_speedup_str} faster than V1 Serial. V4 MPI+CUDA (highest LOC: {get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'LOC (Core)', fmt=\"{{:d}}\")}) achieved a median NP=1 time of {get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'Median T_NP1 (s)')}s, which is {v4_vs_v3_relative_perf_str}.\n",
    "*   **Correlation:** Pearson R = **{correlation_r_str}**, p-value = **{correlation_p_val_str}**.\n",
    "    *   Interpret this: The correlation of R={correlation_r_str} (p={correlation_p_val_str}) suggests a weak and statistically insignificant linear relationship between LOC and median NP=1 runtime for this dataset. This indicates that the choice of parallelization paradigm (CPU, MPI, CUDA, Hybrid) and its specific implementation details had a much stronger impact on single-process performance than mere code volume.\n",
    "\n",
    "**2. Runtime Variability (Stability):**\n",
    "*   **Figure:** `adv_runtime_variability_cv_annotated_revised.png` (Note: 'n' values are in the displayed table `df_variability`).\n",
    "*   **Takeaway:** CV (StdDev/Mean from `run_stats`) shows consistency. Lower is better.\n",
    "*   **Your Observation & Data:**\n",
    "    *   V1 Serial (NP=1) CV: {get_formatted_scorecard_value(df_final_scorecard, 'V1 Serial', 'CV @NP1 (Mean-based)')}.\n",
    "    *   V3 CUDA (NP=1) CV: {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'CV @NP1 (Mean-based)')}.\n",
    "    *   V2.2 ScatterHalo (NP={np_val_str_v22 if np_val_str_v22 != 'N/A' else 'Max'}) CV: {df_variability[(df_variability['version']=='V2.2 ScatterHalo') & (df_variability['np']==int(np_val_str_v22 if np_val_str_v22.isdigit() else 0))]['CV'].iloc[0] if df_variability is not None and np_val_str_v22.isdigit() and not df_variability[(df_variability['version']=='V2.2 ScatterHalo') & (df_variability['np']==int(np_val_str_v22))].empty else 'N/A'}.\n",
    "    *   V4 MPI+CUDA (NP={np_val_str_v4 if np_val_str_v4 != 'N/A' else 'Max'}) CV: {df_variability[(df_variability['version']=='V4 MPI+CUDA') & (df_variability['np']==int(np_val_str_v4 if np_val_str_v4.isdigit() else 0))]['CV'].iloc[0] if df_variability is not None and np_val_str_v4.isdigit() and not df_variability[(df_variability['version']=='V4 MPI+CUDA') & (df_variability['np']==int(np_val_str_v4))].empty else 'N/A'}.\n",
    "    *   Discuss: V1 Serial and V3 CUDA NP=1 runs show CVs suggesting moderate stability. V4 MPI+CUDA at NP=1 has a notably high CV ({get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'CV @NP1 (Mean-based)')}), indicating significant run-to-run variation potentially due to MPI setup or complex interactions even at NP=1. For MPI versions, examine if CV increases with NP. Small 'n' values (check table `df_variability` for sample sizes) reduce CV reliability.\n",
    "\n",
    "**3. Multi-Dimensional Performance (Radar Chart):**\n",
    "*   **Figure:** `adv_multi_metric_radar_chart_final_revised.png`\n",
    "*   **Metrics (Median-based Speedup/Efficiency for this chart):** 'NP1 Perf (1/Med.T_NP1)', 'Max Scaled Speedup', 'Max Scaled Efficiency', 'Code Volume (log10 LOC)' (normalized so outer edge means less code).\n",
    "*   **Takeaway:** Visualizes relative strengths. Outer edge is \"better\" for each metric's normalized scale.\n",
    "*   **Your Observation & Data:**\n",
    "    *   V1 Serial: Strong on 'Code Volume' (less code is better, so it's far out on that axis after normalization). Performance metrics are closer to the center (worse).\n",
    "    *   V2.2 ScatterHalo: Aims for a balance, showing some 'Max Scaled Speedup' and 'Efficiency' for CPU parallelism compared to V1, but with higher 'Code Volume'.\n",
    "    *   V3 CUDA: Dominates 'NP1 Perf.' due to GPU acceleration. 'Max Scaled Speedup/Efficiency' are 1.0 by definition (as it's NP=1). 'Code Volume' is moderate.\n",
    "    *   V4 MPI+CUDA: Achieved good 'NP1 Perf.' (comparable to V3), but its 'Max Scaled Speedup'/'Efficiency' at NP={np_val_str_v4 if np_val_str_v4 != 'N/A' else 'Max'} are poor due to host-staging, pulling it inwards on those axes relative to its NP=1 potential. It has the highest 'Code Volume' (least favorable, so closest to center on that axis before normalization, furthest after 1-x normalization).\n",
    "    *   Refer to the scorecard for absolute magnitudes. The radar chart shows relative normalized strengths.\n",
    "\n",
    "**4. Overall Project Trajectory & Bottlenecks:**\n",
    "*   **Scorecard Table:** `project_final_scorecard_median_cv_from_stats.md` (Medians for T_NP1, T@Max_Scaled_NP; Speedup/Efficiency from these medians).\n",
    "*   **Super-linear Speedup Check:** The V2.2 Speedup (Medians) @NP={np_val_str_v22 if np_val_str_v22 != 'N/A' else 'Max'} is {speedup_median_npmax_v22_formatted_str}. This is typically sub-linear for this type of problem after accounting for communication, as expected.\n",
    "*   **Performance Discussion (from Scorecard):**\n",
    "    *   V1 Median T_NP1: {get_formatted_scorecard_value(df_final_scorecard, 'V1 Serial', 'Median T_NP1 (s)')}s is the reference.\n",
    "    *   V2.2 Median T @Max Scaled NP (NP={np_val_str_v22 if np_val_str_v22 != 'N/A' else 'Max'}): {get_formatted_scorecard_value(df_final_scorecard, 'V2.2 ScatterHalo', 'Median T @Max Scaled NP (s)')}s; Median-based Speedup: {speedup_median_npmax_v22_formatted_str}. Demonstrates effective CPU scaling with MPI scatter/halo.\n",
    "    *   V3 Median T_NP1: {get_formatted_scorecard_value(df_final_scorecard, 'V3 CUDA', 'Median T_NP1 (s)')}s (approx. {v3_vs_v1_speedup_str} vs V1), showing significant GPU acceleration.\n",
    "    *   V4 Median T @Max Scaled NP (NP={np_val_str_v4 if np_val_str_v4 != 'N/A' else 'Max'}): {get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'Median T @Max Scaled NP (s)')}s; Median-based Speedup: {v4_speedup_np_max_str}. Poor scaling indicates host-staging overheads dominate benefits of multi-GPU distribution for this problem size and implementation.\n",
    "*   **Bottleneck Migration:** The project successfully demonstrated bottleneck migration: V1 (CPU compute-bound) -> V2.2 (MPI communication overheads and remaining CPU compute) -> V3 (GPU compute, PCIe bandwidth for H-D transfers) -> V4 (Host-staging: MPI communication on CPU, multiple full-tile H-D-H transfers, CPU logic for halo management).\n",
    "\n",
    "**5. Expert Perspectives & Recommendations (Critique Addressed):**\n",
    "*   **Performance Engineer:** The V4 MPI+CUDA version's poor scaling ({v4_speedup_np_max_str} at NP={np_val_str_v4 if np_val_str_v4 != 'N/A' else 'Max'}) despite good NP=1 performance ({get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'Median T_NP1 (s)')}s) clearly points to inter-process communication and data staging (CPU-GPU transfers) as the major bottleneck. Profiling V4 with Nsight Systems is essential to quantify these overheads. The primary recommendation is to **implement V5 (CUDA-Aware MPI)** to allow direct GPU-GPU communication, bypassing CPU staging for halo exchanges. Subsequently, investigate asynchronous operations (CUDA streams for compute/copy overlap, non-blocking MPI) for V4/V5. The high CV@NP1 for V4 MPI+CUDA ({get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'CV @NP1 (Mean-based)')}) also warrants investigation – could be initial MPI synchronization costs or variability in GPU resource availability even when running as a single MPI process.\n",
    "*   **Software Engineer:** The Lines of Code (LOC) for V4 MPI+CUDA ({get_formatted_scorecard_value(df_final_scorecard, 'V4 MPI+CUDA', 'LOC (Core)', fmt=\"{{:d}}\")}) being the highest reflects the significant integration complexity of combining MPI and CUDA with manual host staging. The `alexnetTileForwardCUDA` in V4 acts as a monolithic GPU compute step per rank, which is good for local GPU utilization but highlights the data movement problem to/from it. The high runtime variability (CV) for V4 is a concern for reproducibility and indicates potential instability or contention. Future work should focus on reducing V4's complexity and improving its stability if V5 is not pursued.\n",
    "*   **Data Analyst:** Using median runtimes for scorecard KPIs enhances robustness against outliers compared to using minimums. The small sample sizes ('n' values in `df_variability`) for some runs limit the precision of Confidence Intervals around CV and median estimates. The statistically insignificant Pearson R ({correlation_r_str}, p={correlation_p_val_str}) between LOC and NP=1 performance implies that for this project, the architectural paradigm (Serial, MPI, CUDA, Hybrid) and the efficiency of its implementation (e.g., scatter/halo vs. broadcast, host-staging vs. direct GPU) were far more determinant of single-process performance than simply the amount of code written.\n",
    "*   **Domain Expert (HPC for AI):** The V1 serial median time ({get_formatted_scorecard_value(df_final_scorecard, 'V1 Serial', 'Median T_NP1 (s)')}s) serves as a critical reference; ensure modern compiler optimizations (-O3, target architecture flags) were used. V3's strong performance ({v3_vs_v1_speedup_str} over V1) confirms the suitability of GPUs for CNN computations. V4's poor scaling is a classic symptom of naive hybrid implementations where data movement between host and device for inter-process communication dominates. **CUDA-Aware MPI (V5) is the standard HPC solution to mitigate this for distributed GPU training/inference of such workloads.** Further, optimizing data layout (e.g., NCHW vs. NHWC) and using optimized libraries (cuDNN, if full layers were built) would be typical next steps in a production setting, though out of scope here.\n",
    "\n",
    "**Further Project Steps (as outlined in presentation):**\n",
    "1.  **Implement V5 (CUDA-Aware MPI):** Directly address V4's primary bottleneck.\n",
    "2.  **Asynchronous Overlap:** If V5 proves difficult or for further optimization, explore CUDA streams and non-blocking MPI to overlap data transfers with computation.\n",
    "3.  **Cluster Profiling:** Utilize Nsight Systems/Compute on the target cluster for detailed analysis of V3, V4, and any V5 implementation to precisely identify bottlenecks.\n",
    "4.  **Reporting:** Clearly distinguish between MIN-based metrics (often used for \"best case\" speedup plots) and MEDIAN-based metrics (used here for robust scorecard evaluation). Discuss the impact of small sample sizes on statistical confidence where applicable. Provide comprehensive details of the hardware/software environment used for reproducibility.\n",
    "\"\"\"\n",
    "\n",
    "if 'Markdown' in globals() and 'display' in globals(): display(Markdown(interpretation_md))\n",
    "else: print(interpretation_md)\n",
    "\n",
    "interpretation_file_path = visuals_output_dir / \"qualitative_interpretation_summary_final_valueerror_fixed.md\"\n",
    "with open(interpretation_file_path, \"w\", encoding=\"utf-8\") as f: f.write(interpretation_md)\n",
    "print(f\"\\n✓ Final qualitative interpretation (ValueError fixed) saved to {interpretation_file_path}\")\n",
    "\n",
    "print(\"\\n--- Grand Synthesis Cell (Critique Addressed & ValueError Fixed) Successfully Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
