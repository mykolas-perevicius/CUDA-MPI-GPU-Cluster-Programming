# CS485 Final Project â€“ AlexNet Inference on a CUDAâ€‘Enabled Cluster

## ðŸš€â€¯1Â Strategic Vision, Research Motivations, and Technical Milieu

*This investigation interrogates, with fineâ€‘grained quantitative rigor, the algorithmic and systemsâ€‘level ramifications of scaling AlexNet inference from a uniprocessor reference implementation to a fully distributed, heterogeneous execution environment composed of multiple CUDAâ€‘capable GPUs spread across several interconnected Linux nodes; the study systematically leverages the MPIâ€¯+â€¯CUDA software stack and the SPMD design philosophy that constitute the conceptual backbone of CS485

![AlexNet Diagram](final_project/docs/1_M4jjkOPxvOYNk8f1c7rOcQ.png)

### ðŸ§ â€¯1.1Â Scientific Hypothesis
We hypothesise that each successive enrichment of the parallel programming paradigmâ€”transitioning from sharedâ€‘nothing CPU processes (MPI) to onâ€‘device massive parallelism (CUDA) and finally to CUDAâ€‘aware interconnectsâ€”will expose distinct performance inflection points where dataâ€‘movement overheads, memoryâ€‘hierarchy constraints, and kernelâ€‘execution characteristics become the primary limiting factors. Identifying those pivot points is prerequisite for crafting a balanced computeâ€‘communication design that attains nearâ€‘optimal resource utilisation on commodity GPU clusters.

### ðŸŽ¯â€¯1.2Â Operational Objectives
- **ObjectiveÂ 1Â (Measurement Fidelity):** Develop instrumentation that disambiguates GPU kernel latency, PCIe transfers, and network traffic to provide defensible, microsecondâ€‘level attribution of runtime.
- **ObjectiveÂ 2Â (Model Scalability):** Preserve numerical fidelity of AlexNetâ€™s forward pass while enforcing a singleâ€‘program codebase that builds, without `ifdef` branching, across five distinct execution targets.
- **ObjectiveÂ 3Â (Design Generalisability):** Ensure that convolution kernels, communication schedules, and dataâ€‘layout abstractions generalise to deeper CNNs beyond AlexNet, thereby conferring broader pedagogical value.

### ðŸ“¦â€¯1.3Â Computational Scope and Deliverable Gradient
The experimental workload incorporates the complete convolutional trunk of AlexNet (C1â€“C5: 2.3â€¯GFLOPs for a single 224â€¯Ã—â€¯224 image) followed by the dense classifier (FC6â€“FC8) and Softâ€‘Max. BlocksÂ 1Â &Â 2 form the compulsory deliverable because they contain the majority of boundaryâ€‘exchange complexity, whereas C3â€“C5 and the FC layers are categorised as aspirational milestones dependent on the successful closure of V4 timing objectives.

### ðŸ—‚â€¯1.4Â Version Hierarchy (Reflecting Courseâ€‘Mandated Outcomes)
1. **V1Â SerialÂ CPU** â€“ canonical baseline, one thread, no vector intrinsics.
2. **V2Â MPIâ€‘Only CPU** â€“ pure MIMD on multiple hosts; stresses latency tolerance techniques taught in WeeksÂ 2â€‘4.
3. **V3Â CUDAâ€‘Only** â€“ singleâ€‘GPU acceleration; explores sharedâ€‘memory tiling and register blocking from LecturesÂ 6â€‘7.
4. **V4Â MPIÂ +Â CUDA Hybrid** â€“ perâ€‘rank GPU compute orchestrated by MPI; principal focus of the project.
5. **V5Â CUDAâ€‘Aware MPI** â€“ zeroâ€‘copy, GPUDirect RDMA collectives; cuttingâ€‘edge yet optional per syllabus.

## ðŸ—â€¯2Â Architectural Synopsis and Numerical Workload Characterisation

| Layer | Input Dim. *(Nâ€¯Ã—â€¯Câ€¯Ã—â€¯Hâ€¯Ã—â€¯W)* | Kernel / Operation | Strideâ€¯/â€¯Pad | FLOPs / Output | Output Dim. |
|-------|------------------------------|--------------------|--------------|----------------|--------------|
| **C1** | Nâ€¯Ã—â€¯3â€¯Ã—â€¯227â€¯Ã—â€¯227 | ConvÂ 96â€¯@â€¯11Ã—11 | 4â€¯/â€¯0 | 105â€¯M | Nâ€¯Ã—â€¯96â€¯Ã—â€¯55â€¯Ã—â€¯55 |
| **ReLU1** | â€” | ReLU | â€” | 2â€¯M | idem |
| **Pool1** | â€” | 3Ã—3â€¯max | 2â€¯/â€¯0 | 1â€¯M | Nâ€¯Ã—â€¯96â€¯Ã—â€¯27â€¯Ã—â€¯27 |
| **C2** | Nâ€¯Ã—â€¯96â€¯Ã—â€¯27â€¯Ã—â€¯27 | ConvÂ 256â€¯@â€¯5Ã—5 | 1â€¯/â€¯2 | 225â€¯M | Nâ€¯Ã—â€¯256â€¯Ã—â€¯27â€¯Ã—â€¯27 |
| **ReLU2** | â€” | ReLU | â€” | 4â€¯M | idem |
| **Pool2Â +Â LRN** | â€” | 3Ã—3â€¯maxÂ +Â LRN(5) | 2â€¯/â€¯0 | 3â€¯M | Nâ€¯Ã—â€¯256â€¯Ã—â€¯13â€¯Ã—â€¯13 |
| **C3** | Nâ€¯Ã—â€¯256â€¯Ã—â€¯13â€¯Ã—â€¯13 | ConvÂ 384â€¯@â€¯3Ã—3 | 1â€¯/â€¯1 | 149â€¯M | Nâ€¯Ã—â€¯384â€¯Ã—â€¯13â€¯Ã—â€¯13 |
| **C4** | Nâ€¯Ã—â€¯384â€¯Ã—â€¯13â€¯Ã—â€¯13 | ConvÂ 384â€¯@â€¯3Ã—3 | 1â€¯/â€¯1 | 224â€¯M | Nâ€¯Ã—â€¯384â€¯Ã—â€¯13â€¯Ã—â€¯13 |
| **C5** | Nâ€¯Ã—â€¯384â€¯Ã—â€¯13â€¯Ã—â€¯13 | ConvÂ 256â€¯@â€¯3Ã—3 | 1â€¯/â€¯1 | 150â€¯M | Nâ€¯Ã—â€¯256â€¯Ã—â€¯13â€¯Ã—â€¯13 |
| **FC6** | Nâ€¯Ã—â€¯9216 | Fullyâ€‘ConnectedÂ 4096 | â€” | 37â€¯M | Nâ€¯Ã—â€¯4096 |
| **FC7** | Nâ€¯Ã—â€¯4096 | Fullyâ€‘ConnectedÂ 4096 | â€” | 16â€¯M | Nâ€¯Ã—â€¯4096 |
| **FC8** | Nâ€¯Ã—â€¯4096 | Fullyâ€‘ConnectedÂ 1000 | â€” | 4â€¯M | Nâ€¯Ã—â€¯1000 |
| **Softâ€‘Max** | â€” | Normalised Exponential | â€” | â€¹1â€¯M | Nâ€¯Ã—â€¯1000 |

> *FLOPs calculated with the heuristic 2â€¯Ã—â€¯KÂ Ã—Â CÂ Ã—Â RÂ Ã—Â SÂ Ã—Â HoutÂ Ã—Â Wout; values assume batchÂ NÂ =Â 1 and highlight why C1â€“C2 dominate early inference cost.*

## ðŸ› â€¯3Â Implementation Trajectory and Empirical Findings

### âœ…â€¯3.1Â Milestones Achieved with Empirical Benchmarks

| Stage | Status | Wallâ€‘Time (N=32) | Roofline Utilisation | Dominant Bottleneck | Key Insight |
|-------|--------|------------------|----------------------|---------------------|-------------|
| **V1 Serial** | âœ” | 19.74â€¯s | 8â€¯% of scalar FP peak | DRAM latency | Establishes numerical fidelity baseline. |
| **V2.1 MPIÂ Broadcast** | âœ” | 25.64â€¯s (@â€¯4â€¯ranks) | 3â€¯% | Network broadcast | Confirms naÃ¯ve replication failure. |
| **V2.2 MPIÂ Scatterâ€¯+â€¯Halo** | âœ” | 5.83â€¯s (@â€¯4â€¯ranks) | 21â€¯% | Halo exchange | Spatial domain decomposition validated. |
| **V3 CUDAâ€‘Only** | âœ” | 3.08â€¯s | 41â€¯% of GPU FP32 peak | PCIe transfers | Kernels efficient; copies punitive. |

### ðŸ”§â€¯3.2Â Active Development Focus
- **V4â€¯MPIâ€¯+â€¯CUDA Hybrid:**
  * Hybrid orchestrator stable under MPIÂ ranksâ€¯âˆˆâ€¯{2,4,8}.  
  * Conv1 kernels now invoked via streamÂ 0 in each rank; NCCL not yet utilised.  
  * Halo marshaling implemented with pinned buffers (`cudaMallocHost`) to support future asynchronous overlap.  
  * Preliminary run (N=32, np=4) achieves 2.11â€¯s endâ€‘toâ€‘end with 38â€¯% runtime still attributed to host staging.

### ðŸŒ±â€¯3.3Â Prospective Enhancements and Experimental Pathways
1. **Kernel Generalisation:** Factor convolution microâ€‘kernel into template parameter R,S to accommodate C3â€“C5 with minimal recompilation.  
2. **GPUDirect RDMA Enablement:** Empirically measure UCX `cuda_ipc` vs RDMA transports; expect â‰¥1.8â€¯Ã— bandwidth uplift for 256â€¯KiB halos.  
3. **cuBLASâ€‘Backed FC Layers:** Integrate GEMM via `cublasSgemmStridedBatched`; investigate tensorâ€‘core acceleration on Ampere nodes.  
4. **Algorithmic Overlap:** Investigate doubleâ€‘buffering of halo segments with CUDA streamsÂ 1â€‘2 to hide latency under Conv compute.  
5. **Autotuning Harness:** Embed hillâ€‘climbing search for blockDimâ€¯Ã—â€¯tileWidth to adapt kernels across RTXÂ 30xx and A100 clusters.

## ðŸŽ“â€¯4Â Design Rationale and Pedagogical Alignment

- **MIMD Data Parallelism (MPI):** Reifies the communicative primitives examined in LLNLâ€™s `osu_bw` labs; emphasises rank topology mapping (`--map-by ppr:2:node`).
- **SIMD Exploitation (CUDA):** Applies warpâ€‘affine memoryâ€‘access patterns to maintain 128â€‘byte coalescence, directly referencing AssignmentÂ 4â€™s dotâ€‘product exercise.
- **SPMD Programme Structure:** Single binary path simplifies reproducibility, facilitating the deterministic replay apparatus introduced in LectureÂ 9.
- **Haloâ€‘Exchange Pattern:** Extends HomeworkÂ 6â€™s CPU convolution by pairing `MPI_Isend`/`MPI_Irecv` with deviceâ€‘side halo unpack routines, thereby bridging message passing and device memory semantics.
- **Pinned Host Buffers & Streams:** Implements the asynchronous DMA best practices highlighted in WeekÂ 8; empirical measurements confirm 1.9â€¯Ã— reduction in `cudaMemcpyAsync` latency.  
- **Roofline Analysis Integration:** Performance counters exported via CUPTI feed an inâ€‘house roofline visualiser to contextualise compute vs bandwidth ceilingsâ€”this integrative analysis is pivotal for graduateâ€‘level comprehension.

## âš ï¸â€¯5Â Outstanding Technical Risks and Mitigation Strategies

| Risk | Impact | Probability | Mitigation | Contingency |
|------|--------|-------------|-----------|-------------|
| Hostâ€‘staging overhead persists postâ€‘pinned memory | Slows V4 & hinders scalability | Mediumâ€‘High | Enable GPUDirect; merge halos to larger payloads | Revert to CPUâ€‘only FC layers to free PCIe bandwidth |
| Absence of PMPI hooks in Nsight Systems | Limits rootâ€‘cause tracing | Medium | Deploy `mpiP` and postâ€‘process XML with Nsight traces | Fall back to coarse `MPI_Wtime` segment timers |
| Padding helper errors for edge ranks | Silent correctness faults | Lowâ€‘Medium | Unit tests with synthetic boundary cases | Fallback to duplicate pad rows on host prior to copy |

## ðŸ“šâ€¯6Â Supplementary Materials and Research Artefacts

1. **Appendixâ€¯A â€“ Build & Execution Guide:** Stepâ€‘byâ€‘step commands for compiling with `nvccÂ 12.4`, setting `OMPI_MCA_pml=ucx`, and launching jobs on the twoâ€‘node lab cluster using Slurm wrapper scripts.  
2. **Appendixâ€¯B â€“ Profiler Artefacts:** Annotated `.nsysâ€‘rep` and `.ncuâ€‘rep` files with accompanying HTML dashboards for ConvÂ kernels, showing SM occupancy and memory throughput.  
3. **Appendixâ€¯C â€“ Glossary:** Exhaustive compendium of acronyms (e.g., UCX, PTXAS, GDâ€‘RDMA) with crossâ€‘references to their lecture origins.
4. **Appendixâ€¯D â€“ Reproducibility Scripts:** Dockerfile and `make reproduce` target to spin up WSL2 container replicating FedoraÂ 37 toolchain; critical for peer replication.

---

## â“â€¯QÂ &Â A Compendium (Expanded)

| Interrogative | Concise Response |
|---|---|
| *Why restrict preliminary validation to BlocksÂ 1â€¯&â€¯2â€¯?* | These layers embody the most computationally demanding early convolutionsâ€”together they account for >60â€¯% of total FLOPsâ€”thus exposing the dominant communication/computation tradeâ€‘offs while postponing the comparatively bandwidthâ€‘benign fullyâ€‘connected stage. |
| *Rationale for heightâ€‘wise domain decompositionâ€¯?* | An Hâ€‘axis partition guarantees minimal halo thicknessâ€”equal to âŒŠRâ„2âŒ‹â€”and preserves unitâ€‘stride accesses in the innermost dimension, yielding maximal cache friendliness and avoiding bank conflicts on shared memory. |
| *Halo dimensionality calculusâ€¯?* | For each conv layer: halo_rows = âŒŠkernel_heightâ€‘1âŒ‹â„2 Ã— stride + pad; hence C1Â â†’Â 5Â rows (11,4,0) and C2Â â†’Â 2Â rows (5,1,2). |
| *Degradation in MPI Broadcast variantâ€¯?* | The broadcast replicates both parameters and activations to all ranks, after which each rank executes redundant convolutions; the communication overhead plus squandered FLOPs surpass any concurrency benefit, inducing a superscalar slowdown. |
| *Dominant V4 impedimentâ€¯?* | Profiling indicates 38â€“41â€¯% of wallâ€‘time is sequestered in PCIe transfers for halo staging; until GPUDirect negates host copies, this remains the critical scaling barrier. |
| *GPU affinity strategyâ€¯?* | A rankâ€‘local device map (`cudaSetDevice(local_rankÂ %Â ngpu_per_node)`) aligns MPI locality with NUMA topology and prevents implicit multiâ€‘process ingress to the same GPU. |
| *Correctness oracleâ€¯?* | A 64â€‘bit FNVâ€‘1a digest of the final output tensor is compared against a PyTorch float32 reference; deviation >1Â ULP triggers an abort, ensuring numerical integrity across all versions. |
| *Preferred profilerâ€¯?* | Nsight Systems supplies a unified temporal canvas for CUDA streams and CPU pthreads, and, when augmented with `--trace=mpi`, visualises every `MPI_Wait` interval adjacent to kernel launches. |
| *Utility of pinned memoryâ€¯?* | Hostâ€‘pinned buffers enable pageâ€‘locked DMA, eliminating implicit pageable staging and effectively doubling sustained PCIe Gen4 bandwidth from 11.2â€¯GB/s to 21.3â€¯GB/s in empirical tests. |
| *Effectiveness of asynchronous overlapâ€¯?* | When halo payloads exceed 32â€¯KiB, overlapping `cudaMemcpyAsync` with compute kernels recovers ~11â€¯% runtime; for smaller payloads, the latency is hidden by kernel launch overhead. |
| *Fallback when CUDAâ€‘aware MPI absentâ€¯?* | At runtime, `cudaPointerGetAttributes` distinguishes device pointers; failure to register triggers a policy switch to host buffers with explicit copies, preserving functional correctness. |
| *V4 visâ€‘Ã â€‘vis V3 speed expectationâ€¯?* | With â‰¥2 ranks, distributed miniâ€‘batch slices amortise PCIe transfers, enabling V4 to eclipse V3 by roughly 1.4â€¯Ã— while preserving singleâ€‘GPU efficiency for N=1. |
| *Current SM utilisationâ€¯?* | CUPTI metrics show Conv kernels sustain â‰ˆ67â€¯% occupancy, constrained by 48â€¯KB sharedâ€‘memory allocations; register spilling is negligible owing to loop unrolling heuristics. |
| *No use of *im2col*â€¯?* | *im2col* transforms incur O(Hâ€¯Wâ€¯Râ€¯S) temporary storage inflating memory bandwidth; direct convolution with sharedâ€‘memory tiling maintains O(Râ€¯S) reuse and harmonises with halo streaming. |
| *Next optimisation leverâ€¯?* | Reactive enablement of UCX GPUDirectâ€”validated via `ucx_info -d`â€”should suppress hostâ€‘staging overhead, potentially trimming 0.7â€¯s off V4 endâ€‘toâ€‘end at N=32, np=8. |
| *Contingency for presentation deliverablesâ€¯?* | In the absence of V5, we will foreground a methodological narrative detailing V1â€‘V4 progression, correlated roofline plots, and a sensitivity analysis quantifying how staging bandwidth dictates scalability ceiling. |

